{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the convolutional network\n",
    "2. Gain a intuitive understanding of the convolution (feature map) and pooling (subsampling) operations\n",
    "3. Develop a basic code implementation of the LeNet-5 and AlexNet netoworks in Python\n",
    "4. Identify what kind of problems can be approached with the convolutional network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hubel and Wiesel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt's photo-perceptron (1958) was the first neural network model attempting to emulate human visual and perceptual capacities. Unfortunetaly, little was known at the time about the mammalian visual cortex that could inform Rosenblatt's work. Consequently, the photo-perceptron architecture was inspired by a very coarse idea of how the information flows from the eyeballs to be processed by the brain. This changed fast in the years following the introduction of the perceptron. \n",
    "\n",
    "In 1962, [David H. Hubel](https://en.wikipedia.org/wiki/David_H._Hubel) and [Torsten Wiesel](https://en.wikipedia.org/wiki/Torsten_Wiesel) published one the major breaktroughts in the neurophysiology of the visual cortex: **the existence of orientation selectivity and columnar organization**. This is what they did: they placed tiny microelectrode in a single neuron in the primary visual cortex (V1) of an anesthetized cat, and proyected light and dark dots into the cat's eye. It did not work at all, they could not get a response from the neuron. But, they had a lucky accident. Since they were using a slide projector to show the dots, the *margin of the slide* with the dot also was projected into the cat's ayes and bam! the neuron fired. From there, they experimented with light and dark bars in different orientations, which led them to propose the existence of **three types of cells in the visual cortex**:\n",
    "\n",
    "1. **simple cells**, that fire at higher (or lower) rate depending on the bar orientation. Sometimes called \"line detectors\".\n",
    "2. **complex cells** that fire in response to a wider variety of orientations, yet, they still show a preference (higher firing rate) to certain orientations. Sometimes are called \"motion detectors\". Importantly, these cells receive input from several *simple cells*. \n",
    "3. **hypercomplex cell**, characterized by reacting to \"stopped\" oriented edges. Again, these cells receive input from several *complex cells*.\n",
    "\n",
    "As you may have noticed, there three types of cells are **hierarchically organized**. Keeps this in mind as it'll become important later. Altogether, these discoveries were the basis of the work that granted them the Nobel Prize in Physiology in 1981. [Here](http://www.youtube.com/watch?v=jw6nBWo21Zk) is short video from their experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fukushima's Neocognitron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of Hubel and Wiesel served as the basis for the precursor of modern convolutional neural netwroks: **Fukushima's Neocognitron** (1980). [Kunihiko Fukushima](https://en.wikipedia.org/wiki/Kunihiko_Fukushima), a japanese computer scientist, developed the the Neocognitron idea while working at the [NHK Science & Technology Research Laboratories](https://en.wikipedia.org/wiki/NHK_Science_%26_Technology_Research_Laboratories). He did this by implementing the simple-cells and complex-cells discovered by Hubel and Wiesel in a multilayer neural network architecture. **Figure X** shows a simplified diagram of the Neocognitron with 3 layers (4 if you count the inputs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Simplified Neocognitrone </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind the Neocognitron is the following: the **input layer $L_0$ works as the retina**, reading the raw input pattern. Then, each cell in a $S_1$ patch \"reads\" a sub-section of the input image based on a \"preference\" for certain type of pattern. Any given layer $L_n$ will have several of this $S_j$ patches as a collection of **feature \"filters\"**. Some may detect a diagonal line, while other a small triangle, or a corner, or something else. Each $S_j$ patch connects to a $C_k$ cell, and such a cell fires if gets any positive input from its corresponding patch. This process is also known as **\"pooling\"**. This cycle of \"feature\" detection and \"pooling\" is repeated as many times as intermediate layers in the network. The last layer correspond to the output, where some neuron will fire depending of the input pattern. Mathematically, \"feature detection\" is accomplished by multiplying the input by a fix matrix of weights, whereas \"pooling\" corresponding to taking an average of the connected patch of S-cells. \n",
    "\n",
    "You may have noticed that the behavior of the S-cells and C-cells replicate (to some extent) what Hubel and Wiesel found in their experiments. The great thing about this architecture is that is **robust to shifts in the input image**: you can move the image around and the combination of \"feature detection\" and \"pooling\" will detect the precense of each part of the image regardless of its position. **Figure X** exemplifies this characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/neocognitron-cells.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neocognitron is also **robust to deformation**: it will detect the object even if it's enlarged, reduced in size, or blurred, by virtue of the same mechanism that allows robustness to positional shifting. It is also important to notice that the pooling operation will \"blur\" the input image, and the fact that C-cells take the average of its corresponding S-cells makes the pooling more robust to random noise added to the image. [Here](http://www.youtube.com/watch?v=Qil4kmvm2Sw) you can find a short video explaining the basics of the Neocognitron as well. In sum, the Neocognitron stablished the following principles: \n",
    "\n",
    "- S-cells extract simple features\n",
    "- C-cells combine and subsample S-cells extracted features\n",
    "- Image features are learned and combined to produce rich representations\n",
    "- The image recognition process is hierarchical organized\n",
    "\n",
    "\n",
    "If you are familiar with convolutional neural networks, you may be wondering what is the difference between the Neocognitron and later models like Yann LeCun's LeNet (1989), since they look remarkably similar. They main (but not only) difference is the training algorithm: **the Neocognitron does not use backpropagation**. At the time, backpropagation was not widely known as a training method for multilayer neural networks reason why Fukushima never use it, and trained his model by using an unsupervised learning approach. Regardless, the Neocognitron lay the groundwork of modern neural network models of vision and computer vision more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeCun's LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture today known as the convolutional neural network was introduced by [Yann LeCun](http://yann.lecun.com/) in 1989. Although LeCun was trained as an Electrical Engineer, he got interested in the idea of building intelligent mahines from early on in his undergradute education by reading a book about the [Piaget vs Chomsky debate on language acquisition](https://www.sciencedirect.com/science/article/abs/pii/0010027794900345). In that book, several researchers argued in favor or against each authors view. Among those contributors was [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) who mentioned Rosenblatt's perceptron in his article, which inspired LeCun to learn about neural networks for the first time. Ironically, this was the same Seymour Papert that published the book (along with Marvin Minsky) that broguht the demise on the interest on neural networks in the late '60s. I don't belive in karma, but this certainly looks like it. \n",
    "\n",
    "Eventually, LeCun became postdoc at the University of Toronto with Geoffrey Hinton and started to prototype the first convolutional network. By the late '80s LeCun was working at [Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs) in New Jersey, place where he and his colleagues developed at published the **first convolutional neural network trained with backpropagation**, the **\"LeNet\"**, that could effectively recognize handwritten zip codes from US post office. This early convolutional network went through several rounds of modifications and improvements (LeNet-1, LeNet-2, etc.), until by 1998 the [LeNet-5](http://yann.lecun.com/exdb/lenet/) reached test error rates of 0.95% in the [MNIST dataset of handwritten digits](http://yann.lecun.com/exdb/mnist/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The convolution opertion: feature detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll begin by schematically describing the LeNet-5 model and leave the mathematics for the next section. This conceptual explanation should be enough to have a higher-level understanding of the model but not necesarilly to implement a convolutional network. For this understanding the mathematics is fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: LeNet-5 Architecture <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/LeNet.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general architecture of the LeNet-5 is shown in **Figure X**. The input layer $L-0$ acts like the retina receiving images of characters that are centered and size-normalized (otherwise, some images may not fit the in the input layer). The next layer $L-1$ is composed of several **feature maps**, which have the same role that the Neocognitron simple-cells: to extract simple features as oriented edges, corners, end-points, etc. In practice, a feature map is a squared matrix of **identical weights**.  Weights *within* a feature map pattern need to be identical so they can detect *the same* local feature in the input image. Weights *between* feature maps patterns are different so they can detect *different* local features. Each unit in a feature map has a **receptive field**. This is, a small $n \\times n$ sub-area of the input image that can be \"perceived\" by a unit in the feature map at any given time.  \n",
    "\n",
    "Feature maps and receptive fields sounds complicated. Here is a methaphor thay may be helpful: imagine that you have 6 flashlights with a *square* beam of light. Each flashlight has the special quality of revealing certain \"features\" of images drawn with invisible ink, like corners or oriented edges. Also imagine that you have a set of images that were drawn witn invisible ink. Now, you need your special flashlights to reveal the hidden character in the image. What you need to do is to carefully iluminate each section of the invisible image, from *right to left and top to bottom*, with each of your 6 flashlights. Once you finish the process, you should be able to put together all the little \"features\" revealed by each flashlight to compose the full image shape. Here, the square beam of light sliding over each pixel represents the aforementioned *receptive field*, and each flashlight represents a *feature map*. \n",
    "\n",
    "**Figure X** shows a represention of the feature detection process (assuming that each time a pixel in the input image *matchs* a pixel in the feature detector we add a value of 1, athough in practice it can be any real-valued scalar). In this example we use a **stride** of 1, meaning that we shift the receptive field by 1 pixel (to the right or down) for each cell in the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Feature detection (convolution) <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/convolution.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of sliding over the image with the receptive field (sometimes called *kernels*) of feature maps equals to a mathematical operation called **convolution**, hence the name **convolutional network**. The full convolution operation involves repeating the process in **Figure X** for each feature map. If you are wondering how do you come up with appropiated features detectors, the answer is that you don't need to: *the feature maps weights are learned in the training process*. More on the mathematics of this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pooling operation: subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the convolution operation is done, what we have learned is whether a feature is present in the image or not. Now, knowing that a collection of features is present in an imagine won't tell us, by itself, which image they correspond to. What we need to know is their **approximate position relative to each other**. For instance, if we know that we have a \"curvy horizontal line\" at the center-bottom, a \"curvy vertical line\" at the middle-right, a \"stright vertical line\" at upper-left, and a \"stright horizontal line\" at the center-top, we should be able to tell we have a \"5\". This is even more important considering that real-life images like handwritten numbers, have considerable *variability* in their shape. No two individuals write numbers in the exact same manner. Hence, we want our network to be as *insensitive as possible* to the absolute position of a feature, and as *sensitive as possible* to its relative position. This is sometimes refered as **invariance to local translation**. One way to accomplish this is by **reducing the spacial resolution of the image**. This is what **sub-sampling** or **pooling** does. \n",
    "\n",
    "The are many ways to sub-sample an image. In the LeNet-5, this operation performs a **local averaging** of a section of the feature map, effectively *reducing the resolution* of the feature map as a whole, and the sensitivity of the network to shifts and distorsions in the input image. A colloquial example is what happens when you \"pixelate\" an image, like in **Figure X**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: sub-sampling effect <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/pixelated.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sub-sampling layer will have as many \"pixelated\" feature maps as \"normal\" feature maps in the convolutional layer. The **mechanics of sub-sampling** are as follow: again, we have $n \\times n$ receptive field that \"perceives\" a section of the \"normal\" feature map and connect to a unit in the \"pixelated\" feature map. This time, there is no overlap between each \"stride\" of the receptive field: each unit is connected to a *non-overlapping section* of the original feature map. You can think on this as taking \"strides\" of a size equal to $n$, e.g., for a $3 \\times 3$ feature map, we take a stride of $3$. Then, we take weighted average of each pixel in the receptive field, and pass the resulting sum through a sigmoid function (or any other non-linear function). The *weights* in the weigthed average are also parameters that the network learns over training. **Figure X** shows this process for a *single* sub-sampled feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: Sub-sampling (pooling)<\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/pooling.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of sub-sampling is another grid of numbers (note that the numbers in **Figure X** are made up). We went from a $12 \\times 12$ input image, to a $3 \\times 3$ feature map after convolution and pooling (keep in mind that I intentionally reduced LeNet-5 original dimnensions to simplify the examples). Since in our original example we had 6 features map, we need to repeat the process in **Figure X** 6 times, one of each feature map.\n",
    "\n",
    "The next sub-sampling hidden layer $S_2$ increases the number of feature maps compared to $S_1$. If you were to add more sets of $S_n$ and $C_n$ hidden layers, you will repeat this altenating pattern again: *as the spatial resolution is reduced (by pooling), the number of feature maps in the next layer is increased*. The idea here is to **compensate the reduction in spatial resolution by increasing the richness of the learned representations** (i.e., more feature maps).\n",
    "\n",
    "Once we are done with the sequence of convolution and pooling, the network implements a traditional fully-connected layer as in the [multi-layer perceptron](https://com-cog-book.github.io/com-cog-book/features/multilayer-perceptron.html). The first fully-connected $F_1$ layer has the role of **\"flattening\"** the $C_2$ pooling layer. Remember that fully-connected layers take a one-dimensional input vector, and the dimensions of the LeNet-5 $C_2$ layer are  $5 \\times 5 \\times 16$, this is, sixteen 5 by 5 feature maps. The dimensionality of the first  fully-connected layer is simply $5 \\times 5 \\times 16 = 120$. The next one hidden layer $F_2$ \"compress\" the output even further into a vector of size $84$. Finally we have the **output-layer** the implements a **softmax function** with 10 neurons to perform the classification of numbers (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 perfomance in the MNIST dataset was impressive but not out of the ordinary. Others methods like the Suport Vector Machines could reach [similar or better perfomance at the time](http://yann.lecun.com/exdb/mnist/). Training neural networks was still costly and complicatd compared to others machine learning techniques, hence the interest in neural nets faded in the late '90s again. However, serveral research group continued to work in neural networks. The next big breaktrough in computer vision came in 2012  when Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton introduced the [\"AlexNet\"](https://en.wikipedia.org/wiki/AlexNet), a convolutional neural network that won the [\"ImageNet Large Scale Visual Recognition Challenge\"](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) for a wide margin, surprising the entire computer vision community.\n",
    "\n",
    "The main innovation introduced by AlexNet compared to the LeNet-5 was **its sheer size**. AlexNet main elements are the same: a sequence of convolutional and pooling layers with a couple of dense layers at the end. The LeNet-5 has two sets of convolutional and pooling layers, two fully-connected layers, and softmax classifier at the end. AlexNet has five convolutional layers, three pooling layers, three fully-connected layers, and the softmax classifier layer. The training time and dataset was larger as well. All of this was possible because of the availability of more raw computational processing (particularly thanks too [Graphics Processing Units (GPUs)](https://en.wikipedia.org/wiki/Graphics_processing_unit), and larger datasets (because of the internet). diagramming AlexNet is complicated because the architecture its too large. I'll use simplified notation to describe AlexNet and compare it with LeNet-5, as shown in **Figure X**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X: AlexNet and LeNet architectures <\\center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/alexnet.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer in AlexNet is three dimensional because it was designed to classify 1000 color images (LeNet-5 classified 10 grey-scale digits). The dimensions represent *width x height x RGB* (red, green, and blue) color values. This type of 3-D arrays of numbers are often refered as [mathematical tensors](https://en.wikipedia.org/wiki/Tensor). The pooling operation is done by taking the maximun value in the receptive field instead of the average of all units, which is known as **max pooling**. The pattern of connectivity between convolutional and pooling layers is different to the one in LeNet-5 too. Other than that, AlexNet utilize the same building blocks and operations than LeNet-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network models of vision and computer vision drifting apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask to a random researcher in computer vision about the correspondene betwen the human visual/perceptual system and convolutional nets, the most likely answer would be something like: \"*Well, CNNs are roughly inspired in the brain but aren't actual models of the brain. I care about solving the problem artificial vision by any means necessary, regardless of the biological correspondance to human vision, more or less in the same manner we solved flying withouth having to imitate birds flapping*\". Or some version of that. This talks to how **computer vision has become an independent area of research with its own goals**. Most researchers are fully aware that many of the design properties of modern convolutional nets are not biologically realistic. Beyond the parallels with human vision, strictly speaking, LeNet-5 and AlexNet are designed to maximize object-recognition perfomance, not biological-realism. And that's is perfectly fine. For instance, the LeNet 5 paper (1998) was published in the context of the debate between traditional pattern recognition with handcrafted features vs the automated learning-based approach of neural nets. Nothing was said about human perception. However, from our perspective, the issue of **whether convolutional nets are useful model of human perception and vision** is critical. This is a open debate. Many researchers do beleive that convolutionls nets are useful models for human vision and perception, and there is a long list of [scientific articles trying to show this point](https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01544). I won't review those arguments now. My point is to highlight the fact that what I'll describe next are \"coarse\" models attempting to approximate human abilities in narrow settings, not full-blown models of human vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll describe the mathematics for the LeNet-5. This architecture of LeNet-5 can be described by the following:\n",
    "\n",
    "- convolution function (for hidden layer) \n",
    "- non-linear transformation function (for hidden layer)\n",
    "- pooling function (for hidden layer)\n",
    "- linear function (for fully-connected layers)\n",
    "- euclidean radial basis (RBF) function (for the output)\n",
    "- cost function (to compute overall error)\n",
    "- learning procedure (i.e., backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution operation *convolutes* pairs of functions. Here I'm using the plain meaning of \"convoluted\": to interwine or twist things together. In the neural network context, the functions we convolute together are the **input function** $P$ and the **kernel function** $K$ (remember that *kernel* is another way to call the *receptive field* of a feature map). For the 2-dimensional inputs as in LeNet-5, the $P_{ij}$ function contains the 2-dimensional values for the input image, which in our case are grayscale values between 0 (white) and 255 (black). The $K_{mn}$ function contains the 2-dimensional values for the kernel, this is, the matrix of weights $W_{mn}$ to be learned by the network. The **output** of the convolution is the feature map $F_{mn}$ in the next layer. In practice, the convolution operation is a **convoluted linear operation**, i.e., a weighted sum.\n",
    "\n",
    "The convolution formula has different forms depending on the context. In the neural network context, we will compute a **discrete convolution**. The **convolution operator** is conventionally represented by the **$\\bf{*}$ symbol**. Hence, we define the convolution between $P$ and $K$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_{mn}= S(i,j) = (P*K)_{ij} =\\sum_m\\sum_nP_{i-m, j-n} * K_{m,n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where ${ij}$ are the width and length of the input image, and ${mn}$ are the width and length of the kernel.\n",
    "\n",
    "Technically, LeNet-5 also adds a \"bias\" $b$ to each convolution, so the full expression becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_{mn}= S(i,j) = b + (P*K)_{ij}  = b + \\sum_m\\sum_nP_{i-m, j-n} * K_{m,n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll ignore the $b$ term since it's not part of a convolution operation and it's not relevant for its explanation.\n",
    "\n",
    "To apply a convolution the most important part is **to get the indices right**. We will work with a $3 \\times 3$ input image example. There are multiple index conventions floating around the internet. I will use the following for convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P=\n",
    "\\begin{bmatrix}\n",
    "p_{-1,1} & p_{0,1} & p_{1,1} \\\\\n",
    "p_{-1,0} & p_{0,0} & p_{1,0} \\\\\n",
    "p_{-1,-1} & p_{0,-1} & p_{1,-1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a $2 \\times 2$ kernel that can't be centered at $0$, we will fix the bottom-left entry at $(0,0)$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K=\n",
    "\\begin{bmatrix}\n",
    "k_{0,1} & k_{1,1}\\\\\n",
    "k_{0,0} & k_{1,0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a trick to make the convolution operation clearer. I'll replace the entries with actual numbers and overlay the matrices in a cartesian plane as in **Figure X**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/cartesian-matrix.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that we want to compute a feature map $F$ with dimensions equalt to $K$. Consequently, we need to compute 4 convolutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_{00} = S(i=0,j=0) \\\\\n",
    "F_{01} = S(i=0,j=1) \\\\\n",
    "F_{10} = S(i=1,j=0) \\\\\n",
    "F_{11} = S(i=1,j=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "f_{0,1} & f_{1,1}\\\\\n",
    "f_{0,0} & f_{1,0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute $F_{00} = S(i=0,j=0)$. Table X shows the entries to be multiplied and added together when we follow the double summtion $\\sum_m\\sum_nP_{i-m, j-n} * K_{m,n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Table X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| i | j | m | n | i-m    | j-n    | P<sub>i-m,j-n</sub> | K<sub>m,n</sub> |\n",
    "|---|---|---|---|--------|--------|---------------------|-----------------|\n",
    "| 0 | 0 | 0 | 0 | 0-0=0  | 0-0=0  | P<sub>0,0</sub>     | K<sub>0,0</sub> |\n",
    "| 0 | 0 | 0 | 1 | 0-0=0  | 0-1=-1 | P<sub>0,-1</sub>    | K<sub>0,1</sub> |\n",
    "| 0 | 0 | 1 | 0 | 0-1=-1 | 0-0=0  | P<sub>-1,0</sub>    | K<sub>1,0</sub> |\n",
    "| 0 | 0 | 1 | 1 | 0-1=-1 | 0-1=-1 | P<sub>-1,-1</sub>   | K<sub>1,1</sub> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will color-code each entry in the matrices and compute the value for $F_{00}$ as in **Figure X**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/cartesian-matrix-1.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill in with the first $F$ entry (here is where you could add $b$ to the summation result):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "f_{0,1} & f_{1,1}\\\\\n",
    "142.5 & f_{1,0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe the indices carefully, you'll notice the $P$ indices are $K$ indices \"flipped\". Taking $-K$ (i.e., $-m,-n$) **reflects** the indices on the horizontal and vertical axes, whereas $j,i$ **offset** the indices on their correspondign axes. In the last example there was no offset becasue both $j,i$ equal $0$. **Figure X** shows the effects of reflecting by taking $-k$ and offsetting by different values of $j,i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/cartesian-matrix-rotations.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we compute the next feature map entry $F_{01}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| i | j | m | n | i-m    | j-n   | P<sub>i-m,j-n</sub> | K<sub>m,n</sub> |\n",
    "|---|---|---|---|--------|-------|---------------------|-----------------|\n",
    "| 0 | 1 | 0 | 0 | 0-0=0  | 1-0=1 | P<sub>0,1</sub>     | K<sub>0,0</sub> |\n",
    "| 0 | 1 | 0 | 1 | 0-0=0  | 1-1=0 | P<sub>0,0</sub>     | K<sub>0,1</sub> |\n",
    "| 0 | 1 | 1 | 0 | 0-1=-1 | 1-0=1 | P<sub>-1,1</sub>    | K<sub>1,0</sub> |\n",
    "| 0 | 1 | 1 | 1 | 0-1=-1 | 1-1=0 | P<sub>-1,0</sub>    | K<sub>1,1</sub> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, this looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/cartesian-matrix-2.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill in with the second entry on $F$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "195 & f_{1,1}\\\\\n",
    "142.5 & f_{1,0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern here is always the same: flipp, offset, overlay, multiply, and add. If we follow the pattern for $F_{11}$ and $F_{10}$, $F$ results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "195 & 241\\\\\n",
    "142.5 & 222\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution in practice: cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may come as a surprise to you but in practice several deep learning libraries like [MXNet](https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.Convolution.html) and [Pytorch](https://pytorch.org/docs/stable/nn.html#convolution-layers) **DO NOT implement convolutions** but a closely related operation called **cross-correlation** (although the authors insist on calling it convolution). The cross-corrleation operation is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_{mn}= S(i,j) = (P \\star K)_{ij} =\\sum_m\\sum_nP_{i+m, j+n} \\star K_{m,n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay close attention to $K_{i+m, j+n}$ you'll see that the only difference is we are replacing the $-$ symbol (in the convolution operation) by $+$ symbol. \n",
    "\n",
    "Now, If we keep the same convention of centering the input image at zero, we will get into trouble. For instance, **Table X** shows the values for $i=0, j=1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Table X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| i | j | m | n | i+m   | j+n   | P<sub>i+m,j+n</sub> | K<sub>m,n</sub> |\n",
    "|---|---|---|---|-------|-------|---------------------|-----------------|\n",
    "| 0 | 1 | 0 | 0 | 0+0=0 | 1+0=1 | P<sub>0,1</sub>     | K<sub>0,0</sub> |\n",
    "| 0 | 1 | 0 | 1 | 0+0=0 | 1+1=2 | P<sub>0,2</sub>     | K<sub>0,1</sub> |\n",
    "| 0 | 1 | 1 | 0 | 0+1=1 | 1+0=1 | P<sub>1,1</sub>     | K<sub>1,0</sub> |\n",
    "| 0 | 1 | 1 | 1 | 0+1=1 | 1+1=0 | P<sub>2,0</sub>     | K<sub>1,1</sub> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get $P_{02} \\star K_{01}$, which does not make sense since we don't have values at $P_{02}$. One way to address this is by \"padding\" the input image with zeros like: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P=\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & p_{-1,1} & p_{0,1} & p_{1,1} & 0 \\\\\n",
    "0 & p_{-1,0} & p_{0,0} & p_{1,0} & 0\\\\\n",
    "0 & p_{-1,-1} & p_{0,-1} & p_{1,-1} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have values at $P_{02}$. I personally find such solution sketchy. A better aproach would it be to change the indices of $P$ to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P=\n",
    "\\begin{bmatrix}\n",
    "p_{0,2} & p_{1,2} & p_{2,2} \\\\\n",
    "p_{0,1} & p_{1,1} & p_{2,1} \\\\\n",
    "p_{0,0} & p_{1,0} & p_{2,0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have values at $P_{02}$ and no padding is nedded. If you iterate over $\\sum_m\\sum_nP_{i+m, j+n} \\star K_{m,n}$ the indices will work just fine to obtain $F$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in cross-correlation there aren't reflections just offsets. This is how the offsets look now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure X <\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cov-net/cartesian-matrix-cross.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll not compute the cross-correlation values. The computation is as simple as overlaying the kernel matrix $K$ on top of the $P$ input matrix and take a weighted sum. This is the reason why you'll see most text in deep learning explain convolutions as \"sliding\" the kernel over the image taking a stride of X. Essentially, cross-correlation is a **measure of similarity** between the kernel and the input image. The better the alignment, the higher the cross-correlation value.\n",
    "\n",
    "We addressed convolution and cross-correlation in 2 dimensions. Yet, keep in mind that you can use such techniques for problems in 1, 2, 3 or N dimensions, and the mathematics extend naturally to such dimensions as well. \n",
    "\n",
    "Before moving to the next section I want to address two questions: **Why to bother with convolutions (or cross-correlation) at all?**, and **Does it matter if I use convolution or cross-correlation?**\n",
    "\n",
    "Regarding the first question, technically, we could \"flatten\" the image into a long vector and use a traditional multi-layer perceptron to classify the images. Just imagine aligning each pixel $[p_{00}, p_{01}, ..., p_{ji}]$ one after the other in a long list. This is way simpler than bothering with convolution or cross-correlation. The short answer is: because the **topological information matters**. In other words, the relative position of each pixel in the grid matters, and we want to exploit such space-dependence when learning the network weights. If you train a model to recognize faces, the relative distance between *your ayes* and your *mouth* matters. Knowing that the images has a pair of eyes and a mouth is not enough. Therefore, we want to keep the grid-structure when training the network.\n",
    "\n",
    "Regarding the second question, the answer is: **no, it does not matter**. Well, it does matter in terms of the implementation, computational complexity, etc. But it does not matter in terms of finding a solution. The training algorithm will **ensure that the appropiate weights are learn** regardless. In short, when you use *convolution* the network will learn a matrix of weights $W$. If you use *cross-correlation* instead, the network will learn a set of weights $-W$, i.e., the weights \"flipped\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After convolution, we \"pool\" the feature map output. \"Pooling\" essentially means to compute a summary statistic of a selected region of pixels. There are several pooling modalities: max pooling, average pooling, $L^2$ norm pooling, and others. \n",
    "\n",
    "LeNet-5 implements **average pooling**, which is simply the average of the selected region in the feature map. For instance, if we take $F$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F=\n",
    "\\begin{bmatrix}\n",
    "195 & 241\\\\\n",
    "142.5 & 222\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M_{mn} = \\frac{1}{m \\times n} \\sum_m\\sum_nf_{mn} = \\frac{1}{2*2} (142.5 + 222 + 195 + 241) = \\frac{1}{4} (800.5) = 200.125\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet implements **max pooling**, which is simply the largest value in the selected region of the feature map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M_{mn} = max(F) = 222\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to say about pooling. For are thorough evaluation in the field of object recognition see [Scherer, Muller, and Behnke (2011)](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear transformation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 implements two non-linear functions: a **sigmoid** after the convolution layer and a **scaled hyperbolic tanget** after the pooling layer. Lecun et all call the latter \"squashing\" function.\n",
    "\n",
    "The **sigmoid** function is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a = \\sigma(f_{mn}) = \\frac{1}{1+e^{-f_{mn}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid has an $S$ shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-59c632b7d0184a318afe2bb5589fc712\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-59c632b7d0184a318afe2bb5589fc712\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"a\"}}}, {\"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"a1\"}}}], \"data\": {\"name\": \"data-4ce8df9580f59e633b8d399e9190ac6d\"}, \"title\": \"Chart 1\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-4ce8df9580f59e633b8d399e9190ac6d\": [{\"a\": 0.0066928509242848554, \"z\": -5.0, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.007391541344281971, \"z\": -4.9, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.00816257115315989, \"z\": -4.800000000000001, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.009013298652847815, \"z\": -4.700000000000001, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.009951801866904308, \"z\": -4.600000000000001, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.010986942630593162, \"z\": -4.500000000000002, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.012128434984274213, \"z\": -4.400000000000002, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.013386917827664744, \"z\": -4.3000000000000025, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.014774031693273017, \"z\": -4.200000000000003, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.01630249937144089, \"z\": -4.100000000000003, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.017986209962091496, \"z\": -4.0000000000000036, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.01984030573407743, \"z\": -3.900000000000004, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.021881270936130383, \"z\": -3.8000000000000043, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.024127021417669092, \"z\": -3.7000000000000046, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.026596993576865725, \"z\": -3.600000000000005, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.02931223075135617, \"z\": -3.5000000000000053, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.03229546469845033, \"z\": -3.4000000000000057, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.035571189272635965, \"z\": -3.300000000000006, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.03916572279676412, \"z\": -3.2000000000000064, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.043107254941085846, \"z\": -3.1000000000000068, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.04742587317756646, \"z\": -3.000000000000007, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.05215356307841737, \"z\": -2.9000000000000075, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.05732417589886832, \"z\": -2.800000000000008, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.06297335605699601, \"z\": -2.700000000000008, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.06913842034334627, \"z\": -2.6000000000000085, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.07585818002124294, \"z\": -2.500000000000009, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.08317269649392166, \"z\": -2.4000000000000092, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.09112296101485534, \"z\": -2.3000000000000096, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.09975048911968425, \"z\": -2.20000000000001, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.10909682119561194, \"z\": -2.1000000000000103, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.11920292202211644, \"z\": -2.0000000000000107, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.1301084743629966, \"z\": -1.900000000000011, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.1418510649004864, \"z\": -1.8000000000000114, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.15446526508353317, \"z\": -1.7000000000000117, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.16798161486607383, \"z\": -1.600000000000012, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.1824255238063545, \"z\": -1.5000000000000124, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.19781611144141623, \"z\": -1.4000000000000128, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.21416501695743917, \"z\": -1.3000000000000131, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.23147521650098, \"z\": -1.2000000000000135, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.24973989440487981, \"z\": -1.1000000000000139, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.26894142136999233, \"z\": -1.0000000000000142, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.28905049737499305, \"z\": -0.9000000000000146, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.3100255188723844, \"z\": -0.8000000000000149, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.3318122278318305, \"z\": -0.7000000000000153, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.35434369377420094, \"z\": -0.6000000000000156, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.3775406687981417, \"z\": -0.500000000000016, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.40131233988754406, \"z\": -0.40000000000001634, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.4255574831883369, \"z\": -0.3000000000000167, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.45016600268751783, \"z\": -0.20000000000001705, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.47502081252105566, \"z\": -0.10000000000001741, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.49999999999999556, \"z\": -1.7763568394002505e-14, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.5249791874789355, \"z\": 0.09999999999998188, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.5498339973124733, \"z\": 0.19999999999998153, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.5744425168116544, \"z\": 0.29999999999998117, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.5986876601124473, \"z\": 0.3999999999999808, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.62245933120185, \"z\": 0.49999999999998046, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.6456563062257908, \"z\": 0.5999999999999801, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.6681877721681616, \"z\": 0.6999999999999797, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.6899744811276081, \"z\": 0.7999999999999794, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.7109495026249997, \"z\": 0.899999999999979, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.7310585786300007, \"z\": 0.9999999999999787, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.7502601055951135, \"z\": 1.0999999999999783, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.7685247834990137, \"z\": 1.199999999999978, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.7858349830425548, \"z\": 1.2999999999999776, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8021838885585781, \"z\": 1.3999999999999773, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8175744761936402, \"z\": 1.499999999999977, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8320183851339212, \"z\": 1.5999999999999766, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8455347349164622, \"z\": 1.6999999999999762, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8581489350995093, \"z\": 1.7999999999999758, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8698915256369995, \"z\": 1.8999999999999755, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8807970779778798, \"z\": 1.9999999999999751, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.8909031788043846, \"z\": 2.0999999999999748, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9002495108803125, \"z\": 2.1999999999999744, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9088770389851418, \"z\": 2.299999999999974, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9168273035060757, \"z\": 2.3999999999999737, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9241418199787546, \"z\": 2.4999999999999734, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9308615796566515, \"z\": 2.599999999999973, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.937026643943002, \"z\": 2.6999999999999726, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9426758241011297, \"z\": 2.7999999999999723, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.947846436921581, \"z\": 2.899999999999972, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9525741268224319, \"z\": 2.9999999999999716, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9568927450589126, \"z\": 3.0999999999999712, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9608342772032344, \"z\": 3.199999999999971, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9644288107273629, \"z\": 3.2999999999999705, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9677045353015485, \"z\": 3.39999999999997, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9706877692486428, \"z\": 3.49999999999997, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9734030064231335, \"z\": 3.5999999999999694, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.97587297858233, \"z\": 3.699999999999969, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9781187290638689, \"z\": 3.7999999999999687, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9801596942659219, \"z\": 3.8999999999999684, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.982013790037908, \"z\": 3.999999999999968, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9836975006285584, \"z\": 4.099999999999968, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9852259683067265, \"z\": 4.199999999999967, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9866130821723347, \"z\": 4.299999999999967, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9878715650157253, \"z\": 4.399999999999967, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9890130573694065, \"z\": 4.499999999999966, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9900481981330953, \"z\": 4.599999999999966, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9909867013471519, \"z\": 4.6999999999999655, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9918374288468399, \"z\": 4.799999999999965, \"z1\": 0, \"a1\": 0.5}, {\"a\": 0.9926084586557179, \"z\": 4.899999999999965, \"z1\": 0, \"a1\": 0.5}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "z = np.arange(-5.0,5.0, 0.1)\n",
    "a = expit(z)\n",
    "df = pd.DataFrame({\"a\":a, \"z\":z})\n",
    "df[\"z1\"] = 0\n",
    "df[\"a1\"] = 0.5\n",
    "sigmoid = alt.Chart(df).mark_line().encode(x=\"z\", y=\"a\")\n",
    "threshold = alt.Chart(df).mark_rule(color=\"red\").encode(x=\"z1\", y=\"a1\")\n",
    "(sigmoid + threshold).properties(title='Chart 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **scaled hyperbolic tangent function** is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tau(f_{mn}) = A \\times \\frac{(e^{f_{mn}} - e^{f_{mn}})}{(e^{f_{mn}} + e^{f_{mn}})}    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With:\n",
    "$$\n",
    "A = 1.7159\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the scaling factor can be omitted with no relevant side effects, as it's done today. \n",
    "\n",
    "The shape of the scaled tanh function is similar to the sigmoid but the tanh can take negative values and the threshold point is at $0,0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c6bce98ecf654cb3a06bbd2e22cf9f21\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-c6bce98ecf654cb3a06bbd2e22cf9f21\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"t\"}}}, {\"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"t1\"}}}], \"data\": {\"name\": \"data-26785d68b27a022afca9cca44793e3af\"}, \"title\": \"Chart 2\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-26785d68b27a022afca9cca44793e3af\": [{\"t\": -0.9999092042625951, \"z\": -5.0, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9998891029505544, \"z\": -4.9, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9998645517007605, \"z\": -4.800000000000001, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9998345655542966, \"z\": -4.700000000000001, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9997979416121845, \"z\": -4.600000000000001, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9997532108480275, \"z\": -4.500000000000002, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9996985792838805, \"z\": -4.400000000000002, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9996318561900731, \"z\": -4.3000000000000025, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9995503664595334, \"z\": -4.200000000000003, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9994508436877974, \"z\": -4.100000000000003, \"z1\": 0, \"t1\": -1}, {\"t\": -0.999329299739067, \"z\": -4.0000000000000036, \"z1\": 0, \"t1\": -1}, {\"t\": -0.999180865670028, \"z\": -3.900000000000004, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9989995977858409, \"z\": -3.8000000000000043, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9987782412811312, \"z\": -3.7000000000000046, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9985079423323266, \"z\": -3.600000000000005, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9981778976111987, \"z\": -3.5000000000000053, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9977749279342794, \"z\": -3.4000000000000057, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9972829600991421, \"z\": -3.300000000000006, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9966823978396512, \"z\": -3.2000000000000064, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9959493592219003, \"z\": -3.1000000000000068, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9950547536867306, \"z\": -3.000000000000007, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9939631673505832, \"z\": -2.9000000000000075, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9926315202011281, \"z\": -2.800000000000008, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9910074536781178, \"z\": -2.700000000000008, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9890274022010994, \"z\": -2.6000000000000085, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9866142981514305, \"z\": -2.500000000000009, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9836748576936805, \"z\": -2.4000000000000092, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9800963962661917, \"z\": -2.3000000000000096, \"z1\": 0, \"t1\": -1}, {\"t\": -0.975743130031452, \"z\": -2.20000000000001, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9704519366134545, \"z\": -2.1000000000000103, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9640275800758177, \"z\": -2.0000000000000107, \"z1\": 0, \"t1\": -1}, {\"t\": -0.95623745812774, \"z\": -1.900000000000011, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9468060128462694, \"z\": -1.8000000000000114, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9354090706031004, \"z\": -1.7000000000000117, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9216685544064731, \"z\": -1.600000000000012, \"z1\": 0, \"t1\": -1}, {\"t\": -0.9051482536448687, \"z\": -1.5000000000000124, \"z1\": 0, \"t1\": -1}, {\"t\": -0.8853516482022653, \"z\": -1.4000000000000128, \"z1\": 0, \"t1\": -1}, {\"t\": -0.8617231593133098, \"z\": -1.3000000000000131, \"z1\": 0, \"t1\": -1}, {\"t\": -0.8336546070121593, \"z\": -1.2000000000000135, \"z1\": 0, \"t1\": -1}, {\"t\": -0.8004990217606347, \"z\": -1.1000000000000139, \"z1\": 0, \"t1\": -1}, {\"t\": -0.7615941559557708, \"z\": -1.0000000000000142, \"z1\": 0, \"t1\": -1}, {\"t\": -0.7162978701990315, \"z\": -0.9000000000000146, \"z1\": 0, \"t1\": -1}, {\"t\": -0.6640367702678572, \"z\": -0.8000000000000149, \"z1\": 0, \"t1\": -1}, {\"t\": -0.6043677771171733, \"z\": -0.7000000000000153, \"z1\": 0, \"t1\": -1}, {\"t\": -0.5370495669980464, \"z\": -0.6000000000000156, \"z1\": 0, \"t1\": -1}, {\"t\": -0.46211715726002234, \"z\": -0.500000000000016, \"z1\": 0, \"t1\": -1}, {\"t\": -0.3799489622552389, \"z\": -0.40000000000001634, \"z1\": 0, \"t1\": -1}, {\"t\": -0.29131261245160617, \"z\": -0.3000000000000167, \"z1\": 0, \"t1\": -1}, {\"t\": -0.19737532022492038, \"z\": -0.20000000000001705, \"z1\": 0, \"t1\": -1}, {\"t\": -0.09966799462497306, \"z\": -0.10000000000001741, \"z1\": 0, \"t1\": -1}, {\"t\": -1.7763568394002505e-14, \"z\": -1.7763568394002505e-14, \"z1\": 0, \"t1\": -1}, {\"t\": 0.09966799462493789, \"z\": 0.09999999999998188, \"z1\": 0, \"t1\": -1}, {\"t\": 0.19737532022488624, \"z\": 0.19999999999998153, \"z1\": 0, \"t1\": -1}, {\"t\": 0.2913126124515737, \"z\": 0.29999999999998117, \"z1\": 0, \"t1\": -1}, {\"t\": 0.3799489622552084, \"z\": 0.3999999999999808, \"z1\": 0, \"t1\": -1}, {\"t\": 0.4621171572599943, \"z\": 0.49999999999998046, \"z1\": 0, \"t1\": -1}, {\"t\": 0.5370495669980211, \"z\": 0.5999999999999801, \"z1\": 0, \"t1\": -1}, {\"t\": 0.6043677771171506, \"z\": 0.6999999999999797, \"z1\": 0, \"t1\": -1}, {\"t\": 0.6640367702678375, \"z\": 0.7999999999999794, \"z1\": 0, \"t1\": -1}, {\"t\": 0.7162978701990141, \"z\": 0.899999999999979, \"z1\": 0, \"t1\": -1}, {\"t\": 0.7615941559557559, \"z\": 0.9999999999999787, \"z1\": 0, \"t1\": -1}, {\"t\": 0.8004990217606219, \"z\": 1.0999999999999783, \"z1\": 0, \"t1\": -1}, {\"t\": 0.8336546070121486, \"z\": 1.199999999999978, \"z1\": 0, \"t1\": -1}, {\"t\": 0.8617231593133006, \"z\": 1.2999999999999776, \"z1\": 0, \"t1\": -1}, {\"t\": 0.8853516482022576, \"z\": 1.3999999999999773, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9051482536448623, \"z\": 1.499999999999977, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9216685544064678, \"z\": 1.5999999999999766, \"z1\": 0, \"t1\": -1}, {\"t\": 0.935409070603096, \"z\": 1.6999999999999762, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9468060128462658, \"z\": 1.7999999999999758, \"z1\": 0, \"t1\": -1}, {\"t\": 0.956237458127737, \"z\": 1.8999999999999755, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9640275800758151, \"z\": 1.9999999999999751, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9704519366134524, \"z\": 2.0999999999999748, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9757431300314503, \"z\": 2.1999999999999744, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9800963962661904, \"z\": 2.299999999999974, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9836748576936793, \"z\": 2.3999999999999737, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9866142981514295, \"z\": 2.4999999999999734, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9890274022010986, \"z\": 2.599999999999973, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9910074536781172, \"z\": 2.6999999999999726, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9926315202011277, \"z\": 2.7999999999999723, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9939631673505828, \"z\": 2.899999999999972, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9950547536867301, \"z\": 2.9999999999999716, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9959493592219, \"z\": 3.0999999999999712, \"z1\": 0, \"t1\": -1}, {\"t\": 0.996682397839651, \"z\": 3.199999999999971, \"z1\": 0, \"t1\": -1}, {\"t\": 0.997282960099142, \"z\": 3.2999999999999705, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9977749279342792, \"z\": 3.39999999999997, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9981778976111986, \"z\": 3.49999999999997, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9985079423323265, \"z\": 3.5999999999999694, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9987782412811311, \"z\": 3.699999999999969, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9989995977858408, \"z\": 3.7999999999999687, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9991808656700278, \"z\": 3.8999999999999684, \"z1\": 0, \"t1\": -1}, {\"t\": 0.999329299739067, \"z\": 3.999999999999968, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9994508436877974, \"z\": 4.099999999999968, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9995503664595334, \"z\": 4.199999999999967, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9996318561900731, \"z\": 4.299999999999967, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9996985792838805, \"z\": 4.399999999999967, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9997532108480275, \"z\": 4.499999999999966, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9997979416121845, \"z\": 4.599999999999966, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9998345655542966, \"z\": 4.6999999999999655, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9998645517007605, \"z\": 4.799999999999965, \"z1\": 0, \"t1\": -1}, {\"t\": 0.9998891029505544, \"z\": 4.899999999999965, \"z1\": 0, \"t1\": -1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "z = np.arange(-5.0,5.0, 0.1)\n",
    "t = np.tanh(z)\n",
    "df = pd.DataFrame({\"t\":t, \"z\":z})\n",
    "df[\"z1\"] = 0\n",
    "df[\"t1\"] = -1\n",
    "tanh = alt.Chart(df).mark_line().encode(x=\"z\", y=\"t\")\n",
    "threshold = alt.Chart(df).mark_rule(color=\"red\").encode(x=\"z1\", y=\"t1\")\n",
    "(tanh + threshold).properties(title='Chart 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason to use this \"squashing\" function is to help *learning convergence*. Basically, symmetric functions are beleived to converge faster. For a more complete explanation see *Appendix A* in LeCun et all (1998).\n",
    "\n",
    "Nowadays, a more popular non-linear function is the **ReLU**, or rectifier linear unit. This is what AlexNet uses. The ReLu simply returns the positive side of the input function, or zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r (x) = max(0,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the ReLU is a stright line in the negative side of the number line, and linear function in the positive side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-e848fde3d7974f9ab5343d1c11484d42\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-e848fde3d7974f9ab5343d1c11484d42\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"r\"}}}, {\"mark\": {\"type\": \"rule\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"z1\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"r1\"}}}], \"data\": {\"name\": \"data-61d1aa4674fe2b4cb14b2351f7bb5df7\"}, \"title\": \"Chart 2\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-61d1aa4674fe2b4cb14b2351f7bb5df7\": [{\"r\": 0.0, \"z\": -5.0, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.9, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.800000000000001, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.700000000000001, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.600000000000001, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.500000000000002, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.400000000000002, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.3000000000000025, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.200000000000003, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.100000000000003, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -4.0000000000000036, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.900000000000004, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.8000000000000043, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.7000000000000046, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.600000000000005, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.5000000000000053, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.4000000000000057, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.300000000000006, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.2000000000000064, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.1000000000000068, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -3.000000000000007, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.9000000000000075, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.800000000000008, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.700000000000008, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.6000000000000085, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.500000000000009, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.4000000000000092, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.3000000000000096, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.20000000000001, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.1000000000000103, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -2.0000000000000107, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.900000000000011, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.8000000000000114, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.7000000000000117, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.600000000000012, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.5000000000000124, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.4000000000000128, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.3000000000000131, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.2000000000000135, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.1000000000000139, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.0000000000000142, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.9000000000000146, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.8000000000000149, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.7000000000000153, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.6000000000000156, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.500000000000016, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.40000000000001634, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.3000000000000167, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.20000000000001705, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -0.10000000000001741, \"z1\": 0, \"r1\": -1}, {\"r\": 0.0, \"z\": -1.7763568394002505e-14, \"z1\": 0, \"r1\": -1}, {\"r\": 0.09999999999998188, \"z\": 0.09999999999998188, \"z1\": 0, \"r1\": -1}, {\"r\": 0.19999999999998153, \"z\": 0.19999999999998153, \"z1\": 0, \"r1\": -1}, {\"r\": 0.29999999999998117, \"z\": 0.29999999999998117, \"z1\": 0, \"r1\": -1}, {\"r\": 0.3999999999999808, \"z\": 0.3999999999999808, \"z1\": 0, \"r1\": -1}, {\"r\": 0.49999999999998046, \"z\": 0.49999999999998046, \"z1\": 0, \"r1\": -1}, {\"r\": 0.5999999999999801, \"z\": 0.5999999999999801, \"z1\": 0, \"r1\": -1}, {\"r\": 0.6999999999999797, \"z\": 0.6999999999999797, \"z1\": 0, \"r1\": -1}, {\"r\": 0.7999999999999794, \"z\": 0.7999999999999794, \"z1\": 0, \"r1\": -1}, {\"r\": 0.899999999999979, \"z\": 0.899999999999979, \"z1\": 0, \"r1\": -1}, {\"r\": 0.9999999999999787, \"z\": 0.9999999999999787, \"z1\": 0, \"r1\": -1}, {\"r\": 1.0999999999999783, \"z\": 1.0999999999999783, \"z1\": 0, \"r1\": -1}, {\"r\": 1.199999999999978, \"z\": 1.199999999999978, \"z1\": 0, \"r1\": -1}, {\"r\": 1.2999999999999776, \"z\": 1.2999999999999776, \"z1\": 0, \"r1\": -1}, {\"r\": 1.3999999999999773, \"z\": 1.3999999999999773, \"z1\": 0, \"r1\": -1}, {\"r\": 1.499999999999977, \"z\": 1.499999999999977, \"z1\": 0, \"r1\": -1}, {\"r\": 1.5999999999999766, \"z\": 1.5999999999999766, \"z1\": 0, \"r1\": -1}, {\"r\": 1.6999999999999762, \"z\": 1.6999999999999762, \"z1\": 0, \"r1\": -1}, {\"r\": 1.7999999999999758, \"z\": 1.7999999999999758, \"z1\": 0, \"r1\": -1}, {\"r\": 1.8999999999999755, \"z\": 1.8999999999999755, \"z1\": 0, \"r1\": -1}, {\"r\": 1.9999999999999751, \"z\": 1.9999999999999751, \"z1\": 0, \"r1\": -1}, {\"r\": 2.0999999999999748, \"z\": 2.0999999999999748, \"z1\": 0, \"r1\": -1}, {\"r\": 2.1999999999999744, \"z\": 2.1999999999999744, \"z1\": 0, \"r1\": -1}, {\"r\": 2.299999999999974, \"z\": 2.299999999999974, \"z1\": 0, \"r1\": -1}, {\"r\": 2.3999999999999737, \"z\": 2.3999999999999737, \"z1\": 0, \"r1\": -1}, {\"r\": 2.4999999999999734, \"z\": 2.4999999999999734, \"z1\": 0, \"r1\": -1}, {\"r\": 2.599999999999973, \"z\": 2.599999999999973, \"z1\": 0, \"r1\": -1}, {\"r\": 2.6999999999999726, \"z\": 2.6999999999999726, \"z1\": 0, \"r1\": -1}, {\"r\": 2.7999999999999723, \"z\": 2.7999999999999723, \"z1\": 0, \"r1\": -1}, {\"r\": 2.899999999999972, \"z\": 2.899999999999972, \"z1\": 0, \"r1\": -1}, {\"r\": 2.9999999999999716, \"z\": 2.9999999999999716, \"z1\": 0, \"r1\": -1}, {\"r\": 3.0999999999999712, \"z\": 3.0999999999999712, \"z1\": 0, \"r1\": -1}, {\"r\": 3.199999999999971, \"z\": 3.199999999999971, \"z1\": 0, \"r1\": -1}, {\"r\": 3.2999999999999705, \"z\": 3.2999999999999705, \"z1\": 0, \"r1\": -1}, {\"r\": 3.39999999999997, \"z\": 3.39999999999997, \"z1\": 0, \"r1\": -1}, {\"r\": 3.49999999999997, \"z\": 3.49999999999997, \"z1\": 0, \"r1\": -1}, {\"r\": 3.5999999999999694, \"z\": 3.5999999999999694, \"z1\": 0, \"r1\": -1}, {\"r\": 3.699999999999969, \"z\": 3.699999999999969, \"z1\": 0, \"r1\": -1}, {\"r\": 3.7999999999999687, \"z\": 3.7999999999999687, \"z1\": 0, \"r1\": -1}, {\"r\": 3.8999999999999684, \"z\": 3.8999999999999684, \"z1\": 0, \"r1\": -1}, {\"r\": 3.999999999999968, \"z\": 3.999999999999968, \"z1\": 0, \"r1\": -1}, {\"r\": 4.099999999999968, \"z\": 4.099999999999968, \"z1\": 0, \"r1\": -1}, {\"r\": 4.199999999999967, \"z\": 4.199999999999967, \"z1\": 0, \"r1\": -1}, {\"r\": 4.299999999999967, \"z\": 4.299999999999967, \"z1\": 0, \"r1\": -1}, {\"r\": 4.399999999999967, \"z\": 4.399999999999967, \"z1\": 0, \"r1\": -1}, {\"r\": 4.499999999999966, \"z\": 4.499999999999966, \"z1\": 0, \"r1\": -1}, {\"r\": 4.599999999999966, \"z\": 4.599999999999966, \"z1\": 0, \"r1\": -1}, {\"r\": 4.6999999999999655, \"z\": 4.6999999999999655, \"z1\": 0, \"r1\": -1}, {\"r\": 4.799999999999965, \"z\": 4.799999999999965, \"z1\": 0, \"r1\": -1}, {\"r\": 4.899999999999965, \"z\": 4.899999999999965, \"z1\": 0, \"r1\": -1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "z = np.arange(-5.0,5.0, 0.1)\n",
    "r = np.maximum(z, 0)\n",
    "df = pd.DataFrame({\"r\":r, \"z\":z})\n",
    "df[\"z1\"] = 0\n",
    "df[\"r1\"] = -1\n",
    "tanh = alt.Chart(df).mark_line().encode(x=\"z\", y=\"r\")\n",
    "threshold = alt.Chart(df).mark_rule(color=\"red\").encode(x=\"z1\", y=\"r1\")\n",
    "(tanh + threshold).properties(title='Chart 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the second pooling layer, LeNet-5 \"flattens\" the 2-dimensional matrix into a vector. Then computes a linear function $Z$, this is, the dot product between the weights and the flattened vector, and passes the the result trough the sigmoid. We have seen this function before here and here. It is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z(f_{mn}) = b + \\sum x_n w_{mn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matrix notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bf{z} = W^T \\times \\bf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapters we worked either binary or real-valued outputs. Now we have a multi-class problem. LeNet-5 approaches this by implementing a **euclidean radial basis (RBF) function**. Each output unit computes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y_i} = \\sum_j (a_j - w_{ij})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equals to compute the Euclidean distance between the input vector $\\bf{a}$ (output of the sigmoid) and the parameter vector $\\bf{w}$. To understand why this works we need to delve into probability theory, which I'll skip for now since it requires to introduce too many new mathematical ideas. In brief, the larger the distance between the input vector $\\bf{x}$ and the parameter vector $\\bf{w}$, the larger the RBF value output, and the more likely the unit to activate.\n",
    "\n",
    "In modern neural networks, like AlexNet, multi-class problems are usually approached with a **softmax function** defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(a)_i = \\frac{e^{a_i}}{\\sum_{j=1}^K e^{a_j}} \\text {  for i} =1,...,K\\text{  and}= (z_1,...,z_K)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this: first, we apply the exponential function to each element of the vector $\\bf{a}$, and then, we normalize by dividing by the sum of all exponentials values. The normalization constrains the the sum of $\\sigma(a)_i$ to 1. Now that we normalized to 1, each element of the output vector **can be interpreted as and activation probability**. This is really convinient, since now we can rank the outputs by its probability. For instance, in a 10-class problem, the unit with the highest value is the most likely value for a given input pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous chapters we used the Mean Squared Error (MSE) as a measure of \"goodness\" (or \"badness\") of the network predictions. This is not possibble here, since we are working a multi-class problem and the RBD function. The LeNet-5 implements the so-called **Maximun a Posteriori** (MAP) criterion as a cost function, defined as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(W) = \\frac{1}{P}\\sum_{p=1}^X(\\hat{y}_{D^{p}}(X^p, W) + \\log(e^{-j} + \\sum_i e^{-\\hat{y}(X^p, W)}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the cost function goes beyong what I want to cover here, and requires familiarity with Bayesian estimation. In brief, it maximize the posterior probability of the correct class $D_p$, and selects the most likely class. If you are curious about the mechanics of MAP [here is a video](https://www.youtube.com/watch?v=kkhdIriddSI) with a detailed explanation.\n",
    "\n",
    "We mentioned the softmax as an alternative output function. The cost function used to train a network with a softmax output tipycally is the **cross-entroppy loss** defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_i = -\\sum_i y_ i log (p_i) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $y_i$ is the true label for $ith$ output unit, and $p_i$ is the softmax probability value for the $ith$ output unit. In short, cross-entropy computes the distance between the **model output distribution** and the **real output distribution**. A video with a extended explanation of the **cross-entropy loss** [here](https://www.youtube.com/watch?v=bLb_Kp5Q9cw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning procedure: backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the multi-layer perceptron, the gradients for the error with respect to the weights are computed with **backpropagation**. I gave an extended explanation for backpropagation for the multi-layer perceptron [here](https://com-cog-book.github.io/com-cog-book/features/multilayer-perceptron.html#Backpropagation-algorithm). For now, let's just remember that the general form for backpropagation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial{W^{L-n}}} = \\frac{\\partial E}{\\partial{f^L}} \\frac{\\partial{f^L}}{\\partial{f^{L-1}}} ... \\frac{\\partial{f^{L-n}}}{\\partial{W^{L-n}}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, sequentially applying the chain-rule of calculus layer by layer, until we reach $W$ in the layer $L-n$.\n",
    "\n",
    "There is one adjustment to mention regarding convolutional networks. Remember that each feature map (convolution) **shares weights**. To account fot this, LeCun et all (1998) recommend to first compute each error derivative as if the netwokr were an standard multi-layer percepron without weight sharing, then add the derivatives of all connections that share the same parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lindsay, G. (2020). Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future. Journal of Cognitive Neuroscience, 115."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
