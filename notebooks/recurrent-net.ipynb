{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the recurrent neural network\n",
    "2. Obtain intuition about difficulties training RNNs, namely: vanishing/exploding gradients and long-term dependencies\n",
    "3. Obtain intuition about mechanics of backpropagation through time BPTT\n",
    "4. Develop a Long Short-Term memory implementation in Keras \n",
    "5. Learn about the uses and limitations of RNNs from a cognitive science perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poet Delmore Schwartz once wrote: **\"...time is the fire in which we burn\"**. We can't escape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that **data is \"perceived\" all at once**, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that **past-states have no influence in future-states**. This is, the input pattern at time-step $t-1$ does not influence the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.\n",
    "\n",
    "Multilayer Perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration (for instance [Cui et al, 2016](https://arxiv.org/pdf/1603.06995.pdf)). Nevertheless, introducing time considerations in such architectures is cumbersome, and better architectures have been envisioned. In particular, **Recurrent Neural Networks (RNNs)** are the modern standard to deal with **time-dependent** and/or **sequence-dependent** problems. This type of network is \"recurrent\" in the sense that they can **revisit or reuse past states as inputs to predict the next or future states**. To put it plainly, they have **memory**. Indeed, memory is what allows us to incorporate our past thoughts and behaviors into our future thoughts and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the earliest examples of networks incorporating \"recurrences\" was the so-called **Hopfield Network**, introduced in 1982 by [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield), at the time, a physicist at Caltech. Hopfield networks were important as they helped to reignite the interest in neural networks in the early '80s. In his 1982 paper, Hopfield wanted to address the fundamental question of **emergence** in cognitive systems: Can relatively stable cognitive phenomena, like memories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physical systems like vortex patterns in fluid flow. Brains seemed like another promising candidate.\n",
    "\n",
    "Hopfield networks are known as a type of **energy-based** (instead of error-based) network because their properties derive from a global energy-function (Raj, 2020). In resemblance to the McCulloch-Pitts neuron, Hopfield neurons are binary threshold units but with recurrent instead of feed-forward connections, where each unit is **bi-directionally connected** to each other, as shown in **Figure 1**. This means that each unit *receives* inputs and *sends* inputs to every other connected unit. A consequence of this architecture is that **weights values are symmetric**, such that weights *coming into* a unit are the same as the ones *coming out* of a unit. The value of each unit is determined by a linear function wrapped into a threshold function $T$, as $y_i = T(\\sum w_{ji}y_j + b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 1: Hopfield Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/hopfield-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopfield network's idea is that each configuration of binary-values $C$ in the network is associated with a **global energy value $-E$**. Here is a simplified picture of the training process: imagine you have a network with five neurons with a configuration of $C_1=(0, 1, 0, 1, 0)$. Now, imagine $C_1$ yields a global energy-value $E_1= 2$ (following the energy function formula). Your goal is to *minimize* $E$ by changing one element of the network $c_i$ at a time. By using the weight updating rule $\\Delta w$, you can subsequently get a new configuration like $C_2=(1, 1, 0, 1, 0)$, as new weights will cause a change in the activation values $(0,1)$. If $C_2$ yields a *lower value of $E$*, let's say, $1.5$, you are moving in the right direction. If you keep iterating with new configurations the network will eventually \"settle\" into a **global energy minimum** (conditioned to the initial state of the network).\n",
    "\n",
    "A fascinating aspect of Hopfield networks, besides the introduction of recurrence, is that is closely based in neuroscience research about learning and memory, particularly Hebbian learning (Hebb, 1949). In fact, Hopfield (1982) proposed this model as a way to capture **memory formation and retrieval**. The idea is that the energy-minima of the network could represent the **formation of a memory**, which further gives rise to a property known as **[content-addressable memory (CAM)](https://en.wikipedia.org/wiki/Content-addressable_memory)**. Here is the idea with a computer analogy: when you access information stored in the random access memory of your computer (RAM), you give the \"address\" where the \"memory\" is located to retrieve it. CAM works the other way around: you give information about the **content** you are searching for, and the computer should retrieve the \"memory\". This is great because this works even when you have **partial or corrupted** information about the content, which is a much more **realistic depiction of how human memory works**. It is similar to doing a google search. Just think in how many times you have searched for lyrics with partial information, like \"song with the beeeee bop ba bodda bope!\".\n",
    "\n",
    "It is important to highlight that the sequential adjustment of Hopfield networks is **not driven by error correction**: there isn't a \"target\" as in supervised-based neural networks. Hopfield networks are systems that \"evolve\" until they find a stable low-energy state. If you \"perturb\" such a system, the system will \"re-evolve\" towards its previous stable-state, similar to how those inflatable \"Bop Bags\" toys get back to their initial position no matter how hard you punch them. It is almost like the system \"remembers\" its previous stable-state (isn't?). This ability to \"return\" to a previous stable-state after the perturbation is why they serve as models of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Hopfield networks where innovative and fascinating models, the first successful example of a recurrent network trained with backpropagation was introduced by [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman), the so-called **Elman Network** (Elman, 1990). Elman was a cognitive scientist at UC San Diego at the time, part of the group of researchers that published the famous PDP book.\n",
    "\n",
    "In 1990, Elman published \"Finding Structure in Time\", a highly influential work for in cognitive science. Elman was concerned with the problem of representing \"time\" or \"sequences\" in neural networks. In his view, you could take either an \"explicit\" approach or an \"implicit\" approach. The **explicit** approach represents time **spacially**. Consider a vector $x = [x_1,x_2 \\cdots, x_n]$, where element $x_1$ represents the first value of a sequence, $x_2$ the second element, and $x_n$ the last element. Hence, the spacial location in $\\bf{x}$ is indicating the temporal location of each element. You can think about elements of $\\bf{x}$ as sequences of words or actions, one after the other, for instance: $x^1=[Sound, of, the, funky, drummer]$ is a sequence of length five. Elman saw **several drawbacks** to this approach. First, although $\\bf{x}$ is a sequence, the network still needs to represent the sequence all at once as an input, this is, a network would need five input neurons to process $x^1$. Second, it imposes a rigid limit on the duration of pattern, in other words, the network needs a fixed number of elements for every input vector $\\bf{x}$: a network with five input units, can't accommodate a sequence of length six. True, you could start with a six input network, but then shorter sequences would be misrepresented since mismatched units would receive zero input. This is a problem for most domains where sequences have a variable duration. Finally, it can't easily distinguish **relative** temporal position from **absolute** temporal position. Consider the sequence $s = [1, 1]$ and a vector input length of four bits. Such a sequence can be presented in at least three variations:\n",
    "\n",
    "$$\n",
    "x_1 = [0, 1, 1, 0]\\\\\n",
    "x_2 = [0, 0, 1, 1]\\\\\n",
    "x_3 = [1, 1, 0, 0]\n",
    "$$\n",
    "\n",
    "Here, $\\bf{x_1}$, $\\bf{x_2}$, and $\\bf{x_3}$ are instances of $\\bf{s}$ but spacially displaced in the input vector. Geometrically, those three vectors are very different from each other (you can compute similarity measures to put a number on that), although representing the same instance. Even though you can train a neural net to learn those three patterns are associated with the same target, their inherent dissimilarity probably will hinder the network's ability to generalize the learned association.\n",
    "\n",
    "The **implicit** approach represents time by **its effect in intermediate computations**. To do this, Elman added a **context unit** to save past computations and incorporate those in future computations. In short, **memory**. Elman based his approach in the work of [Michael I. Jordan](https://people.eecs.berkeley.edu/~jordan/) on serial processing (1986). Jordan's network implements recurrent connections from the network output $\\hat{y}$ to its hidden units $h$, via a \"memory unit\" $\\mu$ (equivalent to Elman's \"context unit\") as depicted in **Figure 2**. In short, the memory unit keeps a running average of **all past outputs**: this is how the past history is implicitly accounted for on each new computation. There is no learning in the memory unit, which means the weights are fixed to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 2: Jordan Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/jordan-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jordan's network diagrams exemplifies the two ways in which recurrent nets are usually represented. On the left, the **compact format** depicts the network structure as a circuit. On the right, the **unfolded representation** incorporates the notion of time-steps calculations. The unfolded representation also illustrates how a recurrent network can be constructed in a pure feed-forward fashion, with as many layers as time-steps in your sequence. One key consideration is that the weights will be identical on each time-step (or layer). Keep this unfolded representation in mind as will become important later.\n",
    "\n",
    "Elman's innovation was twofold: **recurrent connections between hidden units and memory** (context) units, and **trainable parameters from the memory units to the hidden units**. Memory units now have to \"remember\" the past state of hidden units, which means that instead of keeping a running average, they \"clone\" the value at the previous time-step $t-1$. Memory units also have to learn useful representations (weights) for encoding temporal properties of the sequential input.  **Figure 3** summarizes Elman's network in compact and unfolded fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 3: Elman Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/elman-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there is something curious about Elman's architecture. What it is the point of \"cloning\" $h$ into $c$ at each time-step? You could bypass $c$ altogether by sending the value of $h_t$ straight into $h_{t+1}$, wich yield mathematically identical results. The most likely explanation for this was that Elman's starting point was Jordan's network, which had a separated memory unit. Regardless, keep in mind we don't need $c$ units to design a functionally identical network. \n",
    "\n",
    "Elman performed multiple experiments with this architecture demonstrating it was capable to solve multiple problems with a sequential structure: a temporal version of the XOR problem; learning the structure (i.e., vowels and consonants sequential order) in sequences of letters; discovering the notion of \"word\", and even learning complex lexical classes like word order in short sentences. Let's briefly explore the temporal XOR solution as an exemplar. **Table 1** shows the XOR problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**: Truth Table For XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to transform the XOR problem into a sequence. Consider the following vector: \n",
    "\n",
    "$$\n",
    "s= [1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,...]\n",
    "$$\n",
    "\n",
    "In $\\bf{s}$, the first and second elements, $s_1$ and $s_2$, represent $x_1$ and $x_2$ inputs of **Table 1**, whereas the third element, $s_3$, represents the corresponding output $y$. This pattern repeats until the end of the sequence $s$ as shown in **Figure 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 4: Temporal XOR </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/temporal-xor.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elman trained his network with a 3,000 elements sequence for 600 iterations over the entire dataset, on the task of predicting the next item $s_{t+1}$ of the sequence $s$, meaning that he fed inputs to the network **one by one**. He showed that **error pattern** followed a predictable trend: the mean squared error was **lower every 3 outputs**, and higher in between, meaning the network learned to predict the third element in the sequence, as shown in **Chart 1** (the numbers are made up, but the pattern is the same found by Elman (1990))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ef2294cd0f0d4c57b58a84b45ed20df7\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-ef2294cd0f0d4c57b58a84b45ed20df7\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2fb8460af59365532c40b2d7e4fe648a\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"cycle\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"MSE\"}}, \"title\": \"Chart 1\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-2fb8460af59365532c40b2d7e4fe648a\": [{\"MSE\": 0.35, \"cycle\": 1}, {\"MSE\": 0.15, \"cycle\": 2}, {\"MSE\": 0.3, \"cycle\": 3}, {\"MSE\": 0.27, \"cycle\": 4}, {\"MSE\": 0.14, \"cycle\": 5}, {\"MSE\": 0.4, \"cycle\": 6}, {\"MSE\": 0.35, \"cycle\": 7}, {\"MSE\": 0.12, \"cycle\": 8}, {\"MSE\": 0.36, \"cycle\": 9}, {\"MSE\": 0.31, \"cycle\": 10}, {\"MSE\": 0.15, \"cycle\": 11}, {\"MSE\": 0.32, \"cycle\": 12}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame({\"MSE\": [0.35, 0.15, 0.30, 0.27, 0.14, 0.40, 0.35, 0.12, 0.36, 0.31, 0.15, 0.32],\n",
    "                  \"cycle\": np.arange(1, 13)})\n",
    "alt.Chart(s).mark_line().encode(x=\"cycle\", y=\"MSE\").properties(title='Chart 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An immediate advantage of this approach is the network can take **inputs of any length**, without having to alter the network architecture at all.\n",
    "\n",
    "In the same paper, Elman showed that the **internal (hidden) representations** learned by the network grouped into meaningful categories, this is, **semantically similar words group together** when analyzed with [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). This was remarkable as demonstrated the utility of RNNs as a model of cognition in sequence-based problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: vanishing and exploding gradients in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, training recurrent neural networks is hard. Considerably harder than multilayer-perceptrons. When faced with the task of training **very deep networks**, like RNNs, the gradients have the impolite tendency of either (1) **vanishing**, or (2) **exploding** (Bengio et al, 1994; Pascanu et al, 2012). Recall that RNNs can be unfolded so that recurrent connections follow pure feed-forward computations. This unrolled RNN will have as many layers as elements in the sequence. Thus, a sequence of 50 words will be unrolled as an RNN of 50 layers (taking \"word\" as a unit). \n",
    "\n",
    "Concretely, the **vanishing gradient problem** will make close to impossible to learn **long-term dependencies** in sequences. Let's say you have a collection of poems, where the last sentence refers to the first one. Such a dependency will be hard to learn for a deep RNN where gradients vanish as we move backward in the network. The **exploding gradient problem** will completely derail the learning process. In very deep networks this is often a problem because more layers amplify the effect of large gradients, compounding into very large updates to the network weights, to the point values completely blow up.        \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient vanishing**: when gradients *begin small*, as you move backward through the network computing gradients, they will get even smaller as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the output layer will obtain larger updates than weights closer to the input layer. This means that the weights closer to the input layer will hardly change at all, whereas the weights closer to the output layer will change a lot. This is a serious problem when **earlier layers matter for prediction**: they will keep propagating more or less the same signal forward because no learning (i.e., weight updates) will happen, which may significantly hinder the network performance.  \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient explosion**: when gradients *begin large*, as you move backward through the network computing gradients, they will get even larger as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the input layer will obtain larger updates than weights closer to the output layer. Learning can go wrong really fast. Recall that the signal propagated by each layer is the outcome of taking the product between the previous hidden-state and the current hidden-state. If the weights in earlier layers get really large, they will forward-propagate larger and larger signals on each iteration, and the predicted output values will spiral-up out of control, making the error $y-\\hat{y}$ so large that the network will be unable to learn at all. In fact, your computer will \"overflow\" quickly as it would unable to represent numbers that big. Very dramatic.  \n",
    "\n",
    "The mathematics of gradient vanishing and explosion gets complicated quickly. If you want to delve into the mathematics see [Bengio et all (1994)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf), [Pascanu et all (2012)](https://arxiv.org/abs/1211.5063), and [Philipp et all (2017)](https://arxiv.org/abs/1712.05577). \n",
    "\n",
    "For our purposes, I'll give you a simplified numerical example for intuition. Consider the task of predicting a vector $y = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, from inputs $x = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, with a multilayer-perceptron with 5 hidden layers and tanh activation functions. We have two cases:\n",
    "\n",
    "- the weight matrix $W_l$ is initialized to large values $w_{ij} = 2$\n",
    "- the weight matrix $W_s$ is initialized to small values $w_{ij} = 0.02$\n",
    "\n",
    "Now, let's compute a single forward-propagation pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for large initial weights: \n",
      " [[3.99730269]\n",
      " [3.99730269]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_l = np.array([[2, 2],\n",
    "                [2, 2]])\n",
    "\n",
    "h1 = np.tanh(W_l @ x)\n",
    "h2 = np.tanh(W_l @ h1)\n",
    "h3 = np.tanh(W_l @ h2)\n",
    "h4 = np.tanh(W_l @ h3)\n",
    "h5 = np.tanh(W_l @ h4)\n",
    "y_hat = (W_l @ h5)\n",
    "print(f'output for large initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for small initial weights: \n",
      " [[4.09381337e-09]\n",
      " [4.09381337e-09]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_s = np.array([[0.02, 0.02],\n",
    "                [0.02, 0.02]])\n",
    "\n",
    "h1 = np.tanh(W_s @ x)\n",
    "h2 = np.tanh(W_s @ h1)\n",
    "h3 = np.tanh(W_s @ h2)\n",
    "h4 = np.tanh(W_s @ h3)\n",
    "h5 = np.tanh(W_s @ h4)\n",
    "y_hat = (W_s @ h5)\n",
    "print(f'output for small initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for $W_l$ the output $\\hat{y}\\approx4$, whereas for $W_s$ the output $\\hat{y} \\approx 0$. Why does this matter? We haven't done the gradient computation but you can probably anticipate what it's going to happen: for the $W_l$ case, the gradient update is going to be very large, and for the $W_s$ very small. If you keep cycling through forward and backward passes these problems will become worse, leading to gradient explosion and vanishing respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several challenges difficulted progress in RNN in the early '90s (Hochreiter & Schmidhuber, 1997; Pascanu et al, 2012). In addition to vanishing and exploding gradients, we have the fact that the **forward computation is slow**, as RNNs can't compute in parallel: to preserve the time-dependencies through the layers, each layer has to be computed sequentially, which naturally takes more time. Elman networks proved to be effective at solving relatively simple problems, but as the sequences scaled in size and complexity, this type of network struggle. \n",
    "\n",
    "Several approaches were proposed in the '90s to address the aforementioned issues like time-delay neural networks (Lang et al, 1990), simulated annealing (Bengio et al., 1994), and others. The architecture that really moved the field forward was the so-called **Long Short-Term Memory (LSTM) Network**, introduced by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [Jurgen Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) in 1997. As the name suggests, the defining characteristic of LSTMs is the addition of units combining both short-memory and long-memory capabilities. \n",
    "\n",
    "In LSTMs, instead of having a simple memory unit \"cloning\" values from the hidden unit as in Elman networks, we have a (1) **cell unit** (a.k.a., memory unit) which effectively acts as long-term memory storage, and (2) a **hidden-state** which acts as a memory controller. These two elements are integrated as a circuit of logic gates controlling the flow of information at each time-step. Understanding the notation is crucial here, which is depicted in **Figure 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 5: LSTM architecture </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-unit.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTMs $x_t$, $h_t$, and $c_t$ represent vectors of values. Lightish-pink circles represent element-wise operations, and darkish-pink boxes are fully-connected layers with trainable weights. The top part of the diagram acts as a **memory storage**, whereas the bottom part has a double role: (1) passing the hidden-state information from the previous time-step $t-1$ to the next time step $t$, and (2) to regulate the **influx** of information from $x_t$ and $h_{t-1}$ **into** the memory storage, and the **outflux** of information **from** the memory storage into the next hidden state $h-t$. The second role is the core idea behind LSTM. You can think about it as making **three decisions** at each time-step:\n",
    "\n",
    "1. **Is the *old information* $c_{t-1}$ worth to keep in my memory storage $c_t$?** If so, let the information pass, otherwise, \"forget\" such information. This is controlled by the *forget gate*.    \n",
    "2. **Is this *new information* (inputs) worth to be saved into my memory storage $c_t$?** If so, let information flow into $c_t$. This is controlled by the *input gate* and the *candidate memory cell*. \n",
    "3. **What elements of the information saved in my memory storage $c_t$ are relevant for the computation of the next hidden-state $h_t$?** Select them from $c_t$, combine them new hidden-state output, and let them pass into the next hidden-state $h_t$. This is controlled by the *output gate* and the *tanh* function. \n",
    "\n",
    "Decisions 1 and 2 will determine the information that keeps flowing through the memory storage at the top. Decision 3 will determine the information that flows to the next hidden-state at the bottom. The conjunction of these decisions sometimes is called \"memory block\". Now, keep in mind that this *sequence of decision* is just a convenient *interpretation* of LSTM mechanics. In practice, the weights are the ones determining what each function ends up doing, which may or may not fit well with human intuitions or design objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 6: LSTM as a sequence of decisions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-choices.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put LSTMs in context, imagine the following simplified scenerio: we are trying to **predict the next word in a sequence**. Let's say, squences are about sports. From past sequences, we saved in the memory block the type of sport: \"soccer\". For the current sequence, we receive a phrase like \"A basketball player...\". In such a case, we first want to \"forget\" the previous type of sport \"soccer\" (*decision 1*) by multplying $c_{t-1} \\odot f_t$. Next, we want to \"update\" memory with the new type of sport, \"basketball\" (*decision 2*), by adding $c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})$. Finally, we want to output (*decision 3*) a verb relevant for \"A basketball player...\", like \"shoot\" or \"dunk\" by $\\hat{y_t} = softmax(W_{hz}h_t + b_z)$.\n",
    "\n",
    "LSTMs long-term memory capabilities make them good at capturing long-term dependencies. The memory cell effectively counteracts the vanishing gradient problem at preserving information as long the forget gate does not \"erase\" past information (Graves, 2012). All the above make LSTMs sere](https://en.wikipedia.org/wiki/Long_short-term_memory#Applications)). For instance, when you use [Google's Voice Transcription](https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html) services an RNN is doing the hard work of recognizing your voice.uccessful in practical applications in sequence-modeling (see a list [here](https://en.wikipedia.org/wiki/Long_short-term_memory#Applications)). For instance, when you use [Google's Voice Transcription](https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html) services an RNN is doing the hard work of recognizing your voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and cognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Convolutional Neural Networks, researchers utilizing RNN for approaching sequential problems like natural language processing (NLP) or time-series prediction, do not *necessarily* care about (although some might) how good of a model of cognition and brain-activity are RNNs. What they really care is about solving problems like translation, speech recognition, and stock market prediction, and many advances in the field come from pursuing such goals. Still, RNN has many **desirable traits as a model of neuro-cognitive activity**, and have been prolifically used to **model several aspects of human cognition and behavior**: child behavior in an object permanence tasks (Munakata et al, 1997); knowledge-intensive text-comprehension (St. John, 1992); processing in quasi-regular domains, like English word reading (Plaut et al., 1996); human performance in processing recursive language structures (Christiansen & Chater, 1999); human sequential action (Botvinick & Plaut, 2004); movement patterns in typical and atypical developing children (Muñoz-Organero et al., 2019). And many others. Neuroscientists have used RNNs to model a wide variety of aspects as well (for reviews see Barak, 2017, Güçlü & van Gerven, 2017, Jarne & Laje, 2019). Overall, RNN has demonstrated to be a productive tool for modeling cognitive and brain function, in distributed representations paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two mathematically complex issues with RNNs: (1) computing hidden-states, and (2) backpropagation. The rest are common operations found in multilayer-perceptrons. LSTMs and its many variants are the facto standards when modeling any kind of sequential problem. Elman networks can be seen as a simplified version of an LSTM, so I'll focus my attention on LSTMs for the most part. My exposition is based on a combination of sources that you may want to review for extended explanations (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997; Graves, 2012; Chen, 2016; Zhang et al., 2020).\n",
    "\n",
    "The LSTM architecture can be desribed by: \n",
    "\n",
    "**Forward pass**:\n",
    "- non-linear forget function\n",
    "- non-linear input function \n",
    "- non-linear candidate-memory function\n",
    "- non-linear output function\n",
    "- memory cell function\n",
    "- non-linear hidden-state function \n",
    "- softmax function (output)\n",
    "\n",
    "**Backward pass**:\n",
    "- Cost-function\n",
    "- Learning procedure (backpropagation)\n",
    "\n",
    "Following the indices for each function requires some definitions. I'll assume we have $h$ hidden units, training sequences of size $n$, and $d$ input units. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{input-units} &= x_i \\in \\mathbb{R}^d \\\\ \n",
    "\\text{training-sequence} &= s_i \\in \\mathbb{R}^n \\\\\n",
    "\\text{output-class} &= y_i \\in \\mathbb{R}^k \\\\\n",
    "\\text{Input-layer} &= X_t \\in \\mathbb{R}^{n\\times d} \\\\\n",
    "\\text{hidden-layer} &= H_t \\in \\mathbb{R}^{n\\times h}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forget function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. We didn't mentioned the bias before, but it is the same bias that all neural networks incorporate, one for each unit in $f$. More formally:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)\n",
    "$$ \n",
    "\n",
    "Each matrix $W$ has dimensionality equal to (number of incoming units, number for connected units). For example, $W_{xf}$ refers to $W_{input-units, forget-units}$. Keep this in mind to read the indices of the $W$ matrices for subsequent definitions.\n",
    "\n",
    "Here is an important insight: What would it happen if $f_t = 0$? If you look at the diagram in **Figure 6**, $f_t$ performs an elementwise multiplication of each element in $c_{t-1}$, meaning that every value would be reduced to $0$. In short, the network would completely \"forget\" past states. Naturally, if $f_t = 1$, the network would keep its memory intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function and Candidate memory function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. It's defined as:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\n",
    "$$ \n",
    "\n",
    "The **candidate memory function** is an hyperbolic tanget function combining the same elements that $i_t$. It's defined as:\n",
    "\n",
    "$$\n",
    "\\tilde{c}_t = tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\n",
    "$$ \n",
    "\n",
    "Both functions are combined to update the memory cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **output function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. Is defined as:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory cell function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **memory cell function** (what I've been calling \"memory storage\" for conceptual clarity), combines the effect of the forget function, input function, and candidate memory function. It's defined as:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})\n",
    "$$\n",
    "\n",
    "Where $\\odot$ implies an elementwise multiplication (instead of the usual dot product). This expands to:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)) + (\\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \\odot tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden-state function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next **hidden-state function** combines the effect of the output function and the contents of the memory cell scaled by a tanh function. It is defined as:\n",
    "\n",
    "$$\n",
    "h_t = O_t \\odot tanh(c_t) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output function will depend upon the problem to be approached. For our our purposes, we will assume a multi-class problem, for which the **softmax function** is appropiated. For this, we first pass the hidden-state by a linear function, and then the softmax as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= (W_{hz}h_t + b_z)\\\\\n",
    "\\hat{y}_t &= softmax(z_t) = \\frac{e^{z_t}}{\\sum_{j=1}^K e^{z_j}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The softmax computes the exponent for each $z_t$ and then normalized by dividing by the sum of every output value exponentiated. In this manner, the output of the softmax can be interpreted as the likelihood value $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the output function, the cost function will depend upon the problem. For regression problems, the Mean-Squared Error can be used. For our purposes (classification), the cross-entropy function is appropriated. It's defined as:\n",
    "\n",
    "$$\n",
    "E_i = - \\sum_t y_ilog(p_i)\n",
    "$$\n",
    "\n",
    "Where $y_i$ is the true label for the $ith$ output unit, and $log(p_i)$ is the log of the softmax value for the $ith$ output unit. The summation indicates we need to aggregate the cost at each time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning procedure: Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, Hochreiter and Schmidhuber (1997) trained LSTMs with a combination of approximate gradient descent computed with a combination of real-time recurrent learning and backpropagation through time (BPTT). Nevertheless, LSTM can be trained with pure backpropagation. Following Graves (2012), I'll only describe BTT because is more accurate,  easier to debug and to describe.\n",
    "\n",
    "**Note**: we call it **backpropagation through time** because of the sequential time-dependent structure of RNNs. Recall that each layer represents a time-step, and forward propagation happens in sequence, one layer computed after the other. Hence, when we backpropagate, we do the same but backward (i.e., \"through time\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 7: Three-layer simplified RNN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/simple-rnn.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reviewed backpropagation for a simple multilayer perceptron [here](https://com-cog-book.github.io/com-cog-book/features/multilayer-perceptron.html#Backpropagation-algorithm). Nevertheless, I'll sketch BPTT for the simplest case as shown in **Figure 7**, this is, with a generic non-linear hidden-layer similar to Elman network without \"context units\" (some like to call it \"vanilla\" RNN, which I avoid because I believe is derogatory against vanilla!). This exercise will allow us to review backpropagation and to understand how it differs from BPTT. We begin by defining a simplified RNN as: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= W_{hz}h_t + b_z\\\\\n",
    "h_t &= \\sigma(W_{hh}h_{t-1} + W_{xh}x_t+b_h)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $h_t$ and $z_t$ indicates a hidden-state (or layer) and  the output respectively. Therefore, **we have to compute gradients w.r.t. five sets of weights**: $\\{W_{hz}, W_{hh}, W_{xh}, b_z, b_h\\}$.\n",
    "\n",
    "First, consider the error derivatives w.r.t. $W_{hz}$ at time $t$, the weight matrix for the linear function at the output layer. Recall that $W_{hz}$ is shared across all time-steps, hence, we can compute the gradients at each time step and then take the sum as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W_{hz}}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{W_{hz}}}\n",
    "$$\n",
    "\n",
    "Same for the bias term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{b_z}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{b_z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That part is straightforward. The issue arises when we try to compute the gradients w.r.t. the wights $W_{hh}$ in the hidden layer. Consider a three layer RNN (i.e., unfolded over three time-steps). In such a case, we have: \n",
    "\n",
    "- $E_3$ depens on $z_3$\n",
    "- $z_3$ depends on $h_3$\n",
    "- $h_3$ depens on $h_2$\n",
    "- $h_2$ depens on $h_1$\n",
    "- $h_1$ depens on $h_0$, where $h_0$ is a random starting state.\n",
    "\n",
    "Now, we have that $E_3$ w.r.t to $h_3$ becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "The issue here is that $h_3$ depends on $h_2$, since according to our definition, the $W_{hh}$ is multiplied by $h_{t-1}$, meaning **we can't compute $\\frac{\\partial{h_3}}{\\partial{W_{hh}}}$ directly**. Othewise, we would be treating $h_2$ as a constant, which is incorrect: is a function. What we need to do is to **compute the gradients separately**: the direct contribution of ${W_{hh}}$ on $E$ and the indirect contribution via $h_2$. Following the rules of calculus in multiple variables, we compute them independently and add them up together as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Again, we have that we can't compute $\\frac{\\partial{h_2}}{\\partial{W_{hh}}}$ directly. Following the same procedure, we have that our full expression becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Essentially, this means that we compute and add the contribution of $W_{hh}$ to $E$ at each time-step. The expression for $b_h$ is the same:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{b_h}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{b_h}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compute the gradients w.r.t. $W_{xh}$. Here, again, we have to add the contributions of $W_{xh}$ via $h_3$, $h_2$, and $h_1$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{xh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{xh}}}\n",
    "$$\n",
    "\n",
    "That's for BPTT for a simple RNN. The math reviewed here generalizes with minimal changes to more complex architectures as LSTMs. Actually, the only difference regarding LSTMs, is that we have more weights to differentiate for. Instead of a single generic $W_{hh}$, we have $W$ for all the gates: forget, input, output, and candidate cell. The rest remains the same. For a detailed derivation of BPTT for the LSTM see [Graves (2012)](https://www.cs.toronto.edu/~graves/preprint.pdf) and [Chen (2016)](https://arxiv.org/abs/1610.02583)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Sequence-data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with sequence-data, like text or time-series, requires to pre-process it in a manner that is \"digestible\" for RNNs. As with any neural network, RNN can't take raw text as an input, we need to **parse** text sequences and then **\"map\"** them into vectors of numbers. Here I'll briefly review these issues to provide enough context for our example applications. For an extended revision please refer to [Jurafsky and Martin (2019)](https://web.stanford.edu/~jurafsky/slp3/), [Goldberg (2015)](http://u.cs.biu.ac.il/~yogo/nnlp.pdf), [Chollet (2017)](https://www.manning.com/books/deep-learning-with-python), and [Zhang et al (2020)](https://d2l.ai/chapter_recurrent-neural-networks/index.html).\n",
    "\n",
    "Parsing can be done in multiple manners, the most common being: \n",
    "\n",
    "- Using **word** as a unit, which each word represented as a vector\n",
    "- Using **character** as a unit, with each character represented as a vector\n",
    "- Using **n-grams** of words or characters as a unit, with each n-gram represented as a vector. N-grams are sets of words or characters of size \"N\" or less.\n",
    "\n",
    "The process of parsing text into smaller units is called \"tokenization\", and each resulting unit is called a \"token\", the top pane in **Figure 8** displays a sketch of the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 8: Tokenization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/text-pro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a corpus of text has been parsed into tokens, we have to map such tokens into numerical vectors. Two common ways to do this are **one-hot encoding** approach and the **word embeddings** approach, as depicted in the bottom pane of **Figure 8**. We used one-hot encodings to transform the MNIST class-labels into vectors of numbers for classification in the [CovNets chapter](https://com-cog-book.github.io/com-cog-book/features/cov-net.html). In a one-hot encoding vector, each token is mapped into a *unique* vector of zeros and ones. The vector size is determined by the vocabullary size. For instance, for the set $x= \\{\"cat\", \"dog\", \"ferret\"\\}$, we could use a 3-dimensional one-hot encoding as:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "One-hot encodings have the advantages of being straightforward to implement and to provide a unique identifier for each token. Its main disadvantage is that tends to create really **sparse** and **high-dimensional** representations for a large corpus of texts. For instance, if you tried a one-hot encoding for 50,000 tokens, you'd end up with a 50,000x50,000-dimensional matrix, which may be unpractical for most tasks. \n",
    "\n",
    "**Word embeddings** represent text by mapping tokens into vectors of real-valued numbers instead of only zeros and ones. This significantly increments the representational capacity of vectors, reducing the required dimensionality for a given corpus of text compared to one-hot encodings. For instance, 50,000 tokens could be represented by as little as 2 or 3 vectors (although the representation may not be very good). Taking the same set $x$ as before, we could have a 2-dimensional word embedding like:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.8\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "1 \n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You may be wondering why to bother with one-hot encodings when word embeddings are much more space-efficient. The main issue with word-embedding is that **there isn't an obvious way to map tokens into vectors** as with one-hot encodings. For instance, you could assign tokens to vectors at random (assuming every token is assigned to a *unique* vector). The problem with such approach is that the semantic structure in the corpus is broken. Ideally, you want words of similar meaning mapped into similar vectors. We can preserve the semantic structure of a text corpus in the same manner as everything else in machine learning: by **learning from data**. There are two ways to do this:\n",
    "\n",
    "- Learning the word embeddings **at the same time** you train the RNN.\n",
    "- Utilizing **pretrained** word embeddings, this is, embeddings learned in a different task. This is a form of \"transfer learning\".\n",
    "\n",
    "Learning word embeddings for your task is advisable as semantic relationships among words tend to be **context dependent**. For instance, \"exploitation\" in the context of mining is related to resource \"extraction\", hence relative neutral. But, \"exploitation\" in the context of labor rights is related to the idea of \"abuse\", hence a negative connotation. This is more critical when we are dealing with different languages. Nevertheless, learning embeddings for every task sometimes is impractical, either because your corpus is too \"small\" (i.e., not enough data to extract semantic relationships), or too \"large\" (i.e., you don't have enough time and/or resources to learn the embeddings). Examples of freely accessible pretrained word embeddings are Google's [Word2vec](https://code.google.com/archive/p/word2vec/) and the [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) (GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous chapters, I'll use Keras to implement both (a modified version of) the Elman Network for the XOR problem and an LSTM for review prediction based on text-sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, it may be clear to you that Elman networks are a simple RNN with two neurons, one for each input pattern, in the hidden-state. Originally, Elman trained his architecture with a truncated version of BPTT, meaning that only considered two time-steps for computing the gradients, $t$ and $t-1$. We will implement a modified version of Elman's architecture bypassing the \"context\" unit (which does not alter the result at all) and utilizing BPTT instead of its truncated version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, we don't need to generate the 3,000 bits sequence that Elman used in his original work. We can simply generate a single pair of training and testing sets for the XOR problem as in **Table 1**, and pass the training sequence (length two) as the inputs, and the expected outputs as the target. This is very much alike any classification task. An important caveat is that simpleRNN layers in Keras expect an input tensor of shape (number-samples, timesteps, number-input-features). In our case, this has to be: *number-samples*= 4,  *timesteps*=1, *number-input-features*=2. No separate encoding is necessary here because we are manually setting the input and output values to binary vector representations. Finally, we won't worry about training and testing sets for this example, which is way to simple for that (we will do that for the next example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries for this section\n",
    "\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (4, 1, 2)\n",
      "targets data shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "X = np.array([[[0, 0, 1, 1]],\n",
    "              [[0, 1, 0, 1]]]).T\n",
    "\n",
    "# expected values\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "print(f'training data shape: {X.shape}')\n",
    "print(f'targets data shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman network architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a (modified) in Keras is extremely simple as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network as a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add a recurrent layer with 2 units\n",
    "model.add(SimpleRNN(2, input_shape=(1, 2)))\n",
    "\n",
    "# Add the output layer with a sigmoid activation\n",
    "model.add(Dense(1, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows that our architecture yields 13 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman network Application: XOR classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile and fit our model. I'll utilize [Adadelta](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L376) (to avoid manually adjusting the learning rate) as the optimizer, and the Mean-Squared Error (as in Elman original work). I'll train the model for 15,000 epochs over the 4 samples dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adadelta', \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['acc'])\n",
    "history = model.fit(X, y,\n",
    "                    epochs=5000,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chart 2** shows the error curve (red, right axis), and the accuracy curve (blue, left axis) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-33474bf3e5cd497eae523fb0ce9d70a9\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-33474bf3e5cd497eae523fb0ce9d70a9\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}}], \"data\": {\"name\": \"data-c03349beb361db3898adb0cd3bce46d2\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": \"Chart 2\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-c03349beb361db3898adb0cd3bce46d2\": [{\"accuracy\": 0.25, \"loss\": 1.5089318752288818, \"time-step\": 0}, {\"accuracy\": 0.25, \"loss\": 1.5033257007598877, \"time-step\": 1}, {\"accuracy\": 0.25, \"loss\": 1.497619867324829, \"time-step\": 2}, {\"accuracy\": 0.25, \"loss\": 1.4918333292007446, \"time-step\": 3}, {\"accuracy\": 0.25, \"loss\": 1.4859753847122192, \"time-step\": 4}, {\"accuracy\": 0.25, \"loss\": 1.4800519943237305, \"time-step\": 5}, {\"accuracy\": 0.25, \"loss\": 1.4740664958953857, \"time-step\": 6}, {\"accuracy\": 0.25, \"loss\": 1.4680211544036865, \"time-step\": 7}, {\"accuracy\": 0.25, \"loss\": 1.4619183540344238, \"time-step\": 8}, {\"accuracy\": 0.25, \"loss\": 1.4557592868804932, \"time-step\": 9}, {\"accuracy\": 0.25, \"loss\": 1.4495450258255005, \"time-step\": 10}, {\"accuracy\": 0.25, \"loss\": 1.4432766437530518, \"time-step\": 11}, {\"accuracy\": 0.25, \"loss\": 1.4369549751281738, \"time-step\": 12}, {\"accuracy\": 0.25, \"loss\": 1.4305806159973145, \"time-step\": 13}, {\"accuracy\": 0.25, \"loss\": 1.4241544008255005, \"time-step\": 14}, {\"accuracy\": 0.25, \"loss\": 1.4176766872406006, \"time-step\": 15}, {\"accuracy\": 0.25, \"loss\": 1.4111483097076416, \"time-step\": 16}, {\"accuracy\": 0.25, \"loss\": 1.4045696258544922, \"time-step\": 17}, {\"accuracy\": 0.25, \"loss\": 1.3979408740997314, \"time-step\": 18}, {\"accuracy\": 0.25, \"loss\": 1.3912628889083862, \"time-step\": 19}, {\"accuracy\": 0.25, \"loss\": 1.3845360279083252, \"time-step\": 20}, {\"accuracy\": 0.25, \"loss\": 1.377760887145996, \"time-step\": 21}, {\"accuracy\": 0.25, \"loss\": 1.3709375858306885, \"time-step\": 22}, {\"accuracy\": 0.25, \"loss\": 1.3640669584274292, \"time-step\": 23}, {\"accuracy\": 0.25, \"loss\": 1.3571492433547974, \"time-step\": 24}, {\"accuracy\": 0.25, \"loss\": 1.3501852750778198, \"time-step\": 25}, {\"accuracy\": 0.25, \"loss\": 1.3431754112243652, \"time-step\": 26}, {\"accuracy\": 0.25, \"loss\": 1.3361200094223022, \"time-step\": 27}, {\"accuracy\": 0.25, \"loss\": 1.3290200233459473, \"time-step\": 28}, {\"accuracy\": 0.25, \"loss\": 1.3218756914138794, \"time-step\": 29}, {\"accuracy\": 0.25, \"loss\": 1.314687728881836, \"time-step\": 30}, {\"accuracy\": 0.25, \"loss\": 1.3074567317962646, \"time-step\": 31}, {\"accuracy\": 0.25, \"loss\": 1.3001835346221924, \"time-step\": 32}, {\"accuracy\": 0.25, \"loss\": 1.2928686141967773, \"time-step\": 33}, {\"accuracy\": 0.25, \"loss\": 1.2855126857757568, \"time-step\": 34}, {\"accuracy\": 0.25, \"loss\": 1.2781165838241577, \"time-step\": 35}, {\"accuracy\": 0.25, \"loss\": 1.2706809043884277, \"time-step\": 36}, {\"accuracy\": 0.25, \"loss\": 1.2632067203521729, \"time-step\": 37}, {\"accuracy\": 0.25, \"loss\": 1.2556945085525513, \"time-step\": 38}, {\"accuracy\": 0.25, \"loss\": 1.2481452226638794, \"time-step\": 39}, {\"accuracy\": 0.25, \"loss\": 1.240559697151184, \"time-step\": 40}, {\"accuracy\": 0.25, \"loss\": 1.2329390048980713, \"time-step\": 41}, {\"accuracy\": 0.25, \"loss\": 1.2252840995788574, \"time-step\": 42}, {\"accuracy\": 0.25, \"loss\": 1.2175958156585693, \"time-step\": 43}, {\"accuracy\": 0.25, \"loss\": 1.2098748683929443, \"time-step\": 44}, {\"accuracy\": 0.25, \"loss\": 1.202122688293457, \"time-step\": 45}, {\"accuracy\": 0.25, \"loss\": 1.1943401098251343, \"time-step\": 46}, {\"accuracy\": 0.25, \"loss\": 1.1865283250808716, \"time-step\": 47}, {\"accuracy\": 0.25, \"loss\": 1.1786885261535645, \"time-step\": 48}, {\"accuracy\": 0.25, \"loss\": 1.1708214282989502, \"time-step\": 49}, {\"accuracy\": 0.25, \"loss\": 1.1629289388656616, \"time-step\": 50}, {\"accuracy\": 0.25, \"loss\": 1.155011534690857, \"time-step\": 51}, {\"accuracy\": 0.25, \"loss\": 1.1470710039138794, \"time-step\": 52}, {\"accuracy\": 0.25, \"loss\": 1.1391081809997559, \"time-step\": 53}, {\"accuracy\": 0.25, \"loss\": 1.131124496459961, \"time-step\": 54}, {\"accuracy\": 0.25, \"loss\": 1.1231215000152588, \"time-step\": 55}, {\"accuracy\": 0.25, \"loss\": 1.1151000261306763, \"time-step\": 56}, {\"accuracy\": 0.25, \"loss\": 1.1070622205734253, \"time-step\": 57}, {\"accuracy\": 0.25, \"loss\": 1.099008560180664, \"time-step\": 58}, {\"accuracy\": 0.25, \"loss\": 1.090941071510315, \"time-step\": 59}, {\"accuracy\": 0.25, \"loss\": 1.0828611850738525, \"time-step\": 60}, {\"accuracy\": 0.25, \"loss\": 1.0747700929641724, \"time-step\": 61}, {\"accuracy\": 0.25, \"loss\": 1.0666694641113281, \"time-step\": 62}, {\"accuracy\": 0.25, \"loss\": 1.0585607290267944, \"time-step\": 63}, {\"accuracy\": 0.25, \"loss\": 1.050445556640625, \"time-step\": 64}, {\"accuracy\": 0.25, \"loss\": 1.0423253774642944, \"time-step\": 65}, {\"accuracy\": 0.25, \"loss\": 1.034201741218567, \"time-step\": 66}, {\"accuracy\": 0.25, \"loss\": 1.0260764360427856, \"time-step\": 67}, {\"accuracy\": 0.25, \"loss\": 1.0179507732391357, \"time-step\": 68}, {\"accuracy\": 0.25, \"loss\": 1.009826421737671, \"time-step\": 69}, {\"accuracy\": 0.25, \"loss\": 1.0017049312591553, \"time-step\": 70}, {\"accuracy\": 0.25, \"loss\": 0.9935883283615112, \"time-step\": 71}, {\"accuracy\": 0.25, \"loss\": 0.9854778051376343, \"time-step\": 72}, {\"accuracy\": 0.25, \"loss\": 0.9773752689361572, \"time-step\": 73}, {\"accuracy\": 0.25, \"loss\": 0.9692823886871338, \"time-step\": 74}, {\"accuracy\": 0.25, \"loss\": 0.9612005949020386, \"time-step\": 75}, {\"accuracy\": 0.25, \"loss\": 0.9531317949295044, \"time-step\": 76}, {\"accuracy\": 0.25, \"loss\": 0.945077657699585, \"time-step\": 77}, {\"accuracy\": 0.25, \"loss\": 0.9370394945144653, \"time-step\": 78}, {\"accuracy\": 0.25, \"loss\": 0.9290193319320679, \"time-step\": 79}, {\"accuracy\": 0.25, \"loss\": 0.921018660068512, \"time-step\": 80}, {\"accuracy\": 0.25, \"loss\": 0.9130392670631409, \"time-step\": 81}, {\"accuracy\": 0.25, \"loss\": 0.9050825834274292, \"time-step\": 82}, {\"accuracy\": 0.25, \"loss\": 0.8971503973007202, \"time-step\": 83}, {\"accuracy\": 0.25, \"loss\": 0.8892441391944885, \"time-step\": 84}, {\"accuracy\": 0.25, \"loss\": 0.8813656568527222, \"time-step\": 85}, {\"accuracy\": 0.25, \"loss\": 0.8735163807868958, \"time-step\": 86}, {\"accuracy\": 0.25, \"loss\": 0.865697979927063, \"time-step\": 87}, {\"accuracy\": 0.25, \"loss\": 0.8579118847846985, \"time-step\": 88}, {\"accuracy\": 0.25, \"loss\": 0.8501597046852112, \"time-step\": 89}, {\"accuracy\": 0.25, \"loss\": 0.8424429893493652, \"time-step\": 90}, {\"accuracy\": 0.25, \"loss\": 0.8347631692886353, \"time-step\": 91}, {\"accuracy\": 0.25, \"loss\": 0.8271217346191406, \"time-step\": 92}, {\"accuracy\": 0.25, \"loss\": 0.819520115852356, \"time-step\": 93}, {\"accuracy\": 0.25, \"loss\": 0.8119597434997559, \"time-step\": 94}, {\"accuracy\": 0.25, \"loss\": 0.8044420480728149, \"time-step\": 95}, {\"accuracy\": 0.25, \"loss\": 0.7969682216644287, \"time-step\": 96}, {\"accuracy\": 0.5, \"loss\": 0.7895396947860718, \"time-step\": 97}, {\"accuracy\": 0.5, \"loss\": 0.7821577787399292, \"time-step\": 98}, {\"accuracy\": 0.5, \"loss\": 0.7748238444328308, \"time-step\": 99}, {\"accuracy\": 0.5, \"loss\": 0.767538845539093, \"time-step\": 100}, {\"accuracy\": 0.5, \"loss\": 0.7603042721748352, \"time-step\": 101}, {\"accuracy\": 0.5, \"loss\": 0.753121018409729, \"time-step\": 102}, {\"accuracy\": 0.5, \"loss\": 0.7459902763366699, \"time-step\": 103}, {\"accuracy\": 0.5, \"loss\": 0.7389131784439087, \"time-step\": 104}, {\"accuracy\": 0.5, \"loss\": 0.7318906784057617, \"time-step\": 105}, {\"accuracy\": 0.5, \"loss\": 0.7249237298965454, \"time-step\": 106}, {\"accuracy\": 0.5, \"loss\": 0.7180136442184448, \"time-step\": 107}, {\"accuracy\": 0.5, \"loss\": 0.7111608386039734, \"time-step\": 108}, {\"accuracy\": 0.5, \"loss\": 0.7043665647506714, \"time-step\": 109}, {\"accuracy\": 0.5, \"loss\": 0.6976313591003418, \"time-step\": 110}, {\"accuracy\": 0.5, \"loss\": 0.6909562945365906, \"time-step\": 111}, {\"accuracy\": 0.5, \"loss\": 0.6843421459197998, \"time-step\": 112}, {\"accuracy\": 0.5, \"loss\": 0.6777893304824829, \"time-step\": 113}, {\"accuracy\": 0.5, \"loss\": 0.6712986826896667, \"time-step\": 114}, {\"accuracy\": 0.5, \"loss\": 0.6648709177970886, \"time-step\": 115}, {\"accuracy\": 0.5, \"loss\": 0.6585065126419067, \"time-step\": 116}, {\"accuracy\": 0.5, \"loss\": 0.6522060036659241, \"time-step\": 117}, {\"accuracy\": 0.5, \"loss\": 0.6459700465202332, \"time-step\": 118}, {\"accuracy\": 0.5, \"loss\": 0.6397987604141235, \"time-step\": 119}, {\"accuracy\": 0.5, \"loss\": 0.6336930394172668, \"time-step\": 120}, {\"accuracy\": 0.5, \"loss\": 0.6276530027389526, \"time-step\": 121}, {\"accuracy\": 0.5, \"loss\": 0.6216789484024048, \"time-step\": 122}, {\"accuracy\": 0.5, \"loss\": 0.6157712936401367, \"time-step\": 123}, {\"accuracy\": 0.5, \"loss\": 0.6099302172660828, \"time-step\": 124}, {\"accuracy\": 0.5, \"loss\": 0.6041560173034668, \"time-step\": 125}, {\"accuracy\": 0.5, \"loss\": 0.5984489917755127, \"time-step\": 126}, {\"accuracy\": 0.5, \"loss\": 0.5928089618682861, \"time-step\": 127}, {\"accuracy\": 0.5, \"loss\": 0.5872364044189453, \"time-step\": 128}, {\"accuracy\": 0.5, \"loss\": 0.5817312002182007, \"time-step\": 129}, {\"accuracy\": 0.5, \"loss\": 0.5762934684753418, \"time-step\": 130}, {\"accuracy\": 0.5, \"loss\": 0.5709232091903687, \"time-step\": 131}, {\"accuracy\": 0.5, \"loss\": 0.5656203031539917, \"time-step\": 132}, {\"accuracy\": 0.5, \"loss\": 0.5603848695755005, \"time-step\": 133}, {\"accuracy\": 0.5, \"loss\": 0.5552167892456055, \"time-step\": 134}, {\"accuracy\": 0.5, \"loss\": 0.5501158833503723, \"time-step\": 135}, {\"accuracy\": 0.5, \"loss\": 0.5450820326805115, \"time-step\": 136}, {\"accuracy\": 0.5, \"loss\": 0.5401151180267334, \"time-step\": 137}, {\"accuracy\": 0.5, \"loss\": 0.5352148413658142, \"time-step\": 138}, {\"accuracy\": 0.5, \"loss\": 0.5303810834884644, \"time-step\": 139}, {\"accuracy\": 0.5, \"loss\": 0.5256134271621704, \"time-step\": 140}, {\"accuracy\": 0.5, \"loss\": 0.5209119319915771, \"time-step\": 141}, {\"accuracy\": 0.5, \"loss\": 0.516275942325592, \"time-step\": 142}, {\"accuracy\": 0.5, \"loss\": 0.5117052793502808, \"time-step\": 143}, {\"accuracy\": 0.5, \"loss\": 0.5071996450424194, \"time-step\": 144}, {\"accuracy\": 0.5, \"loss\": 0.5027585625648499, \"time-step\": 145}, {\"accuracy\": 0.5, \"loss\": 0.49838173389434814, \"time-step\": 146}, {\"accuracy\": 0.5, \"loss\": 0.4940686821937561, \"time-step\": 147}, {\"accuracy\": 0.5, \"loss\": 0.4898190498352051, \"time-step\": 148}, {\"accuracy\": 0.5, \"loss\": 0.48563235998153687, \"time-step\": 149}, {\"accuracy\": 0.5, \"loss\": 0.48150819540023804, \"time-step\": 150}, {\"accuracy\": 0.5, \"loss\": 0.47744596004486084, \"time-step\": 151}, {\"accuracy\": 0.5, \"loss\": 0.4734453558921814, \"time-step\": 152}, {\"accuracy\": 0.5, \"loss\": 0.4695056974887848, \"time-step\": 153}, {\"accuracy\": 0.5, \"loss\": 0.4656265676021576, \"time-step\": 154}, {\"accuracy\": 0.5, \"loss\": 0.46180739998817444, \"time-step\": 155}, {\"accuracy\": 0.5, \"loss\": 0.4580475986003876, \"time-step\": 156}, {\"accuracy\": 0.5, \"loss\": 0.4543467164039612, \"time-step\": 157}, {\"accuracy\": 0.5, \"loss\": 0.45070409774780273, \"time-step\": 158}, {\"accuracy\": 0.5, \"loss\": 0.44711923599243164, \"time-step\": 159}, {\"accuracy\": 0.5, \"loss\": 0.44359150528907776, \"time-step\": 160}, {\"accuracy\": 0.5, \"loss\": 0.44012027978897095, \"time-step\": 161}, {\"accuracy\": 0.5, \"loss\": 0.4367050528526306, \"time-step\": 162}, {\"accuracy\": 0.5, \"loss\": 0.43334513902664185, \"time-step\": 163}, {\"accuracy\": 0.5, \"loss\": 0.43004003167152405, \"time-step\": 164}, {\"accuracy\": 0.5, \"loss\": 0.42678892612457275, \"time-step\": 165}, {\"accuracy\": 0.5, \"loss\": 0.4235913157463074, \"time-step\": 166}, {\"accuracy\": 0.5, \"loss\": 0.42044660449028015, \"time-step\": 167}, {\"accuracy\": 0.5, \"loss\": 0.41735410690307617, \"time-step\": 168}, {\"accuracy\": 0.5, \"loss\": 0.4143132269382477, \"time-step\": 169}, {\"accuracy\": 0.5, \"loss\": 0.41132330894470215, \"time-step\": 170}, {\"accuracy\": 0.5, \"loss\": 0.40838363766670227, \"time-step\": 171}, {\"accuracy\": 0.5, \"loss\": 0.40549367666244507, \"time-step\": 172}, {\"accuracy\": 0.5, \"loss\": 0.4026528000831604, \"time-step\": 173}, {\"accuracy\": 0.5, \"loss\": 0.3998602628707886, \"time-step\": 174}, {\"accuracy\": 0.5, \"loss\": 0.39711546897888184, \"time-step\": 175}, {\"accuracy\": 0.5, \"loss\": 0.39441773295402527, \"time-step\": 176}, {\"accuracy\": 0.5, \"loss\": 0.3917664885520935, \"time-step\": 177}, {\"accuracy\": 0.5, \"loss\": 0.38916105031967163, \"time-step\": 178}, {\"accuracy\": 0.5, \"loss\": 0.3866007924079895, \"time-step\": 179}, {\"accuracy\": 0.5, \"loss\": 0.3840850591659546, \"time-step\": 180}, {\"accuracy\": 0.5, \"loss\": 0.38161322474479675, \"time-step\": 181}, {\"accuracy\": 0.5, \"loss\": 0.37918463349342346, \"time-step\": 182}, {\"accuracy\": 0.5, \"loss\": 0.37679868936538696, \"time-step\": 183}, {\"accuracy\": 0.5, \"loss\": 0.3744547367095947, \"time-step\": 184}, {\"accuracy\": 0.5, \"loss\": 0.3721521496772766, \"time-step\": 185}, {\"accuracy\": 0.5, \"loss\": 0.3698903024196625, \"time-step\": 186}, {\"accuracy\": 0.5, \"loss\": 0.36766859889030457, \"time-step\": 187}, {\"accuracy\": 0.5, \"loss\": 0.36548638343811035, \"time-step\": 188}, {\"accuracy\": 0.5, \"loss\": 0.3633430600166321, \"time-step\": 189}, {\"accuracy\": 0.5, \"loss\": 0.361238032579422, \"time-step\": 190}, {\"accuracy\": 0.5, \"loss\": 0.35917067527770996, \"time-step\": 191}, {\"accuracy\": 0.5, \"loss\": 0.3571404218673706, \"time-step\": 192}, {\"accuracy\": 0.5, \"loss\": 0.35514670610427856, \"time-step\": 193}, {\"accuracy\": 0.5, \"loss\": 0.3531888425350189, \"time-step\": 194}, {\"accuracy\": 0.5, \"loss\": 0.35126638412475586, \"time-step\": 195}, {\"accuracy\": 0.5, \"loss\": 0.3493786156177521, \"time-step\": 196}, {\"accuracy\": 0.5, \"loss\": 0.34752511978149414, \"time-step\": 197}, {\"accuracy\": 0.5, \"loss\": 0.34570518136024475, \"time-step\": 198}, {\"accuracy\": 0.5, \"loss\": 0.3439183235168457, \"time-step\": 199}, {\"accuracy\": 0.5, \"loss\": 0.34216398000717163, \"time-step\": 200}, {\"accuracy\": 0.25, \"loss\": 0.3404415249824524, \"time-step\": 201}, {\"accuracy\": 0.25, \"loss\": 0.33875054121017456, \"time-step\": 202}, {\"accuracy\": 0.25, \"loss\": 0.33709046244621277, \"time-step\": 203}, {\"accuracy\": 0.25, \"loss\": 0.3354607820510864, \"time-step\": 204}, {\"accuracy\": 0.25, \"loss\": 0.3338608741760254, \"time-step\": 205}, {\"accuracy\": 0.25, \"loss\": 0.3322903513908386, \"time-step\": 206}, {\"accuracy\": 0.25, \"loss\": 0.33074861764907837, \"time-step\": 207}, {\"accuracy\": 0.25, \"loss\": 0.32923516631126404, \"time-step\": 208}, {\"accuracy\": 0.25, \"loss\": 0.3277495503425598, \"time-step\": 209}, {\"accuracy\": 0.25, \"loss\": 0.3262912631034851, \"time-step\": 210}, {\"accuracy\": 0.25, \"loss\": 0.3248597979545593, \"time-step\": 211}, {\"accuracy\": 0.25, \"loss\": 0.32345473766326904, \"time-step\": 212}, {\"accuracy\": 0.25, \"loss\": 0.32207560539245605, \"time-step\": 213}, {\"accuracy\": 0.25, \"loss\": 0.320721834897995, \"time-step\": 214}, {\"accuracy\": 0.25, \"loss\": 0.319393128156662, \"time-step\": 215}, {\"accuracy\": 0.25, \"loss\": 0.31808891892433167, \"time-step\": 216}, {\"accuracy\": 0.25, \"loss\": 0.3168087303638458, \"time-step\": 217}, {\"accuracy\": 0.25, \"loss\": 0.315552294254303, \"time-step\": 218}, {\"accuracy\": 0.25, \"loss\": 0.31431901454925537, \"time-step\": 219}, {\"accuracy\": 0.25, \"loss\": 0.3131085932254791, \"time-step\": 220}, {\"accuracy\": 0.25, \"loss\": 0.31192055344581604, \"time-step\": 221}, {\"accuracy\": 0.25, \"loss\": 0.3107544779777527, \"time-step\": 222}, {\"accuracy\": 0.25, \"loss\": 0.30960994958877563, \"time-step\": 223}, {\"accuracy\": 0.25, \"loss\": 0.3084866404533386, \"time-step\": 224}, {\"accuracy\": 0.25, \"loss\": 0.30738404393196106, \"time-step\": 225}, {\"accuracy\": 0.25, \"loss\": 0.30630189180374146, \"time-step\": 226}, {\"accuracy\": 0.25, \"loss\": 0.3052397668361664, \"time-step\": 227}, {\"accuracy\": 0.25, \"loss\": 0.3041972815990448, \"time-step\": 228}, {\"accuracy\": 0.25, \"loss\": 0.30317404866218567, \"time-step\": 229}, {\"accuracy\": 0.25, \"loss\": 0.3021697998046875, \"time-step\": 230}, {\"accuracy\": 0.25, \"loss\": 0.3011840879917145, \"time-step\": 231}, {\"accuracy\": 0.25, \"loss\": 0.3002166152000427, \"time-step\": 232}, {\"accuracy\": 0.25, \"loss\": 0.2992669939994812, \"time-step\": 233}, {\"accuracy\": 0.25, \"loss\": 0.2983349561691284, \"time-step\": 234}, {\"accuracy\": 0.25, \"loss\": 0.29742011427879333, \"time-step\": 235}, {\"accuracy\": 0.25, \"loss\": 0.2965221703052521, \"time-step\": 236}, {\"accuracy\": 0.25, \"loss\": 0.2956407964229584, \"time-step\": 237}, {\"accuracy\": 0.25, \"loss\": 0.29477566480636597, \"time-step\": 238}, {\"accuracy\": 0.25, \"loss\": 0.293926477432251, \"time-step\": 239}, {\"accuracy\": 0.25, \"loss\": 0.2930929958820343, \"time-step\": 240}, {\"accuracy\": 0.25, \"loss\": 0.2922748923301697, \"time-step\": 241}, {\"accuracy\": 0.25, \"loss\": 0.2914717495441437, \"time-step\": 242}, {\"accuracy\": 0.25, \"loss\": 0.29068344831466675, \"time-step\": 243}, {\"accuracy\": 0.25, \"loss\": 0.2899096608161926, \"time-step\": 244}, {\"accuracy\": 0.25, \"loss\": 0.28915008902549744, \"time-step\": 245}, {\"accuracy\": 0.25, \"loss\": 0.2884044945240021, \"time-step\": 246}, {\"accuracy\": 0.25, \"loss\": 0.2876725196838379, \"time-step\": 247}, {\"accuracy\": 0.25, \"loss\": 0.28695398569107056, \"time-step\": 248}, {\"accuracy\": 0.25, \"loss\": 0.28624868392944336, \"time-step\": 249}, {\"accuracy\": 0.25, \"loss\": 0.28555622696876526, \"time-step\": 250}, {\"accuracy\": 0.25, \"loss\": 0.2848765254020691, \"time-step\": 251}, {\"accuracy\": 0.25, \"loss\": 0.2842091917991638, \"time-step\": 252}, {\"accuracy\": 0.25, \"loss\": 0.2835541367530823, \"time-step\": 253}, {\"accuracy\": 0.25, \"loss\": 0.2829110026359558, \"time-step\": 254}, {\"accuracy\": 0.25, \"loss\": 0.2822795808315277, \"time-step\": 255}, {\"accuracy\": 0.25, \"loss\": 0.28165969252586365, \"time-step\": 256}, {\"accuracy\": 0.25, \"loss\": 0.2810510993003845, \"time-step\": 257}, {\"accuracy\": 0.25, \"loss\": 0.2804535925388336, \"time-step\": 258}, {\"accuracy\": 0.25, \"loss\": 0.2798669636249542, \"time-step\": 259}, {\"accuracy\": 0.25, \"loss\": 0.279291033744812, \"time-step\": 260}, {\"accuracy\": 0.25, \"loss\": 0.2787255346775055, \"time-step\": 261}, {\"accuracy\": 0.25, \"loss\": 0.27817025780677795, \"time-step\": 262}, {\"accuracy\": 0.25, \"loss\": 0.27762508392333984, \"time-step\": 263}, {\"accuracy\": 0.25, \"loss\": 0.27708977460861206, \"time-step\": 264}, {\"accuracy\": 0.25, \"loss\": 0.27656418085098267, \"time-step\": 265}, {\"accuracy\": 0.25, \"loss\": 0.27604806423187256, \"time-step\": 266}, {\"accuracy\": 0.25, \"loss\": 0.2755413055419922, \"time-step\": 267}, {\"accuracy\": 0.25, \"loss\": 0.2750437259674072, \"time-step\": 268}, {\"accuracy\": 0.25, \"loss\": 0.2745550274848938, \"time-step\": 269}, {\"accuracy\": 0.25, \"loss\": 0.2740752696990967, \"time-step\": 270}, {\"accuracy\": 0.25, \"loss\": 0.27360403537750244, \"time-step\": 271}, {\"accuracy\": 0.25, \"loss\": 0.2731413245201111, \"time-step\": 272}, {\"accuracy\": 0.25, \"loss\": 0.2726869583129883, \"time-step\": 273}, {\"accuracy\": 0.25, \"loss\": 0.2722407281398773, \"time-step\": 274}, {\"accuracy\": 0.25, \"loss\": 0.27180251479148865, \"time-step\": 275}, {\"accuracy\": 0.25, \"loss\": 0.2713721692562103, \"time-step\": 276}, {\"accuracy\": 0.25, \"loss\": 0.2709495425224304, \"time-step\": 277}, {\"accuracy\": 0.25, \"loss\": 0.2705344557762146, \"time-step\": 278}, {\"accuracy\": 0.25, \"loss\": 0.2701268494129181, \"time-step\": 279}, {\"accuracy\": 0.25, \"loss\": 0.2697264850139618, \"time-step\": 280}, {\"accuracy\": 0.25, \"loss\": 0.2693333029747009, \"time-step\": 281}, {\"accuracy\": 0.25, \"loss\": 0.26894715428352356, \"time-step\": 282}, {\"accuracy\": 0.25, \"loss\": 0.26856786012649536, \"time-step\": 283}, {\"accuracy\": 0.25, \"loss\": 0.26819533109664917, \"time-step\": 284}, {\"accuracy\": 0.25, \"loss\": 0.2678294777870178, \"time-step\": 285}, {\"accuracy\": 0.25, \"loss\": 0.2674700915813446, \"time-step\": 286}, {\"accuracy\": 0.25, \"loss\": 0.26711714267730713, \"time-step\": 287}, {\"accuracy\": 0.25, \"loss\": 0.26677048206329346, \"time-step\": 288}, {\"accuracy\": 0.25, \"loss\": 0.26642993092536926, \"time-step\": 289}, {\"accuracy\": 0.25, \"loss\": 0.26609548926353455, \"time-step\": 290}, {\"accuracy\": 0.25, \"loss\": 0.265766978263855, \"time-step\": 291}, {\"accuracy\": 0.25, \"loss\": 0.265444278717041, \"time-step\": 292}, {\"accuracy\": 0.25, \"loss\": 0.2651273012161255, \"time-step\": 293}, {\"accuracy\": 0.25, \"loss\": 0.264816015958786, \"time-step\": 294}, {\"accuracy\": 0.25, \"loss\": 0.2645101249217987, \"time-step\": 295}, {\"accuracy\": 0.25, \"loss\": 0.2642097473144531, \"time-step\": 296}, {\"accuracy\": 0.25, \"loss\": 0.26391470432281494, \"time-step\": 297}, {\"accuracy\": 0.25, \"loss\": 0.2636248767375946, \"time-step\": 298}, {\"accuracy\": 0.25, \"loss\": 0.26334017515182495, \"time-step\": 299}, {\"accuracy\": 0.25, \"loss\": 0.2630605101585388, \"time-step\": 300}, {\"accuracy\": 0.25, \"loss\": 0.2627858519554138, \"time-step\": 301}, {\"accuracy\": 0.25, \"loss\": 0.26251599192619324, \"time-step\": 302}, {\"accuracy\": 0.25, \"loss\": 0.2622509300708771, \"time-step\": 303}, {\"accuracy\": 0.25, \"loss\": 0.2619905471801758, \"time-step\": 304}, {\"accuracy\": 0.25, \"loss\": 0.26173487305641174, \"time-step\": 305}, {\"accuracy\": 0.25, \"loss\": 0.26148369908332825, \"time-step\": 306}, {\"accuracy\": 0.25, \"loss\": 0.26123690605163574, \"time-step\": 307}, {\"accuracy\": 0.25, \"loss\": 0.260994553565979, \"time-step\": 308}, {\"accuracy\": 0.25, \"loss\": 0.2607564926147461, \"time-step\": 309}, {\"accuracy\": 0.25, \"loss\": 0.26052266359329224, \"time-step\": 310}, {\"accuracy\": 0.25, \"loss\": 0.2602929472923279, \"time-step\": 311}, {\"accuracy\": 0.25, \"loss\": 0.260067343711853, \"time-step\": 312}, {\"accuracy\": 0.25, \"loss\": 0.2598458230495453, \"time-step\": 313}, {\"accuracy\": 0.25, \"loss\": 0.25962817668914795, \"time-step\": 314}, {\"accuracy\": 0.25, \"loss\": 0.259414404630661, \"time-step\": 315}, {\"accuracy\": 0.25, \"loss\": 0.2592044472694397, \"time-step\": 316}, {\"accuracy\": 0.25, \"loss\": 0.2589983344078064, \"time-step\": 317}, {\"accuracy\": 0.25, \"loss\": 0.258795827627182, \"time-step\": 318}, {\"accuracy\": 0.25, \"loss\": 0.2585970163345337, \"time-step\": 319}, {\"accuracy\": 0.25, \"loss\": 0.2584017515182495, \"time-step\": 320}, {\"accuracy\": 0.25, \"loss\": 0.2582099437713623, \"time-step\": 321}, {\"accuracy\": 0.25, \"loss\": 0.25802165269851685, \"time-step\": 322}, {\"accuracy\": 0.25, \"loss\": 0.25783678889274597, \"time-step\": 323}, {\"accuracy\": 0.25, \"loss\": 0.25765520334243774, \"time-step\": 324}, {\"accuracy\": 0.25, \"loss\": 0.25747689604759216, \"time-step\": 325}, {\"accuracy\": 0.25, \"loss\": 0.25730183720588684, \"time-step\": 326}, {\"accuracy\": 0.25, \"loss\": 0.2571300268173218, \"time-step\": 327}, {\"accuracy\": 0.25, \"loss\": 0.2569613456726074, \"time-step\": 328}, {\"accuracy\": 0.25, \"loss\": 0.2567957043647766, \"time-step\": 329}, {\"accuracy\": 0.25, \"loss\": 0.25663313269615173, \"time-step\": 330}, {\"accuracy\": 0.25, \"loss\": 0.2564736008644104, \"time-step\": 331}, {\"accuracy\": 0.25, \"loss\": 0.2563169598579407, \"time-step\": 332}, {\"accuracy\": 0.25, \"loss\": 0.25616323947906494, \"time-step\": 333}, {\"accuracy\": 0.25, \"loss\": 0.2560123801231384, \"time-step\": 334}, {\"accuracy\": 0.25, \"loss\": 0.25586438179016113, \"time-step\": 335}, {\"accuracy\": 0.25, \"loss\": 0.2557190954685211, \"time-step\": 336}, {\"accuracy\": 0.25, \"loss\": 0.25557658076286316, \"time-step\": 337}, {\"accuracy\": 0.25, \"loss\": 0.2554367780685425, \"time-step\": 338}, {\"accuracy\": 0.25, \"loss\": 0.2552995979785919, \"time-step\": 339}, {\"accuracy\": 0.25, \"loss\": 0.2551650106906891, \"time-step\": 340}, {\"accuracy\": 0.25, \"loss\": 0.25503304600715637, \"time-step\": 341}, {\"accuracy\": 0.25, \"loss\": 0.254903644323349, \"time-step\": 342}, {\"accuracy\": 0.25, \"loss\": 0.2547766864299774, \"time-step\": 343}, {\"accuracy\": 0.25, \"loss\": 0.254652202129364, \"time-step\": 344}, {\"accuracy\": 0.25, \"loss\": 0.254530131816864, \"time-step\": 345}, {\"accuracy\": 0.25, \"loss\": 0.2544105052947998, \"time-step\": 346}, {\"accuracy\": 0.25, \"loss\": 0.254293292760849, \"time-step\": 347}, {\"accuracy\": 0.25, \"loss\": 0.25417834520339966, \"time-step\": 348}, {\"accuracy\": 0.25, \"loss\": 0.2540656626224518, \"time-step\": 349}, {\"accuracy\": 0.25, \"loss\": 0.25395530462265015, \"time-step\": 350}, {\"accuracy\": 0.25, \"loss\": 0.2538471519947052, \"time-step\": 351}, {\"accuracy\": 0.25, \"loss\": 0.25374117493629456, \"time-step\": 352}, {\"accuracy\": 0.25, \"loss\": 0.2536374032497406, \"time-step\": 353}, {\"accuracy\": 0.25, \"loss\": 0.25353574752807617, \"time-step\": 354}, {\"accuracy\": 0.25, \"loss\": 0.2534361481666565, \"time-step\": 355}, {\"accuracy\": 0.25, \"loss\": 0.2533387243747711, \"time-step\": 356}, {\"accuracy\": 0.25, \"loss\": 0.2532432973384857, \"time-step\": 357}, {\"accuracy\": 0.25, \"loss\": 0.2531498670578003, \"time-step\": 358}, {\"accuracy\": 0.25, \"loss\": 0.2530584931373596, \"time-step\": 359}, {\"accuracy\": 0.25, \"loss\": 0.25296899676322937, \"time-step\": 360}, {\"accuracy\": 0.25, \"loss\": 0.2528814971446991, \"time-step\": 361}, {\"accuracy\": 0.25, \"loss\": 0.25279587507247925, \"time-step\": 362}, {\"accuracy\": 0.25, \"loss\": 0.25271210074424744, \"time-step\": 363}, {\"accuracy\": 0.25, \"loss\": 0.25263020396232605, \"time-step\": 364}, {\"accuracy\": 0.25, \"loss\": 0.25255006551742554, \"time-step\": 365}, {\"accuracy\": 0.25, \"loss\": 0.25247180461883545, \"time-step\": 366}, {\"accuracy\": 0.25, \"loss\": 0.25239524245262146, \"time-step\": 367}, {\"accuracy\": 0.25, \"loss\": 0.25232046842575073, \"time-step\": 368}, {\"accuracy\": 0.25, \"loss\": 0.2522473633289337, \"time-step\": 369}, {\"accuracy\": 0.25, \"loss\": 0.2521759867668152, \"time-step\": 370}, {\"accuracy\": 0.25, \"loss\": 0.2521062195301056, \"time-step\": 371}, {\"accuracy\": 0.25, \"loss\": 0.2520381212234497, \"time-step\": 372}, {\"accuracy\": 0.25, \"loss\": 0.25197160243988037, \"time-step\": 373}, {\"accuracy\": 0.25, \"loss\": 0.25190669298171997, \"time-step\": 374}, {\"accuracy\": 0.25, \"loss\": 0.25184330344200134, \"time-step\": 375}, {\"accuracy\": 0.25, \"loss\": 0.25178149342536926, \"time-step\": 376}, {\"accuracy\": 0.25, \"loss\": 0.2517211139202118, \"time-step\": 377}, {\"accuracy\": 0.25, \"loss\": 0.2516622543334961, \"time-step\": 378}, {\"accuracy\": 0.25, \"loss\": 0.2516048550605774, \"time-step\": 379}, {\"accuracy\": 0.25, \"loss\": 0.2515488862991333, \"time-step\": 380}, {\"accuracy\": 0.25, \"loss\": 0.25149428844451904, \"time-step\": 381}, {\"accuracy\": 0.25, \"loss\": 0.2514410614967346, \"time-step\": 382}, {\"accuracy\": 0.25, \"loss\": 0.2513892352581024, \"time-step\": 383}, {\"accuracy\": 0.25, \"loss\": 0.2513387203216553, \"time-step\": 384}, {\"accuracy\": 0.25, \"loss\": 0.2512894868850708, \"time-step\": 385}, {\"accuracy\": 0.25, \"loss\": 0.2512415647506714, \"time-step\": 386}, {\"accuracy\": 0.25, \"loss\": 0.25119489431381226, \"time-step\": 387}, {\"accuracy\": 0.25, \"loss\": 0.251149445772171, \"time-step\": 388}, {\"accuracy\": 0.25, \"loss\": 0.2511051893234253, \"time-step\": 389}, {\"accuracy\": 0.25, \"loss\": 0.25106215476989746, \"time-step\": 390}, {\"accuracy\": 0.25, \"loss\": 0.251020222902298, \"time-step\": 391}, {\"accuracy\": 0.25, \"loss\": 0.250979483127594, \"time-step\": 392}, {\"accuracy\": 0.25, \"loss\": 0.25093984603881836, \"time-step\": 393}, {\"accuracy\": 0.25, \"loss\": 0.2509012520313263, \"time-step\": 394}, {\"accuracy\": 0.5, \"loss\": 0.2508637309074402, \"time-step\": 395}, {\"accuracy\": 0.5, \"loss\": 0.25082728266716003, \"time-step\": 396}, {\"accuracy\": 0.5, \"loss\": 0.2507918179035187, \"time-step\": 397}, {\"accuracy\": 0.5, \"loss\": 0.2507574260234833, \"time-step\": 398}, {\"accuracy\": 0.5, \"loss\": 0.2507238984107971, \"time-step\": 399}, {\"accuracy\": 0.5, \"loss\": 0.25069135427474976, \"time-step\": 400}, {\"accuracy\": 0.5, \"loss\": 0.2506597638130188, \"time-step\": 401}, {\"accuracy\": 0.5, \"loss\": 0.2506290376186371, \"time-step\": 402}, {\"accuracy\": 0.5, \"loss\": 0.2505992352962494, \"time-step\": 403}, {\"accuracy\": 0.5, \"loss\": 0.25057026743888855, \"time-step\": 404}, {\"accuracy\": 0.5, \"loss\": 0.25054213404655457, \"time-step\": 405}, {\"accuracy\": 0.5, \"loss\": 0.25051480531692505, \"time-step\": 406}, {\"accuracy\": 0.5, \"loss\": 0.2504882514476776, \"time-step\": 407}, {\"accuracy\": 0.5, \"loss\": 0.25046250224113464, \"time-step\": 408}, {\"accuracy\": 0.5, \"loss\": 0.2504374086856842, \"time-step\": 409}, {\"accuracy\": 0.5, \"loss\": 0.25041311979293823, \"time-step\": 410}, {\"accuracy\": 0.5, \"loss\": 0.2503895163536072, \"time-step\": 411}, {\"accuracy\": 0.5, \"loss\": 0.25036656856536865, \"time-step\": 412}, {\"accuracy\": 0.5, \"loss\": 0.25034430623054504, \"time-step\": 413}, {\"accuracy\": 0.5, \"loss\": 0.2503226399421692, \"time-step\": 414}, {\"accuracy\": 0.5, \"loss\": 0.25030165910720825, \"time-step\": 415}, {\"accuracy\": 0.5, \"loss\": 0.2502812147140503, \"time-step\": 416}, {\"accuracy\": 0.5, \"loss\": 0.2502613365650177, \"time-step\": 417}, {\"accuracy\": 0.5, \"loss\": 0.2502420246601105, \"time-step\": 418}, {\"accuracy\": 0.5, \"loss\": 0.25022315979003906, \"time-step\": 419}, {\"accuracy\": 0.5, \"loss\": 0.2502049207687378, \"time-step\": 420}, {\"accuracy\": 0.5, \"loss\": 0.25018709897994995, \"time-step\": 421}, {\"accuracy\": 0.5, \"loss\": 0.2501698136329651, \"time-step\": 422}, {\"accuracy\": 0.75, \"loss\": 0.2501528859138489, \"time-step\": 423}, {\"accuracy\": 0.75, \"loss\": 0.25013646483421326, \"time-step\": 424}, {\"accuracy\": 0.75, \"loss\": 0.2501204013824463, \"time-step\": 425}, {\"accuracy\": 0.75, \"loss\": 0.25010472536087036, \"time-step\": 426}, {\"accuracy\": 0.75, \"loss\": 0.2500894367694855, \"time-step\": 427}, {\"accuracy\": 0.75, \"loss\": 0.25007450580596924, \"time-step\": 428}, {\"accuracy\": 0.75, \"loss\": 0.2500598430633545, \"time-step\": 429}, {\"accuracy\": 0.75, \"loss\": 0.2500455975532532, \"time-step\": 430}, {\"accuracy\": 0.75, \"loss\": 0.25003162026405334, \"time-step\": 431}, {\"accuracy\": 0.75, \"loss\": 0.2500178813934326, \"time-step\": 432}, {\"accuracy\": 0.75, \"loss\": 0.25000444054603577, \"time-step\": 433}, {\"accuracy\": 0.75, \"loss\": 0.24999120831489563, \"time-step\": 434}, {\"accuracy\": 0.75, \"loss\": 0.2499781996011734, \"time-step\": 435}, {\"accuracy\": 0.75, \"loss\": 0.24996545910835266, \"time-step\": 436}, {\"accuracy\": 0.75, \"loss\": 0.24995288252830505, \"time-step\": 437}, {\"accuracy\": 0.75, \"loss\": 0.24994046986103058, \"time-step\": 438}, {\"accuracy\": 0.75, \"loss\": 0.24992819130420685, \"time-step\": 439}, {\"accuracy\": 0.75, \"loss\": 0.24991609156131744, \"time-step\": 440}, {\"accuracy\": 0.75, \"loss\": 0.24990414083003998, \"time-step\": 441}, {\"accuracy\": 0.75, \"loss\": 0.24989232420921326, \"time-step\": 442}, {\"accuracy\": 0.75, \"loss\": 0.2498805820941925, \"time-step\": 443}, {\"accuracy\": 0.75, \"loss\": 0.2498689591884613, \"time-step\": 444}, {\"accuracy\": 0.75, \"loss\": 0.24985739588737488, \"time-step\": 445}, {\"accuracy\": 0.75, \"loss\": 0.24984590709209442, \"time-step\": 446}, {\"accuracy\": 0.75, \"loss\": 0.24983450770378113, \"time-step\": 447}, {\"accuracy\": 0.75, \"loss\": 0.24982309341430664, \"time-step\": 448}, {\"accuracy\": 0.75, \"loss\": 0.2498117983341217, \"time-step\": 449}, {\"accuracy\": 0.75, \"loss\": 0.24980048835277557, \"time-step\": 450}, {\"accuracy\": 0.75, \"loss\": 0.24978920817375183, \"time-step\": 451}, {\"accuracy\": 0.75, \"loss\": 0.24977795779705048, \"time-step\": 452}, {\"accuracy\": 0.75, \"loss\": 0.24976672232151031, \"time-step\": 453}, {\"accuracy\": 0.75, \"loss\": 0.24975545704364777, \"time-step\": 454}, {\"accuracy\": 0.75, \"loss\": 0.2497442364692688, \"time-step\": 455}, {\"accuracy\": 0.75, \"loss\": 0.24973294138908386, \"time-step\": 456}, {\"accuracy\": 0.75, \"loss\": 0.2497216761112213, \"time-step\": 457}, {\"accuracy\": 0.75, \"loss\": 0.2497103363275528, \"time-step\": 458}, {\"accuracy\": 0.75, \"loss\": 0.24969902634620667, \"time-step\": 459}, {\"accuracy\": 0.75, \"loss\": 0.24968770146369934, \"time-step\": 460}, {\"accuracy\": 0.75, \"loss\": 0.24967628717422485, \"time-step\": 461}, {\"accuracy\": 0.75, \"loss\": 0.24966487288475037, \"time-step\": 462}, {\"accuracy\": 0.75, \"loss\": 0.2496534287929535, \"time-step\": 463}, {\"accuracy\": 0.75, \"loss\": 0.24964191019535065, \"time-step\": 464}, {\"accuracy\": 0.75, \"loss\": 0.24963034689426422, \"time-step\": 465}, {\"accuracy\": 0.75, \"loss\": 0.2496187686920166, \"time-step\": 466}, {\"accuracy\": 0.75, \"loss\": 0.249607115983963, \"time-step\": 467}, {\"accuracy\": 0.75, \"loss\": 0.24959540367126465, \"time-step\": 468}, {\"accuracy\": 0.75, \"loss\": 0.24958372116088867, \"time-step\": 469}, {\"accuracy\": 0.75, \"loss\": 0.24957190454006195, \"time-step\": 470}, {\"accuracy\": 0.75, \"loss\": 0.24956010282039642, \"time-step\": 471}, {\"accuracy\": 0.75, \"loss\": 0.24954818189144135, \"time-step\": 472}, {\"accuracy\": 0.75, \"loss\": 0.24953627586364746, \"time-step\": 473}, {\"accuracy\": 0.75, \"loss\": 0.2495243102312088, \"time-step\": 474}, {\"accuracy\": 0.75, \"loss\": 0.24951225519180298, \"time-step\": 475}, {\"accuracy\": 0.75, \"loss\": 0.24950018525123596, \"time-step\": 476}, {\"accuracy\": 0.75, \"loss\": 0.24948804080486298, \"time-step\": 477}, {\"accuracy\": 0.75, \"loss\": 0.24947589635849, \"time-step\": 478}, {\"accuracy\": 0.75, \"loss\": 0.24946367740631104, \"time-step\": 479}, {\"accuracy\": 0.75, \"loss\": 0.2494513988494873, \"time-step\": 480}, {\"accuracy\": 0.75, \"loss\": 0.24943907558918, \"time-step\": 481}, {\"accuracy\": 0.75, \"loss\": 0.24942675232887268, \"time-step\": 482}, {\"accuracy\": 0.75, \"loss\": 0.2494143545627594, \"time-step\": 483}, {\"accuracy\": 0.75, \"loss\": 0.24940189719200134, \"time-step\": 484}, {\"accuracy\": 0.75, \"loss\": 0.2493894100189209, \"time-step\": 485}, {\"accuracy\": 0.75, \"loss\": 0.24937687814235687, \"time-step\": 486}, {\"accuracy\": 0.75, \"loss\": 0.24936431646347046, \"time-step\": 487}, {\"accuracy\": 0.75, \"loss\": 0.24935171008110046, \"time-step\": 488}, {\"accuracy\": 0.75, \"loss\": 0.24933910369873047, \"time-step\": 489}, {\"accuracy\": 0.75, \"loss\": 0.2493264228105545, \"time-step\": 490}, {\"accuracy\": 0.75, \"loss\": 0.24931375682353973, \"time-step\": 491}, {\"accuracy\": 0.75, \"loss\": 0.249301016330719, \"time-step\": 492}, {\"accuracy\": 0.75, \"loss\": 0.24928824603557587, \"time-step\": 493}, {\"accuracy\": 0.75, \"loss\": 0.24927544593811035, \"time-step\": 494}, {\"accuracy\": 0.75, \"loss\": 0.24926261603832245, \"time-step\": 495}, {\"accuracy\": 0.75, \"loss\": 0.24924971163272858, \"time-step\": 496}, {\"accuracy\": 0.75, \"loss\": 0.24923686683177948, \"time-step\": 497}, {\"accuracy\": 0.75, \"loss\": 0.24922394752502441, \"time-step\": 498}, {\"accuracy\": 0.75, \"loss\": 0.24921099841594696, \"time-step\": 499}, {\"accuracy\": 0.75, \"loss\": 0.2491980344057083, \"time-step\": 500}, {\"accuracy\": 0.75, \"loss\": 0.24918505549430847, \"time-step\": 501}, {\"accuracy\": 0.75, \"loss\": 0.24917200207710266, \"time-step\": 502}, {\"accuracy\": 0.75, \"loss\": 0.24915897846221924, \"time-step\": 503}, {\"accuracy\": 0.75, \"loss\": 0.24914589524269104, \"time-step\": 504}, {\"accuracy\": 0.75, \"loss\": 0.24913281202316284, \"time-step\": 505}, {\"accuracy\": 0.75, \"loss\": 0.24911974370479584, \"time-step\": 506}, {\"accuracy\": 0.75, \"loss\": 0.24910661578178406, \"time-step\": 507}, {\"accuracy\": 0.75, \"loss\": 0.2490934580564499, \"time-step\": 508}, {\"accuracy\": 0.75, \"loss\": 0.24908031523227692, \"time-step\": 509}, {\"accuracy\": 0.75, \"loss\": 0.24906709790229797, \"time-step\": 510}, {\"accuracy\": 0.75, \"loss\": 0.24905389547348022, \"time-step\": 511}, {\"accuracy\": 0.75, \"loss\": 0.24904075264930725, \"time-step\": 512}, {\"accuracy\": 0.75, \"loss\": 0.24902749061584473, \"time-step\": 513}, {\"accuracy\": 0.75, \"loss\": 0.24901428818702698, \"time-step\": 514}, {\"accuracy\": 0.75, \"loss\": 0.24900104105472565, \"time-step\": 515}, {\"accuracy\": 0.75, \"loss\": 0.24898776412010193, \"time-step\": 516}, {\"accuracy\": 0.75, \"loss\": 0.24897445738315582, \"time-step\": 517}, {\"accuracy\": 0.75, \"loss\": 0.2489611804485321, \"time-step\": 518}, {\"accuracy\": 0.75, \"loss\": 0.2489478886127472, \"time-step\": 519}, {\"accuracy\": 0.75, \"loss\": 0.2489345669746399, \"time-step\": 520}, {\"accuracy\": 0.75, \"loss\": 0.2489212304353714, \"time-step\": 521}, {\"accuracy\": 0.75, \"loss\": 0.2489078789949417, \"time-step\": 522}, {\"accuracy\": 0.75, \"loss\": 0.24889454245567322, \"time-step\": 523}, {\"accuracy\": 0.75, \"loss\": 0.24888113141059875, \"time-step\": 524}, {\"accuracy\": 0.75, \"loss\": 0.24886777997016907, \"time-step\": 525}, {\"accuracy\": 0.75, \"loss\": 0.248854398727417, \"time-step\": 526}, {\"accuracy\": 0.75, \"loss\": 0.24884100258350372, \"time-step\": 527}, {\"accuracy\": 0.75, \"loss\": 0.24882759153842926, \"time-step\": 528}, {\"accuracy\": 0.75, \"loss\": 0.2488141506910324, \"time-step\": 529}, {\"accuracy\": 0.75, \"loss\": 0.24880072474479675, \"time-step\": 530}, {\"accuracy\": 0.75, \"loss\": 0.2487873136997223, \"time-step\": 531}, {\"accuracy\": 0.75, \"loss\": 0.24877387285232544, \"time-step\": 532}, {\"accuracy\": 0.75, \"loss\": 0.2487604022026062, \"time-step\": 533}, {\"accuracy\": 0.75, \"loss\": 0.24874694645404816, \"time-step\": 534}, {\"accuracy\": 0.75, \"loss\": 0.24873346090316772, \"time-step\": 535}, {\"accuracy\": 0.75, \"loss\": 0.2487199902534485, \"time-step\": 536}, {\"accuracy\": 0.75, \"loss\": 0.24870644509792328, \"time-step\": 537}, {\"accuracy\": 0.75, \"loss\": 0.24869295954704285, \"time-step\": 538}, {\"accuracy\": 0.75, \"loss\": 0.24867947399616241, \"time-step\": 539}, {\"accuracy\": 0.75, \"loss\": 0.2486659586429596, \"time-step\": 540}, {\"accuracy\": 0.75, \"loss\": 0.2486523985862732, \"time-step\": 541}, {\"accuracy\": 0.75, \"loss\": 0.24863886833190918, \"time-step\": 542}, {\"accuracy\": 0.75, \"loss\": 0.24862529337406158, \"time-step\": 543}, {\"accuracy\": 0.75, \"loss\": 0.248611718416214, \"time-step\": 544}, {\"accuracy\": 0.75, \"loss\": 0.2485981285572052, \"time-step\": 545}, {\"accuracy\": 0.75, \"loss\": 0.2485845386981964, \"time-step\": 546}, {\"accuracy\": 0.75, \"loss\": 0.24857090413570404, \"time-step\": 547}, {\"accuracy\": 0.75, \"loss\": 0.24855728447437286, \"time-step\": 548}, {\"accuracy\": 0.75, \"loss\": 0.24854367971420288, \"time-step\": 549}, {\"accuracy\": 0.75, \"loss\": 0.2485300600528717, \"time-step\": 550}, {\"accuracy\": 0.75, \"loss\": 0.24851632118225098, \"time-step\": 551}, {\"accuracy\": 0.75, \"loss\": 0.24850264191627502, \"time-step\": 552}, {\"accuracy\": 0.75, \"loss\": 0.24848894774913788, \"time-step\": 553}, {\"accuracy\": 0.75, \"loss\": 0.24847525358200073, \"time-step\": 554}, {\"accuracy\": 0.75, \"loss\": 0.2484615445137024, \"time-step\": 555}, {\"accuracy\": 0.75, \"loss\": 0.24844782054424286, \"time-step\": 556}, {\"accuracy\": 0.75, \"loss\": 0.24843400716781616, \"time-step\": 557}, {\"accuracy\": 0.75, \"loss\": 0.24842023849487305, \"time-step\": 558}, {\"accuracy\": 0.75, \"loss\": 0.24840641021728516, \"time-step\": 559}, {\"accuracy\": 0.75, \"loss\": 0.24839256703853607, \"time-step\": 560}, {\"accuracy\": 0.75, \"loss\": 0.24837875366210938, \"time-step\": 561}, {\"accuracy\": 0.75, \"loss\": 0.2483648657798767, \"time-step\": 562}, {\"accuracy\": 0.75, \"loss\": 0.24835094809532166, \"time-step\": 563}, {\"accuracy\": 0.75, \"loss\": 0.2483370304107666, \"time-step\": 564}, {\"accuracy\": 0.75, \"loss\": 0.24832311272621155, \"time-step\": 565}, {\"accuracy\": 0.75, \"loss\": 0.24830912053585052, \"time-step\": 566}, {\"accuracy\": 0.75, \"loss\": 0.24829509854316711, \"time-step\": 567}, {\"accuracy\": 0.75, \"loss\": 0.2482810765504837, \"time-step\": 568}, {\"accuracy\": 0.75, \"loss\": 0.24826699495315552, \"time-step\": 569}, {\"accuracy\": 0.75, \"loss\": 0.24825289845466614, \"time-step\": 570}, {\"accuracy\": 0.75, \"loss\": 0.24823877215385437, \"time-step\": 571}, {\"accuracy\": 0.75, \"loss\": 0.24822461605072021, \"time-step\": 572}, {\"accuracy\": 0.75, \"loss\": 0.2482103854417801, \"time-step\": 573}, {\"accuracy\": 0.75, \"loss\": 0.24819612503051758, \"time-step\": 574}, {\"accuracy\": 0.75, \"loss\": 0.24818181991577148, \"time-step\": 575}, {\"accuracy\": 0.75, \"loss\": 0.248167484998703, \"time-step\": 576}, {\"accuracy\": 0.75, \"loss\": 0.24815309047698975, \"time-step\": 577}, {\"accuracy\": 0.75, \"loss\": 0.2481386810541153, \"time-step\": 578}, {\"accuracy\": 0.75, \"loss\": 0.24812419712543488, \"time-step\": 579}, {\"accuracy\": 0.75, \"loss\": 0.24810966849327087, \"time-step\": 580}, {\"accuracy\": 0.75, \"loss\": 0.2480950802564621, \"time-step\": 581}, {\"accuracy\": 0.75, \"loss\": 0.24808044731616974, \"time-step\": 582}, {\"accuracy\": 0.75, \"loss\": 0.2480657547712326, \"time-step\": 583}, {\"accuracy\": 0.75, \"loss\": 0.24805095791816711, \"time-step\": 584}, {\"accuracy\": 0.75, \"loss\": 0.24803617596626282, \"time-step\": 585}, {\"accuracy\": 0.75, \"loss\": 0.24802130460739136, \"time-step\": 586}, {\"accuracy\": 0.75, \"loss\": 0.24800634384155273, \"time-step\": 587}, {\"accuracy\": 0.75, \"loss\": 0.24799129366874695, \"time-step\": 588}, {\"accuracy\": 0.75, \"loss\": 0.24797624349594116, \"time-step\": 589}, {\"accuracy\": 0.75, \"loss\": 0.24796102941036224, \"time-step\": 590}, {\"accuracy\": 0.75, \"loss\": 0.24794578552246094, \"time-step\": 591}, {\"accuracy\": 0.75, \"loss\": 0.24793049693107605, \"time-step\": 592}, {\"accuracy\": 0.75, \"loss\": 0.24791507422924042, \"time-step\": 593}, {\"accuracy\": 0.75, \"loss\": 0.2478996366262436, \"time-step\": 594}, {\"accuracy\": 0.75, \"loss\": 0.24788400530815125, \"time-step\": 595}, {\"accuracy\": 0.75, \"loss\": 0.24786829948425293, \"time-step\": 596}, {\"accuracy\": 0.75, \"loss\": 0.24785259366035461, \"time-step\": 597}, {\"accuracy\": 0.75, \"loss\": 0.24783667922019958, \"time-step\": 598}, {\"accuracy\": 0.75, \"loss\": 0.24782072007656097, \"time-step\": 599}, {\"accuracy\": 0.75, \"loss\": 0.24780462682247162, \"time-step\": 600}, {\"accuracy\": 0.75, \"loss\": 0.2477884292602539, \"time-step\": 601}, {\"accuracy\": 0.75, \"loss\": 0.2477721869945526, \"time-step\": 602}, {\"accuracy\": 0.75, \"loss\": 0.2477557510137558, \"time-step\": 603}, {\"accuracy\": 0.75, \"loss\": 0.2477392554283142, \"time-step\": 604}, {\"accuracy\": 0.75, \"loss\": 0.2477225810289383, \"time-step\": 605}, {\"accuracy\": 0.75, \"loss\": 0.24770580232143402, \"time-step\": 606}, {\"accuracy\": 0.75, \"loss\": 0.247688889503479, \"time-step\": 607}, {\"accuracy\": 0.75, \"loss\": 0.24767184257507324, \"time-step\": 608}, {\"accuracy\": 0.75, \"loss\": 0.24765467643737793, \"time-step\": 609}, {\"accuracy\": 0.75, \"loss\": 0.24763740599155426, \"time-step\": 610}, {\"accuracy\": 0.75, \"loss\": 0.2476198971271515, \"time-step\": 611}, {\"accuracy\": 0.75, \"loss\": 0.24760234355926514, \"time-step\": 612}, {\"accuracy\": 0.75, \"loss\": 0.24758456647396088, \"time-step\": 613}, {\"accuracy\": 0.75, \"loss\": 0.24756665527820587, \"time-step\": 614}, {\"accuracy\": 0.75, \"loss\": 0.24754858016967773, \"time-step\": 615}, {\"accuracy\": 0.75, \"loss\": 0.24753035604953766, \"time-step\": 616}, {\"accuracy\": 0.75, \"loss\": 0.24751195311546326, \"time-step\": 617}, {\"accuracy\": 0.75, \"loss\": 0.24749340116977692, \"time-step\": 618}, {\"accuracy\": 0.75, \"loss\": 0.24747467041015625, \"time-step\": 619}, {\"accuracy\": 0.75, \"loss\": 0.24745570123195648, \"time-step\": 620}, {\"accuracy\": 0.75, \"loss\": 0.2474365532398224, \"time-step\": 621}, {\"accuracy\": 0.75, \"loss\": 0.24741728603839874, \"time-step\": 622}, {\"accuracy\": 0.75, \"loss\": 0.247397780418396, \"time-step\": 623}, {\"accuracy\": 0.75, \"loss\": 0.24737811088562012, \"time-step\": 624}, {\"accuracy\": 0.75, \"loss\": 0.24735818803310394, \"time-step\": 625}, {\"accuracy\": 0.75, \"loss\": 0.24733810126781464, \"time-step\": 626}, {\"accuracy\": 0.75, \"loss\": 0.24731776118278503, \"time-step\": 627}, {\"accuracy\": 0.75, \"loss\": 0.2472972273826599, \"time-step\": 628}, {\"accuracy\": 0.75, \"loss\": 0.24727647006511688, \"time-step\": 629}, {\"accuracy\": 0.75, \"loss\": 0.24725544452667236, \"time-step\": 630}, {\"accuracy\": 0.75, \"loss\": 0.2472342699766159, \"time-step\": 631}, {\"accuracy\": 0.75, \"loss\": 0.24721281230449677, \"time-step\": 632}, {\"accuracy\": 0.75, \"loss\": 0.24719110131263733, \"time-step\": 633}, {\"accuracy\": 0.75, \"loss\": 0.24716916680335999, \"time-step\": 634}, {\"accuracy\": 0.75, \"loss\": 0.24714696407318115, \"time-step\": 635}, {\"accuracy\": 0.75, \"loss\": 0.24712452292442322, \"time-step\": 636}, {\"accuracy\": 0.75, \"loss\": 0.24710184335708618, \"time-step\": 637}, {\"accuracy\": 0.75, \"loss\": 0.24707885086536407, \"time-step\": 638}, {\"accuracy\": 0.75, \"loss\": 0.24705559015274048, \"time-step\": 639}, {\"accuracy\": 0.75, \"loss\": 0.24703212082386017, \"time-step\": 640}, {\"accuracy\": 0.75, \"loss\": 0.2470083236694336, \"time-step\": 641}, {\"accuracy\": 0.75, \"loss\": 0.24698424339294434, \"time-step\": 642}, {\"accuracy\": 0.75, \"loss\": 0.24695982038974762, \"time-step\": 643}, {\"accuracy\": 0.75, \"loss\": 0.24693523347377777, \"time-step\": 644}, {\"accuracy\": 0.75, \"loss\": 0.24691027402877808, \"time-step\": 645}, {\"accuracy\": 0.75, \"loss\": 0.24688498675823212, \"time-step\": 646}, {\"accuracy\": 0.75, \"loss\": 0.24685943126678467, \"time-step\": 647}, {\"accuracy\": 0.75, \"loss\": 0.24683354794979095, \"time-step\": 648}, {\"accuracy\": 0.75, \"loss\": 0.2468073070049286, \"time-step\": 649}, {\"accuracy\": 0.75, \"loss\": 0.24678079783916473, \"time-step\": 650}, {\"accuracy\": 0.75, \"loss\": 0.24675396084785461, \"time-step\": 651}, {\"accuracy\": 0.75, \"loss\": 0.24672678112983704, \"time-step\": 652}, {\"accuracy\": 0.75, \"loss\": 0.2466992288827896, \"time-step\": 653}, {\"accuracy\": 0.75, \"loss\": 0.24667134881019592, \"time-step\": 654}, {\"accuracy\": 0.75, \"loss\": 0.24664315581321716, \"time-step\": 655}, {\"accuracy\": 0.75, \"loss\": 0.24661460518836975, \"time-step\": 656}, {\"accuracy\": 0.75, \"loss\": 0.2465856373310089, \"time-step\": 657}, {\"accuracy\": 0.75, \"loss\": 0.2465563416481018, \"time-step\": 658}, {\"accuracy\": 0.75, \"loss\": 0.24652668833732605, \"time-step\": 659}, {\"accuracy\": 0.75, \"loss\": 0.24649666249752045, \"time-step\": 660}, {\"accuracy\": 0.75, \"loss\": 0.24646621942520142, \"time-step\": 661}, {\"accuracy\": 0.75, \"loss\": 0.24643543362617493, \"time-step\": 662}, {\"accuracy\": 0.75, \"loss\": 0.246404230594635, \"time-step\": 663}, {\"accuracy\": 0.75, \"loss\": 0.24637261033058167, \"time-step\": 664}, {\"accuracy\": 0.75, \"loss\": 0.24634063243865967, \"time-step\": 665}, {\"accuracy\": 0.75, \"loss\": 0.24630822241306305, \"time-step\": 666}, {\"accuracy\": 0.75, \"loss\": 0.2462754100561142, \"time-step\": 667}, {\"accuracy\": 0.75, \"loss\": 0.24624216556549072, \"time-step\": 668}, {\"accuracy\": 0.75, \"loss\": 0.24620850384235382, \"time-step\": 669}, {\"accuracy\": 0.75, \"loss\": 0.2461744099855423, \"time-step\": 670}, {\"accuracy\": 0.75, \"loss\": 0.24613985419273376, \"time-step\": 671}, {\"accuracy\": 0.75, \"loss\": 0.2461048662662506, \"time-step\": 672}, {\"accuracy\": 0.75, \"loss\": 0.24606946110725403, \"time-step\": 673}, {\"accuracy\": 0.75, \"loss\": 0.24603357911109924, \"time-step\": 674}, {\"accuracy\": 0.75, \"loss\": 0.24599725008010864, \"time-step\": 675}, {\"accuracy\": 0.75, \"loss\": 0.24596042931079865, \"time-step\": 676}, {\"accuracy\": 0.75, \"loss\": 0.24592314660549164, \"time-step\": 677}, {\"accuracy\": 0.75, \"loss\": 0.24588537216186523, \"time-step\": 678}, {\"accuracy\": 0.75, \"loss\": 0.2458471655845642, \"time-step\": 679}, {\"accuracy\": 0.75, \"loss\": 0.2458084225654602, \"time-step\": 680}, {\"accuracy\": 0.75, \"loss\": 0.2457691878080368, \"time-step\": 681}, {\"accuracy\": 0.75, \"loss\": 0.245729461312294, \"time-step\": 682}, {\"accuracy\": 0.75, \"loss\": 0.24568922817707062, \"time-step\": 683}, {\"accuracy\": 0.75, \"loss\": 0.24564844369888306, \"time-step\": 684}, {\"accuracy\": 0.75, \"loss\": 0.2456071376800537, \"time-step\": 685}, {\"accuracy\": 0.75, \"loss\": 0.24556533992290497, \"time-step\": 686}, {\"accuracy\": 0.75, \"loss\": 0.24552297592163086, \"time-step\": 687}, {\"accuracy\": 0.75, \"loss\": 0.24548007547855377, \"time-step\": 688}, {\"accuracy\": 0.75, \"loss\": 0.2454366683959961, \"time-step\": 689}, {\"accuracy\": 0.75, \"loss\": 0.24539268016815186, \"time-step\": 690}, {\"accuracy\": 0.75, \"loss\": 0.24534806609153748, \"time-step\": 691}, {\"accuracy\": 0.75, \"loss\": 0.2453029751777649, \"time-step\": 692}, {\"accuracy\": 0.75, \"loss\": 0.2452571988105774, \"time-step\": 693}, {\"accuracy\": 0.75, \"loss\": 0.2452109307050705, \"time-step\": 694}, {\"accuracy\": 0.75, \"loss\": 0.24516408145427704, \"time-step\": 695}, {\"accuracy\": 0.75, \"loss\": 0.24511662125587463, \"time-step\": 696}, {\"accuracy\": 0.75, \"loss\": 0.24506855010986328, \"time-step\": 697}, {\"accuracy\": 0.75, \"loss\": 0.2450198531150818, \"time-step\": 698}, {\"accuracy\": 0.75, \"loss\": 0.24497054517269135, \"time-step\": 699}, {\"accuracy\": 0.75, \"loss\": 0.24492061138153076, \"time-step\": 700}, {\"accuracy\": 0.75, \"loss\": 0.24487000703811646, \"time-step\": 701}, {\"accuracy\": 0.75, \"loss\": 0.24481883645057678, \"time-step\": 702}, {\"accuracy\": 0.75, \"loss\": 0.2447669804096222, \"time-step\": 703}, {\"accuracy\": 0.75, \"loss\": 0.2447144091129303, \"time-step\": 704}, {\"accuracy\": 0.75, \"loss\": 0.24466127157211304, \"time-step\": 705}, {\"accuracy\": 0.75, \"loss\": 0.24460746347904205, \"time-step\": 706}, {\"accuracy\": 0.75, \"loss\": 0.24455291032791138, \"time-step\": 707}, {\"accuracy\": 0.75, \"loss\": 0.24449771642684937, \"time-step\": 708}, {\"accuracy\": 0.75, \"loss\": 0.24444180727005005, \"time-step\": 709}, {\"accuracy\": 0.75, \"loss\": 0.24438521265983582, \"time-step\": 710}, {\"accuracy\": 0.75, \"loss\": 0.24432790279388428, \"time-step\": 711}, {\"accuracy\": 0.75, \"loss\": 0.24426990747451782, \"time-step\": 712}, {\"accuracy\": 0.75, \"loss\": 0.24421116709709167, \"time-step\": 713}, {\"accuracy\": 0.75, \"loss\": 0.24415165185928345, \"time-step\": 714}, {\"accuracy\": 0.75, \"loss\": 0.2440914511680603, \"time-step\": 715}, {\"accuracy\": 0.75, \"loss\": 0.2440304458141327, \"time-step\": 716}, {\"accuracy\": 0.75, \"loss\": 0.24396872520446777, \"time-step\": 717}, {\"accuracy\": 0.75, \"loss\": 0.24390622973442078, \"time-step\": 718}, {\"accuracy\": 0.75, \"loss\": 0.2438429743051529, \"time-step\": 719}, {\"accuracy\": 0.75, \"loss\": 0.24377889931201935, \"time-step\": 720}, {\"accuracy\": 0.75, \"loss\": 0.2437140941619873, \"time-step\": 721}, {\"accuracy\": 0.75, \"loss\": 0.2436484396457672, \"time-step\": 722}, {\"accuracy\": 0.75, \"loss\": 0.24358198046684265, \"time-step\": 723}, {\"accuracy\": 0.75, \"loss\": 0.24351468682289124, \"time-step\": 724}, {\"accuracy\": 0.75, \"loss\": 0.24344658851623535, \"time-step\": 725}, {\"accuracy\": 0.75, \"loss\": 0.24337764084339142, \"time-step\": 726}, {\"accuracy\": 0.75, \"loss\": 0.24330788850784302, \"time-step\": 727}, {\"accuracy\": 0.75, \"loss\": 0.2432372123003006, \"time-step\": 728}, {\"accuracy\": 0.75, \"loss\": 0.2431657463312149, \"time-step\": 729}, {\"accuracy\": 0.75, \"loss\": 0.243093341588974, \"time-step\": 730}, {\"accuracy\": 0.75, \"loss\": 0.24302008748054504, \"time-step\": 731}, {\"accuracy\": 0.75, \"loss\": 0.24294589459896088, \"time-step\": 732}, {\"accuracy\": 0.75, \"loss\": 0.24287086725234985, \"time-step\": 733}, {\"accuracy\": 0.75, \"loss\": 0.24279487133026123, \"time-step\": 734}, {\"accuracy\": 0.75, \"loss\": 0.24271798133850098, \"time-step\": 735}, {\"accuracy\": 0.75, \"loss\": 0.2426401525735855, \"time-step\": 736}, {\"accuracy\": 0.75, \"loss\": 0.24256131052970886, \"time-step\": 737}, {\"accuracy\": 0.75, \"loss\": 0.24248160421848297, \"time-step\": 738}, {\"accuracy\": 0.75, \"loss\": 0.24240082502365112, \"time-step\": 739}, {\"accuracy\": 0.75, \"loss\": 0.24231918156147003, \"time-step\": 740}, {\"accuracy\": 0.75, \"loss\": 0.24223646521568298, \"time-step\": 741}, {\"accuracy\": 0.75, \"loss\": 0.24215275049209595, \"time-step\": 742}, {\"accuracy\": 0.75, \"loss\": 0.24206799268722534, \"time-step\": 743}, {\"accuracy\": 0.75, \"loss\": 0.24198225140571594, \"time-step\": 744}, {\"accuracy\": 0.75, \"loss\": 0.24189543724060059, \"time-step\": 745}, {\"accuracy\": 0.75, \"loss\": 0.24180755019187927, \"time-step\": 746}, {\"accuracy\": 0.75, \"loss\": 0.2417186200618744, \"time-step\": 747}, {\"accuracy\": 0.75, \"loss\": 0.24162855744361877, \"time-step\": 748}, {\"accuracy\": 0.75, \"loss\": 0.2415374219417572, \"time-step\": 749}, {\"accuracy\": 0.75, \"loss\": 0.2414451390504837, \"time-step\": 750}, {\"accuracy\": 0.75, \"loss\": 0.2413516789674759, \"time-step\": 751}, {\"accuracy\": 0.75, \"loss\": 0.24125711619853973, \"time-step\": 752}, {\"accuracy\": 0.75, \"loss\": 0.24116136133670807, \"time-step\": 753}, {\"accuracy\": 0.75, \"loss\": 0.2410643994808197, \"time-step\": 754}, {\"accuracy\": 0.75, \"loss\": 0.24096623063087463, \"time-step\": 755}, {\"accuracy\": 0.75, \"loss\": 0.24086683988571167, \"time-step\": 756}, {\"accuracy\": 0.75, \"loss\": 0.24076618254184723, \"time-step\": 757}, {\"accuracy\": 0.75, \"loss\": 0.24066424369812012, \"time-step\": 758}, {\"accuracy\": 0.75, \"loss\": 0.24056096374988556, \"time-step\": 759}, {\"accuracy\": 0.75, \"loss\": 0.24045640230178833, \"time-step\": 760}, {\"accuracy\": 0.75, \"loss\": 0.24035045504570007, \"time-step\": 761}, {\"accuracy\": 0.75, \"loss\": 0.24024319648742676, \"time-step\": 762}, {\"accuracy\": 0.75, \"loss\": 0.24013447761535645, \"time-step\": 763}, {\"accuracy\": 0.75, \"loss\": 0.24002432823181152, \"time-step\": 764}, {\"accuracy\": 0.75, \"loss\": 0.23991268873214722, \"time-step\": 765}, {\"accuracy\": 0.75, \"loss\": 0.23979957401752472, \"time-step\": 766}, {\"accuracy\": 0.75, \"loss\": 0.23968487977981567, \"time-step\": 767}, {\"accuracy\": 0.75, \"loss\": 0.23956863582134247, \"time-step\": 768}, {\"accuracy\": 0.75, \"loss\": 0.23945075273513794, \"time-step\": 769}, {\"accuracy\": 0.75, \"loss\": 0.23933126032352448, \"time-step\": 770}, {\"accuracy\": 0.75, \"loss\": 0.23921000957489014, \"time-step\": 771}, {\"accuracy\": 0.75, \"loss\": 0.2390870302915573, \"time-step\": 772}, {\"accuracy\": 0.75, \"loss\": 0.2389622926712036, \"time-step\": 773}, {\"accuracy\": 0.75, \"loss\": 0.2388356477022171, \"time-step\": 774}, {\"accuracy\": 0.75, \"loss\": 0.23870709538459778, \"time-step\": 775}, {\"accuracy\": 0.75, \"loss\": 0.23857663571834564, \"time-step\": 776}, {\"accuracy\": 0.75, \"loss\": 0.23844406008720398, \"time-step\": 777}, {\"accuracy\": 0.75, \"loss\": 0.23830941319465637, \"time-step\": 778}, {\"accuracy\": 0.75, \"loss\": 0.23817265033721924, \"time-step\": 779}, {\"accuracy\": 0.75, \"loss\": 0.23803357779979706, \"time-step\": 780}, {\"accuracy\": 0.75, \"loss\": 0.23789218068122864, \"time-step\": 781}, {\"accuracy\": 0.75, \"loss\": 0.23774844408035278, \"time-step\": 782}, {\"accuracy\": 0.75, \"loss\": 0.237602099776268, \"time-step\": 783}, {\"accuracy\": 0.75, \"loss\": 0.23745325207710266, \"time-step\": 784}, {\"accuracy\": 0.75, \"loss\": 0.23730166256427765, \"time-step\": 785}, {\"accuracy\": 0.75, \"loss\": 0.237147256731987, \"time-step\": 786}, {\"accuracy\": 0.75, \"loss\": 0.23698991537094116, \"time-step\": 787}, {\"accuracy\": 0.75, \"loss\": 0.23682954907417297, \"time-step\": 788}, {\"accuracy\": 0.75, \"loss\": 0.2366659939289093, \"time-step\": 789}, {\"accuracy\": 0.75, \"loss\": 0.2364991009235382, \"time-step\": 790}, {\"accuracy\": 0.75, \"loss\": 0.23632881045341492, \"time-step\": 791}, {\"accuracy\": 0.75, \"loss\": 0.23615488409996033, \"time-step\": 792}, {\"accuracy\": 0.75, \"loss\": 0.2359772026538849, \"time-step\": 793}, {\"accuracy\": 0.75, \"loss\": 0.23579555749893188, \"time-step\": 794}, {\"accuracy\": 0.75, \"loss\": 0.23560982942581177, \"time-step\": 795}, {\"accuracy\": 0.75, \"loss\": 0.23541975021362305, \"time-step\": 796}, {\"accuracy\": 0.75, \"loss\": 0.23522521555423737, \"time-step\": 797}, {\"accuracy\": 0.75, \"loss\": 0.23502600193023682, \"time-step\": 798}, {\"accuracy\": 0.75, \"loss\": 0.23482179641723633, \"time-step\": 799}, {\"accuracy\": 0.75, \"loss\": 0.23461253941059113, \"time-step\": 800}, {\"accuracy\": 0.75, \"loss\": 0.23439787328243256, \"time-step\": 801}, {\"accuracy\": 0.75, \"loss\": 0.23417752981185913, \"time-step\": 802}, {\"accuracy\": 0.75, \"loss\": 0.2339514046907425, \"time-step\": 803}, {\"accuracy\": 0.75, \"loss\": 0.23371915519237518, \"time-step\": 804}, {\"accuracy\": 0.75, \"loss\": 0.23348048329353333, \"time-step\": 805}, {\"accuracy\": 0.75, \"loss\": 0.23323510587215424, \"time-step\": 806}, {\"accuracy\": 0.75, \"loss\": 0.23298276960849762, \"time-step\": 807}, {\"accuracy\": 0.75, \"loss\": 0.23272311687469482, \"time-step\": 808}, {\"accuracy\": 0.75, \"loss\": 0.23245592415332794, \"time-step\": 809}, {\"accuracy\": 0.75, \"loss\": 0.23218081891536713, \"time-step\": 810}, {\"accuracy\": 0.75, \"loss\": 0.2318975031375885, \"time-step\": 811}, {\"accuracy\": 0.75, \"loss\": 0.2316056489944458, \"time-step\": 812}, {\"accuracy\": 0.75, \"loss\": 0.23130486905574799, \"time-step\": 813}, {\"accuracy\": 0.75, \"loss\": 0.23099485039710999, \"time-step\": 814}, {\"accuracy\": 0.75, \"loss\": 0.2306753247976303, \"time-step\": 815}, {\"accuracy\": 0.75, \"loss\": 0.23034575581550598, \"time-step\": 816}, {\"accuracy\": 0.75, \"loss\": 0.23000594973564148, \"time-step\": 817}, {\"accuracy\": 0.75, \"loss\": 0.22965547442436218, \"time-step\": 818}, {\"accuracy\": 0.75, \"loss\": 0.2292940318584442, \"time-step\": 819}, {\"accuracy\": 0.75, \"loss\": 0.2289211004972458, \"time-step\": 820}, {\"accuracy\": 0.75, \"loss\": 0.2285364717245102, \"time-step\": 821}, {\"accuracy\": 0.75, \"loss\": 0.22813966870307922, \"time-step\": 822}, {\"accuracy\": 0.75, \"loss\": 0.22773034870624542, \"time-step\": 823}, {\"accuracy\": 0.75, \"loss\": 0.22730816900730133, \"time-step\": 824}, {\"accuracy\": 0.75, \"loss\": 0.22687269747257233, \"time-step\": 825}, {\"accuracy\": 0.75, \"loss\": 0.22642365097999573, \"time-step\": 826}, {\"accuracy\": 0.75, \"loss\": 0.22596056759357452, \"time-step\": 827}, {\"accuracy\": 0.75, \"loss\": 0.22548316419124603, \"time-step\": 828}, {\"accuracy\": 0.75, \"loss\": 0.22499100863933563, \"time-step\": 829}, {\"accuracy\": 0.75, \"loss\": 0.22448375821113586, \"time-step\": 830}, {\"accuracy\": 0.75, \"loss\": 0.22396109998226166, \"time-step\": 831}, {\"accuracy\": 0.75, \"loss\": 0.22342267632484436, \"time-step\": 832}, {\"accuracy\": 0.75, \"loss\": 0.22286808490753174, \"time-step\": 833}, {\"accuracy\": 0.75, \"loss\": 0.2222970724105835, \"time-step\": 834}, {\"accuracy\": 0.75, \"loss\": 0.22170940041542053, \"time-step\": 835}, {\"accuracy\": 0.75, \"loss\": 0.22110462188720703, \"time-step\": 836}, {\"accuracy\": 0.75, \"loss\": 0.2204824686050415, \"time-step\": 837}, {\"accuracy\": 0.75, \"loss\": 0.2198428213596344, \"time-step\": 838}, {\"accuracy\": 0.75, \"loss\": 0.21918538212776184, \"time-step\": 839}, {\"accuracy\": 0.75, \"loss\": 0.21850985288619995, \"time-step\": 840}, {\"accuracy\": 0.75, \"loss\": 0.21781617403030396, \"time-step\": 841}, {\"accuracy\": 0.75, \"loss\": 0.21710409224033356, \"time-step\": 842}, {\"accuracy\": 0.75, \"loss\": 0.21637342870235443, \"time-step\": 843}, {\"accuracy\": 0.75, \"loss\": 0.21562430262565613, \"time-step\": 844}, {\"accuracy\": 0.75, \"loss\": 0.21485650539398193, \"time-step\": 845}, {\"accuracy\": 0.75, \"loss\": 0.21407000720500946, \"time-step\": 846}, {\"accuracy\": 0.75, \"loss\": 0.21326494216918945, \"time-step\": 847}, {\"accuracy\": 0.75, \"loss\": 0.21244125068187714, \"time-step\": 848}, {\"accuracy\": 0.75, \"loss\": 0.21159915626049042, \"time-step\": 849}, {\"accuracy\": 0.75, \"loss\": 0.21073879301548004, \"time-step\": 850}, {\"accuracy\": 0.75, \"loss\": 0.20986036956310272, \"time-step\": 851}, {\"accuracy\": 0.75, \"loss\": 0.20896410942077637, \"time-step\": 852}, {\"accuracy\": 0.5, \"loss\": 0.2080504149198532, \"time-step\": 853}, {\"accuracy\": 0.5, \"loss\": 0.20711955428123474, \"time-step\": 854}, {\"accuracy\": 0.5, \"loss\": 0.20617210865020752, \"time-step\": 855}, {\"accuracy\": 0.5, \"loss\": 0.20520833134651184, \"time-step\": 856}, {\"accuracy\": 0.5, \"loss\": 0.20422884821891785, \"time-step\": 857}, {\"accuracy\": 0.5, \"loss\": 0.20323427021503448, \"time-step\": 858}, {\"accuracy\": 0.5, \"loss\": 0.20222511887550354, \"time-step\": 859}, {\"accuracy\": 0.5, \"loss\": 0.20120209455490112, \"time-step\": 860}, {\"accuracy\": 0.5, \"loss\": 0.20016583800315857, \"time-step\": 861}, {\"accuracy\": 0.5, \"loss\": 0.19911719858646393, \"time-step\": 862}, {\"accuracy\": 0.5, \"loss\": 0.19805675745010376, \"time-step\": 863}, {\"accuracy\": 0.5, \"loss\": 0.19698548316955566, \"time-step\": 864}, {\"accuracy\": 0.5, \"loss\": 0.19590400159358978, \"time-step\": 865}, {\"accuracy\": 0.5, \"loss\": 0.19481319189071655, \"time-step\": 866}, {\"accuracy\": 0.5, \"loss\": 0.19371400773525238, \"time-step\": 867}, {\"accuracy\": 0.5, \"loss\": 0.19260713458061218, \"time-step\": 868}, {\"accuracy\": 0.5, \"loss\": 0.1914934515953064, \"time-step\": 869}, {\"accuracy\": 0.5, \"loss\": 0.19037394225597382, \"time-step\": 870}, {\"accuracy\": 0.5, \"loss\": 0.18924929201602936, \"time-step\": 871}, {\"accuracy\": 0.5, \"loss\": 0.18812039494514465, \"time-step\": 872}, {\"accuracy\": 0.5, \"loss\": 0.1869879961013794, \"time-step\": 873}, {\"accuracy\": 0.5, \"loss\": 0.18585297465324402, \"time-step\": 874}, {\"accuracy\": 0.5, \"loss\": 0.184716135263443, \"time-step\": 875}, {\"accuracy\": 0.5, \"loss\": 0.18357811868190765, \"time-step\": 876}, {\"accuracy\": 0.5, \"loss\": 0.18243971467018127, \"time-step\": 877}, {\"accuracy\": 0.5, \"loss\": 0.1813015639781952, \"time-step\": 878}, {\"accuracy\": 0.5, \"loss\": 0.18016430735588074, \"time-step\": 879}, {\"accuracy\": 0.5, \"loss\": 0.17902854084968567, \"time-step\": 880}, {\"accuracy\": 0.5, \"loss\": 0.17789486050605774, \"time-step\": 881}, {\"accuracy\": 0.5, \"loss\": 0.1767638921737671, \"time-step\": 882}, {\"accuracy\": 0.5, \"loss\": 0.1756359040737152, \"time-step\": 883}, {\"accuracy\": 0.5, \"loss\": 0.1745116114616394, \"time-step\": 884}, {\"accuracy\": 0.5, \"loss\": 0.173391193151474, \"time-step\": 885}, {\"accuracy\": 0.5, \"loss\": 0.17227506637573242, \"time-step\": 886}, {\"accuracy\": 0.5, \"loss\": 0.1711636334657669, \"time-step\": 887}, {\"accuracy\": 0.5, \"loss\": 0.17005710303783417, \"time-step\": 888}, {\"accuracy\": 0.5, \"loss\": 0.16895562410354614, \"time-step\": 889}, {\"accuracy\": 0.5, \"loss\": 0.16785959899425507, \"time-step\": 890}, {\"accuracy\": 0.5, \"loss\": 0.1667691171169281, \"time-step\": 891}, {\"accuracy\": 0.5, \"loss\": 0.16568425297737122, \"time-step\": 892}, {\"accuracy\": 0.5, \"loss\": 0.16460508108139038, \"time-step\": 893}, {\"accuracy\": 0.5, \"loss\": 0.16353166103363037, \"time-step\": 894}, {\"accuracy\": 0.5, \"loss\": 0.16246408224105835, \"time-step\": 895}, {\"accuracy\": 0.5, \"loss\": 0.16140228509902954, \"time-step\": 896}, {\"accuracy\": 0.5, \"loss\": 0.16034623980522156, \"time-step\": 897}, {\"accuracy\": 0.5, \"loss\": 0.15929582715034485, \"time-step\": 898}, {\"accuracy\": 0.5, \"loss\": 0.15825104713439941, \"time-step\": 899}, {\"accuracy\": 0.5, \"loss\": 0.15721166133880615, \"time-step\": 900}, {\"accuracy\": 0.5, \"loss\": 0.15617774426937103, \"time-step\": 901}, {\"accuracy\": 0.5, \"loss\": 0.1551489531993866, \"time-step\": 902}, {\"accuracy\": 0.5, \"loss\": 0.15412506461143494, \"time-step\": 903}, {\"accuracy\": 0.5, \"loss\": 0.15310604870319366, \"time-step\": 904}, {\"accuracy\": 0.5, \"loss\": 0.15209168195724487, \"time-step\": 905}, {\"accuracy\": 0.5, \"loss\": 0.1510816365480423, \"time-step\": 906}, {\"accuracy\": 0.5, \"loss\": 0.15007568895816803, \"time-step\": 907}, {\"accuracy\": 0.5, \"loss\": 0.14907363057136536, \"time-step\": 908}, {\"accuracy\": 0.5, \"loss\": 0.1480751931667328, \"time-step\": 909}, {\"accuracy\": 0.5, \"loss\": 0.1470799744129181, \"time-step\": 910}, {\"accuracy\": 0.5, \"loss\": 0.1460878700017929, \"time-step\": 911}, {\"accuracy\": 0.75, \"loss\": 0.145098477602005, \"time-step\": 912}, {\"accuracy\": 0.75, \"loss\": 0.14411142468452454, \"time-step\": 913}, {\"accuracy\": 0.75, \"loss\": 0.14312644302845, \"time-step\": 914}, {\"accuracy\": 0.75, \"loss\": 0.14214323461055756, \"time-step\": 915}, {\"accuracy\": 0.75, \"loss\": 0.14116138219833374, \"time-step\": 916}, {\"accuracy\": 0.75, \"loss\": 0.1401805430650711, \"time-step\": 917}, {\"accuracy\": 0.75, \"loss\": 0.13920041918754578, \"time-step\": 918}, {\"accuracy\": 0.75, \"loss\": 0.13822059333324432, \"time-step\": 919}, {\"accuracy\": 0.75, \"loss\": 0.1372406929731369, \"time-step\": 920}, {\"accuracy\": 0.75, \"loss\": 0.13626039028167725, \"time-step\": 921}, {\"accuracy\": 0.75, \"loss\": 0.13527914881706238, \"time-step\": 922}, {\"accuracy\": 0.75, \"loss\": 0.13429684937000275, \"time-step\": 923}, {\"accuracy\": 0.75, \"loss\": 0.1333128660917282, \"time-step\": 924}, {\"accuracy\": 0.75, \"loss\": 0.1323268711566925, \"time-step\": 925}, {\"accuracy\": 0.75, \"loss\": 0.131338432431221, \"time-step\": 926}, {\"accuracy\": 0.75, \"loss\": 0.13034725189208984, \"time-step\": 927}, {\"accuracy\": 0.75, \"loss\": 0.12935280799865723, \"time-step\": 928}, {\"accuracy\": 0.75, \"loss\": 0.1283547580242157, \"time-step\": 929}, {\"accuracy\": 0.75, \"loss\": 0.12735262513160706, \"time-step\": 930}, {\"accuracy\": 0.75, \"loss\": 0.12634606659412384, \"time-step\": 931}, {\"accuracy\": 0.75, \"loss\": 0.125334694981575, \"time-step\": 932}, {\"accuracy\": 0.75, \"loss\": 0.12431798875331879, \"time-step\": 933}, {\"accuracy\": 0.75, \"loss\": 0.1232956275343895, \"time-step\": 934}, {\"accuracy\": 0.75, \"loss\": 0.12226720154285431, \"time-step\": 935}, {\"accuracy\": 0.75, \"loss\": 0.12123221158981323, \"time-step\": 936}, {\"accuracy\": 1.0, \"loss\": 0.12019031494855881, \"time-step\": 937}, {\"accuracy\": 1.0, \"loss\": 0.1191411241889, \"time-step\": 938}, {\"accuracy\": 1.0, \"loss\": 0.11808420717716217, \"time-step\": 939}, {\"accuracy\": 1.0, \"loss\": 0.11701927334070206, \"time-step\": 940}, {\"accuracy\": 1.0, \"loss\": 0.11594580858945847, \"time-step\": 941}, {\"accuracy\": 1.0, \"loss\": 0.11486349999904633, \"time-step\": 942}, {\"accuracy\": 1.0, \"loss\": 0.11377200484275818, \"time-step\": 943}, {\"accuracy\": 1.0, \"loss\": 0.11267095804214478, \"time-step\": 944}, {\"accuracy\": 1.0, \"loss\": 0.11155995726585388, \"time-step\": 945}, {\"accuracy\": 1.0, \"loss\": 0.11043877899646759, \"time-step\": 946}, {\"accuracy\": 1.0, \"loss\": 0.10930699110031128, \"time-step\": 947}, {\"accuracy\": 1.0, \"loss\": 0.10816442966461182, \"time-step\": 948}, {\"accuracy\": 1.0, \"loss\": 0.1070108413696289, \"time-step\": 949}, {\"accuracy\": 1.0, \"loss\": 0.10584581643342972, \"time-step\": 950}, {\"accuracy\": 1.0, \"loss\": 0.10466928035020828, \"time-step\": 951}, {\"accuracy\": 1.0, \"loss\": 0.1034809798002243, \"time-step\": 952}, {\"accuracy\": 1.0, \"loss\": 0.10228095948696136, \"time-step\": 953}, {\"accuracy\": 1.0, \"loss\": 0.10106883198022842, \"time-step\": 954}, {\"accuracy\": 1.0, \"loss\": 0.09984508156776428, \"time-step\": 955}, {\"accuracy\": 1.0, \"loss\": 0.09860888123512268, \"time-step\": 956}, {\"accuracy\": 1.0, \"loss\": 0.09736178815364838, \"time-step\": 957}, {\"accuracy\": 1.0, \"loss\": 0.09610174596309662, \"time-step\": 958}, {\"accuracy\": 1.0, \"loss\": 0.09483177959918976, \"time-step\": 959}, {\"accuracy\": 1.0, \"loss\": 0.09354835748672485, \"time-step\": 960}, {\"accuracy\": 1.0, \"loss\": 0.09225624054670334, \"time-step\": 961}, {\"accuracy\": 1.0, \"loss\": 0.09095024317502975, \"time-step\": 962}, {\"accuracy\": 1.0, \"loss\": 0.08963748067617416, \"time-step\": 963}, {\"accuracy\": 1.0, \"loss\": 0.08831043541431427, \"time-step\": 964}, {\"accuracy\": 1.0, \"loss\": 0.08697907626628876, \"time-step\": 965}, {\"accuracy\": 1.0, \"loss\": 0.08563310652971268, \"time-step\": 966}, {\"accuracy\": 1.0, \"loss\": 0.08428596705198288, \"time-step\": 967}, {\"accuracy\": 1.0, \"loss\": 0.0829240083694458, \"time-step\": 968}, {\"accuracy\": 1.0, \"loss\": 0.08156482875347137, \"time-step\": 969}, {\"accuracy\": 1.0, \"loss\": 0.08019039034843445, \"time-step\": 970}, {\"accuracy\": 1.0, \"loss\": 0.07882388681173325, \"time-step\": 971}, {\"accuracy\": 1.0, \"loss\": 0.07744128257036209, \"time-step\": 972}, {\"accuracy\": 1.0, \"loss\": 0.07607310265302658, \"time-step\": 973}, {\"accuracy\": 1.0, \"loss\": 0.07468760013580322, \"time-step\": 974}, {\"accuracy\": 1.0, \"loss\": 0.07332432270050049, \"time-step\": 975}, {\"accuracy\": 1.0, \"loss\": 0.07194184511899948, \"time-step\": 976}, {\"accuracy\": 1.0, \"loss\": 0.07059114426374435, \"time-step\": 977}, {\"accuracy\": 1.0, \"loss\": 0.0692184567451477, \"time-step\": 978}, {\"accuracy\": 1.0, \"loss\": 0.06788836419582367, \"time-step\": 979}, {\"accuracy\": 1.0, \"loss\": 0.06653304398059845, \"time-step\": 980}, {\"accuracy\": 1.0, \"loss\": 0.06523188948631287, \"time-step\": 981}, {\"accuracy\": 1.0, \"loss\": 0.06390216946601868, \"time-step\": 982}, {\"accuracy\": 1.0, \"loss\": 0.06263817101716995, \"time-step\": 983}, {\"accuracy\": 1.0, \"loss\": 0.061343226581811905, \"time-step\": 984}, {\"accuracy\": 1.0, \"loss\": 0.060123734176158905, \"time-step\": 985}, {\"accuracy\": 1.0, \"loss\": 0.05887332558631897, \"time-step\": 986}, {\"accuracy\": 1.0, \"loss\": 0.05770482122898102, \"time-step\": 987}, {\"accuracy\": 1.0, \"loss\": 0.05650946497917175, \"time-step\": 988}, {\"accuracy\": 1.0, \"loss\": 0.05539662390947342, \"time-step\": 989}, {\"accuracy\": 1.0, \"loss\": 0.05426725745201111, \"time-step\": 990}, {\"accuracy\": 1.0, \"loss\": 0.053213030099868774, \"time-step\": 991}, {\"accuracy\": 1.0, \"loss\": 0.05216085910797119, \"time-step\": 992}, {\"accuracy\": 1.0, \"loss\": 0.051165621727705, \"time-step\": 993}, {\"accuracy\": 1.0, \"loss\": 0.05020182579755783, \"time-step\": 994}, {\"accuracy\": 1.0, \"loss\": 0.049263034015893936, \"time-step\": 995}, {\"accuracy\": 1.0, \"loss\": 0.048398569226264954, \"time-step\": 996}, {\"accuracy\": 1.0, \"loss\": 0.04751051962375641, \"time-step\": 997}, {\"accuracy\": 1.0, \"loss\": 0.04675581306219101, \"time-step\": 998}, {\"accuracy\": 1.0, \"loss\": 0.045909978449344635, \"time-step\": 999}, {\"accuracy\": 1.0, \"loss\": 0.04527479410171509, \"time-step\": 1000}, {\"accuracy\": 1.0, \"loss\": 0.044459693133831024, \"time-step\": 1001}, {\"accuracy\": 1.0, \"loss\": 0.04395335912704468, \"time-step\": 1002}, {\"accuracy\": 1.0, \"loss\": 0.04315529018640518, \"time-step\": 1003}, {\"accuracy\": 1.0, \"loss\": 0.04278681427240372, \"time-step\": 1004}, {\"accuracy\": 1.0, \"loss\": 0.041989970952272415, \"time-step\": 1005}, {\"accuracy\": 1.0, \"loss\": 0.04176823049783707, \"time-step\": 1006}, {\"accuracy\": 1.0, \"loss\": 0.04095545411109924, \"time-step\": 1007}, {\"accuracy\": 1.0, \"loss\": 0.04088909551501274, \"time-step\": 1008}, {\"accuracy\": 1.0, \"loss\": 0.04004218056797981, \"time-step\": 1009}, {\"accuracy\": 1.0, \"loss\": 0.040140021592378616, \"time-step\": 1010}, {\"accuracy\": 1.0, \"loss\": 0.0392397977411747, \"time-step\": 1011}, {\"accuracy\": 1.0, \"loss\": 0.039510201662778854, \"time-step\": 1012}, {\"accuracy\": 1.0, \"loss\": 0.03853726014494896, \"time-step\": 1013}, {\"accuracy\": 1.0, \"loss\": 0.038988277316093445, \"time-step\": 1014}, {\"accuracy\": 1.0, \"loss\": 0.037923093885183334, \"time-step\": 1015}, {\"accuracy\": 1.0, \"loss\": 0.03856203705072403, \"time-step\": 1016}, {\"accuracy\": 1.0, \"loss\": 0.03738584369421005, \"time-step\": 1017}, {\"accuracy\": 1.0, \"loss\": 0.03821903094649315, \"time-step\": 1018}, {\"accuracy\": 1.0, \"loss\": 0.03691423684358597, \"time-step\": 1019}, {\"accuracy\": 1.0, \"loss\": 0.03794679045677185, \"time-step\": 1020}, {\"accuracy\": 1.0, \"loss\": 0.036497704684734344, \"time-step\": 1021}, {\"accuracy\": 1.0, \"loss\": 0.03773330897092819, \"time-step\": 1022}, {\"accuracy\": 1.0, \"loss\": 0.03612663969397545, \"time-step\": 1023}, {\"accuracy\": 1.0, \"loss\": 0.03756755590438843, \"time-step\": 1024}, {\"accuracy\": 1.0, \"loss\": 0.035792432725429535, \"time-step\": 1025}, {\"accuracy\": 1.0, \"loss\": 0.03743947669863701, \"time-step\": 1026}, {\"accuracy\": 1.0, \"loss\": 0.03548794984817505, \"time-step\": 1027}, {\"accuracy\": 1.0, \"loss\": 0.03734058886766434, \"time-step\": 1028}, {\"accuracy\": 1.0, \"loss\": 0.03520739823579788, \"time-step\": 1029}, {\"accuracy\": 1.0, \"loss\": 0.037263937294483185, \"time-step\": 1030}, {\"accuracy\": 1.0, \"loss\": 0.03494612127542496, \"time-step\": 1031}, {\"accuracy\": 1.0, \"loss\": 0.037203676998615265, \"time-step\": 1032}, {\"accuracy\": 1.0, \"loss\": 0.034700553864240646, \"time-step\": 1033}, {\"accuracy\": 1.0, \"loss\": 0.03715544566512108, \"time-step\": 1034}, {\"accuracy\": 1.0, \"loss\": 0.0344681553542614, \"time-step\": 1035}, {\"accuracy\": 1.0, \"loss\": 0.03711593151092529, \"time-step\": 1036}, {\"accuracy\": 1.0, \"loss\": 0.03424680978059769, \"time-step\": 1037}, {\"accuracy\": 1.0, \"loss\": 0.03708237037062645, \"time-step\": 1038}, {\"accuracy\": 1.0, \"loss\": 0.03403501585125923, \"time-step\": 1039}, {\"accuracy\": 1.0, \"loss\": 0.037052784115076065, \"time-step\": 1040}, {\"accuracy\": 1.0, \"loss\": 0.033831723034381866, \"time-step\": 1041}, {\"accuracy\": 1.0, \"loss\": 0.037025921046733856, \"time-step\": 1042}, {\"accuracy\": 1.0, \"loss\": 0.03363615646958351, \"time-step\": 1043}, {\"accuracy\": 1.0, \"loss\": 0.03700060024857521, \"time-step\": 1044}, {\"accuracy\": 1.0, \"loss\": 0.03344758227467537, \"time-step\": 1045}, {\"accuracy\": 1.0, \"loss\": 0.03697604686021805, \"time-step\": 1046}, {\"accuracy\": 1.0, \"loss\": 0.03326553851366043, \"time-step\": 1047}, {\"accuracy\": 1.0, \"loss\": 0.03695162013173103, \"time-step\": 1048}, {\"accuracy\": 1.0, \"loss\": 0.03308945521712303, \"time-step\": 1049}, {\"accuracy\": 1.0, \"loss\": 0.03692666441202164, \"time-step\": 1050}, {\"accuracy\": 1.0, \"loss\": 0.03291904181241989, \"time-step\": 1051}, {\"accuracy\": 1.0, \"loss\": 0.03690089285373688, \"time-step\": 1052}, {\"accuracy\": 1.0, \"loss\": 0.03275385871529579, \"time-step\": 1053}, {\"accuracy\": 1.0, \"loss\": 0.03687392920255661, \"time-step\": 1054}, {\"accuracy\": 1.0, \"loss\": 0.032593630254268646, \"time-step\": 1055}, {\"accuracy\": 1.0, \"loss\": 0.03684549778699875, \"time-step\": 1056}, {\"accuracy\": 1.0, \"loss\": 0.03243802860379219, \"time-step\": 1057}, {\"accuracy\": 1.0, \"loss\": 0.03681543469429016, \"time-step\": 1058}, {\"accuracy\": 1.0, \"loss\": 0.03228673338890076, \"time-step\": 1059}, {\"accuracy\": 1.0, \"loss\": 0.036783378571271896, \"time-step\": 1060}, {\"accuracy\": 1.0, \"loss\": 0.03213958442211151, \"time-step\": 1061}, {\"accuracy\": 1.0, \"loss\": 0.03674937039613724, \"time-step\": 1062}, {\"accuracy\": 1.0, \"loss\": 0.031996242702007294, \"time-step\": 1063}, {\"accuracy\": 1.0, \"loss\": 0.036713212728500366, \"time-step\": 1064}, {\"accuracy\": 1.0, \"loss\": 0.03185631334781647, \"time-step\": 1065}, {\"accuracy\": 1.0, \"loss\": 0.03667450323700905, \"time-step\": 1066}, {\"accuracy\": 1.0, \"loss\": 0.031719692051410675, \"time-step\": 1067}, {\"accuracy\": 1.0, \"loss\": 0.03663345053792, \"time-step\": 1068}, {\"accuracy\": 1.0, \"loss\": 0.03158624470233917, \"time-step\": 1069}, {\"accuracy\": 1.0, \"loss\": 0.03658987209200859, \"time-step\": 1070}, {\"accuracy\": 1.0, \"loss\": 0.031455401331186295, \"time-step\": 1071}, {\"accuracy\": 1.0, \"loss\": 0.03654346615076065, \"time-step\": 1072}, {\"accuracy\": 1.0, \"loss\": 0.03132728487253189, \"time-step\": 1073}, {\"accuracy\": 1.0, \"loss\": 0.03649444133043289, \"time-step\": 1074}, {\"accuracy\": 1.0, \"loss\": 0.03120161034166813, \"time-step\": 1075}, {\"accuracy\": 1.0, \"loss\": 0.03644275292754173, \"time-step\": 1076}, {\"accuracy\": 1.0, \"loss\": 0.031078152358531952, \"time-step\": 1077}, {\"accuracy\": 1.0, \"loss\": 0.03638822212815285, \"time-step\": 1078}, {\"accuracy\": 1.0, \"loss\": 0.030956782400608063, \"time-step\": 1079}, {\"accuracy\": 1.0, \"loss\": 0.03633086010813713, \"time-step\": 1080}, {\"accuracy\": 1.0, \"loss\": 0.030837232246994972, \"time-step\": 1081}, {\"accuracy\": 1.0, \"loss\": 0.03627074509859085, \"time-step\": 1082}, {\"accuracy\": 1.0, \"loss\": 0.030719567090272903, \"time-step\": 1083}, {\"accuracy\": 1.0, \"loss\": 0.036207862198352814, \"time-step\": 1084}, {\"accuracy\": 1.0, \"loss\": 0.03060349076986313, \"time-step\": 1085}, {\"accuracy\": 1.0, \"loss\": 0.036142174154520035, \"time-step\": 1086}, {\"accuracy\": 1.0, \"loss\": 0.030488822609186172, \"time-step\": 1087}, {\"accuracy\": 1.0, \"loss\": 0.036073651164770126, \"time-step\": 1088}, {\"accuracy\": 1.0, \"loss\": 0.03037554956972599, \"time-step\": 1089}, {\"accuracy\": 1.0, \"loss\": 0.03600246086716652, \"time-step\": 1090}, {\"accuracy\": 1.0, \"loss\": 0.030263591557741165, \"time-step\": 1091}, {\"accuracy\": 1.0, \"loss\": 0.03592861443758011, \"time-step\": 1092}, {\"accuracy\": 1.0, \"loss\": 0.0301528237760067, \"time-step\": 1093}, {\"accuracy\": 1.0, \"loss\": 0.035852156579494476, \"time-step\": 1094}, {\"accuracy\": 1.0, \"loss\": 0.030043065547943115, \"time-step\": 1095}, {\"accuracy\": 1.0, \"loss\": 0.03577306494116783, \"time-step\": 1096}, {\"accuracy\": 1.0, \"loss\": 0.029934337362647057, \"time-step\": 1097}, {\"accuracy\": 1.0, \"loss\": 0.03569144010543823, \"time-step\": 1098}, {\"accuracy\": 1.0, \"loss\": 0.02982647903263569, \"time-step\": 1099}, {\"accuracy\": 1.0, \"loss\": 0.035607367753982544, \"time-step\": 1100}, {\"accuracy\": 1.0, \"loss\": 0.029719457030296326, \"time-step\": 1101}, {\"accuracy\": 1.0, \"loss\": 0.035520877689123154, \"time-step\": 1102}, {\"accuracy\": 1.0, \"loss\": 0.029613249003887177, \"time-step\": 1103}, {\"accuracy\": 1.0, \"loss\": 0.035432081669569016, \"time-step\": 1104}, {\"accuracy\": 1.0, \"loss\": 0.029507696628570557, \"time-step\": 1105}, {\"accuracy\": 1.0, \"loss\": 0.035340916365385056, \"time-step\": 1106}, {\"accuracy\": 1.0, \"loss\": 0.029402881860733032, \"time-step\": 1107}, {\"accuracy\": 1.0, \"loss\": 0.03524773567914963, \"time-step\": 1108}, {\"accuracy\": 1.0, \"loss\": 0.029298575595021248, \"time-step\": 1109}, {\"accuracy\": 1.0, \"loss\": 0.03515227511525154, \"time-step\": 1110}, {\"accuracy\": 1.0, \"loss\": 0.029194947332143784, \"time-step\": 1111}, {\"accuracy\": 1.0, \"loss\": 0.03505488112568855, \"time-step\": 1112}, {\"accuracy\": 1.0, \"loss\": 0.02909175492823124, \"time-step\": 1113}, {\"accuracy\": 1.0, \"loss\": 0.0349554643034935, \"time-step\": 1114}, {\"accuracy\": 1.0, \"loss\": 0.028989091515541077, \"time-step\": 1115}, {\"accuracy\": 1.0, \"loss\": 0.03485419601202011, \"time-step\": 1116}, {\"accuracy\": 1.0, \"loss\": 0.02888689935207367, \"time-step\": 1117}, {\"accuracy\": 1.0, \"loss\": 0.03475112095475197, \"time-step\": 1118}, {\"accuracy\": 1.0, \"loss\": 0.028785187751054764, \"time-step\": 1119}, {\"accuracy\": 1.0, \"loss\": 0.03464648127555847, \"time-step\": 1120}, {\"accuracy\": 1.0, \"loss\": 0.028683871030807495, \"time-step\": 1121}, {\"accuracy\": 1.0, \"loss\": 0.034540124237537384, \"time-step\": 1122}, {\"accuracy\": 1.0, \"loss\": 0.028582941740751266, \"time-step\": 1123}, {\"accuracy\": 1.0, \"loss\": 0.03443213924765587, \"time-step\": 1124}, {\"accuracy\": 1.0, \"loss\": 0.02848239243030548, \"time-step\": 1125}, {\"accuracy\": 1.0, \"loss\": 0.034322772175073624, \"time-step\": 1126}, {\"accuracy\": 1.0, \"loss\": 0.028382154181599617, \"time-step\": 1127}, {\"accuracy\": 1.0, \"loss\": 0.03421185910701752, \"time-step\": 1128}, {\"accuracy\": 1.0, \"loss\": 0.028282269835472107, \"time-step\": 1129}, {\"accuracy\": 1.0, \"loss\": 0.03409970551729202, \"time-step\": 1130}, {\"accuracy\": 1.0, \"loss\": 0.02818278782069683, \"time-step\": 1131}, {\"accuracy\": 1.0, \"loss\": 0.03398643061518669, \"time-step\": 1132}, {\"accuracy\": 1.0, \"loss\": 0.02808353491127491, \"time-step\": 1133}, {\"accuracy\": 1.0, \"loss\": 0.03387177363038063, \"time-step\": 1134}, {\"accuracy\": 1.0, \"loss\": 0.02798452042043209, \"time-step\": 1135}, {\"accuracy\": 1.0, \"loss\": 0.03375602513551712, \"time-step\": 1136}, {\"accuracy\": 1.0, \"loss\": 0.02788597159087658, \"time-step\": 1137}, {\"accuracy\": 1.0, \"loss\": 0.03363935276865959, \"time-step\": 1138}, {\"accuracy\": 1.0, \"loss\": 0.027787627652287483, \"time-step\": 1139}, {\"accuracy\": 1.0, \"loss\": 0.0335216261446476, \"time-step\": 1140}, {\"accuracy\": 1.0, \"loss\": 0.027689501643180847, \"time-step\": 1141}, {\"accuracy\": 1.0, \"loss\": 0.03340292349457741, \"time-step\": 1142}, {\"accuracy\": 1.0, \"loss\": 0.0275917649269104, \"time-step\": 1143}, {\"accuracy\": 1.0, \"loss\": 0.0332835428416729, \"time-step\": 1144}, {\"accuracy\": 1.0, \"loss\": 0.02749428153038025, \"time-step\": 1145}, {\"accuracy\": 1.0, \"loss\": 0.03316320851445198, \"time-step\": 1146}, {\"accuracy\": 1.0, \"loss\": 0.027396971359848976, \"time-step\": 1147}, {\"accuracy\": 1.0, \"loss\": 0.03304218128323555, \"time-step\": 1148}, {\"accuracy\": 1.0, \"loss\": 0.027299996465444565, \"time-step\": 1149}, {\"accuracy\": 1.0, \"loss\": 0.03292052447795868, \"time-step\": 1150}, {\"accuracy\": 1.0, \"loss\": 0.02720329537987709, \"time-step\": 1151}, {\"accuracy\": 1.0, \"loss\": 0.032798297703266144, \"time-step\": 1152}, {\"accuracy\": 1.0, \"loss\": 0.02710685133934021, \"time-step\": 1153}, {\"accuracy\": 1.0, \"loss\": 0.032675400376319885, \"time-step\": 1154}, {\"accuracy\": 1.0, \"loss\": 0.027010556310415268, \"time-step\": 1155}, {\"accuracy\": 1.0, \"loss\": 0.03255201131105423, \"time-step\": 1156}, {\"accuracy\": 1.0, \"loss\": 0.026914719492197037, \"time-step\": 1157}, {\"accuracy\": 1.0, \"loss\": 0.0324283093214035, \"time-step\": 1158}, {\"accuracy\": 1.0, \"loss\": 0.026819050312042236, \"time-step\": 1159}, {\"accuracy\": 1.0, \"loss\": 0.032304052263498306, \"time-step\": 1160}, {\"accuracy\": 1.0, \"loss\": 0.02672361582517624, \"time-step\": 1161}, {\"accuracy\": 1.0, \"loss\": 0.0321795716881752, \"time-step\": 1162}, {\"accuracy\": 1.0, \"loss\": 0.02662854827940464, \"time-step\": 1163}, {\"accuracy\": 1.0, \"loss\": 0.032054681330919266, \"time-step\": 1164}, {\"accuracy\": 1.0, \"loss\": 0.0265335850417614, \"time-step\": 1165}, {\"accuracy\": 1.0, \"loss\": 0.03192945569753647, \"time-step\": 1166}, {\"accuracy\": 1.0, \"loss\": 0.02643892914056778, \"time-step\": 1167}, {\"accuracy\": 1.0, \"loss\": 0.03180404379963875, \"time-step\": 1168}, {\"accuracy\": 1.0, \"loss\": 0.026344604790210724, \"time-step\": 1169}, {\"accuracy\": 1.0, \"loss\": 0.03167841210961342, \"time-step\": 1170}, {\"accuracy\": 1.0, \"loss\": 0.026250487193465233, \"time-step\": 1171}, {\"accuracy\": 1.0, \"loss\": 0.03155267983675003, \"time-step\": 1172}, {\"accuracy\": 1.0, \"loss\": 0.026156682521104813, \"time-step\": 1173}, {\"accuracy\": 1.0, \"loss\": 0.0314268134534359, \"time-step\": 1174}, {\"accuracy\": 1.0, \"loss\": 0.026063062250614166, \"time-step\": 1175}, {\"accuracy\": 1.0, \"loss\": 0.03130084276199341, \"time-step\": 1176}, {\"accuracy\": 1.0, \"loss\": 0.025969862937927246, \"time-step\": 1177}, {\"accuracy\": 1.0, \"loss\": 0.031174909323453903, \"time-step\": 1178}, {\"accuracy\": 1.0, \"loss\": 0.025876760482788086, \"time-step\": 1179}, {\"accuracy\": 1.0, \"loss\": 0.031048765406012535, \"time-step\": 1180}, {\"accuracy\": 1.0, \"loss\": 0.02578401193022728, \"time-step\": 1181}, {\"accuracy\": 1.0, \"loss\": 0.030922815203666687, \"time-step\": 1182}, {\"accuracy\": 1.0, \"loss\": 0.025691594928503036, \"time-step\": 1183}, {\"accuracy\": 1.0, \"loss\": 0.03079686313867569, \"time-step\": 1184}, {\"accuracy\": 1.0, \"loss\": 0.025599345564842224, \"time-step\": 1185}, {\"accuracy\": 1.0, \"loss\": 0.030670901760458946, \"time-step\": 1186}, {\"accuracy\": 1.0, \"loss\": 0.025507444515824318, \"time-step\": 1187}, {\"accuracy\": 1.0, \"loss\": 0.030545057728886604, \"time-step\": 1188}, {\"accuracy\": 1.0, \"loss\": 0.025415819138288498, \"time-step\": 1189}, {\"accuracy\": 1.0, \"loss\": 0.03041943907737732, \"time-step\": 1190}, {\"accuracy\": 1.0, \"loss\": 0.025324515998363495, \"time-step\": 1191}, {\"accuracy\": 1.0, \"loss\": 0.030294014140963554, \"time-step\": 1192}, {\"accuracy\": 1.0, \"loss\": 0.025233451277017593, \"time-step\": 1193}, {\"accuracy\": 1.0, \"loss\": 0.03016853518784046, \"time-step\": 1194}, {\"accuracy\": 1.0, \"loss\": 0.025142621248960495, \"time-step\": 1195}, {\"accuracy\": 1.0, \"loss\": 0.030043330043554306, \"time-step\": 1196}, {\"accuracy\": 1.0, \"loss\": 0.025052104145288467, \"time-step\": 1197}, {\"accuracy\": 1.0, \"loss\": 0.02991832047700882, \"time-step\": 1198}, {\"accuracy\": 1.0, \"loss\": 0.02496197074651718, \"time-step\": 1199}, {\"accuracy\": 1.0, \"loss\": 0.02979363687336445, \"time-step\": 1200}, {\"accuracy\": 1.0, \"loss\": 0.024872014299035072, \"time-step\": 1201}, {\"accuracy\": 1.0, \"loss\": 0.02966904640197754, \"time-step\": 1202}, {\"accuracy\": 1.0, \"loss\": 0.02478235960006714, \"time-step\": 1203}, {\"accuracy\": 1.0, \"loss\": 0.029544804245233536, \"time-step\": 1204}, {\"accuracy\": 1.0, \"loss\": 0.024693049490451813, \"time-step\": 1205}, {\"accuracy\": 1.0, \"loss\": 0.02942080795764923, \"time-step\": 1206}, {\"accuracy\": 1.0, \"loss\": 0.02460402250289917, \"time-step\": 1207}, {\"accuracy\": 1.0, \"loss\": 0.029297107830643654, \"time-step\": 1208}, {\"accuracy\": 1.0, \"loss\": 0.024515211582183838, \"time-step\": 1209}, {\"accuracy\": 1.0, \"loss\": 0.029173707589507103, \"time-step\": 1210}, {\"accuracy\": 1.0, \"loss\": 0.024426830932497978, \"time-step\": 1211}, {\"accuracy\": 1.0, \"loss\": 0.02905074506998062, \"time-step\": 1212}, {\"accuracy\": 1.0, \"loss\": 0.02433866262435913, \"time-step\": 1213}, {\"accuracy\": 1.0, \"loss\": 0.02892804704606533, \"time-step\": 1214}, {\"accuracy\": 1.0, \"loss\": 0.02425079233944416, \"time-step\": 1215}, {\"accuracy\": 1.0, \"loss\": 0.028805628418922424, \"time-step\": 1216}, {\"accuracy\": 1.0, \"loss\": 0.02416321076452732, \"time-step\": 1217}, {\"accuracy\": 1.0, \"loss\": 0.028683632612228394, \"time-step\": 1218}, {\"accuracy\": 1.0, \"loss\": 0.02407592162489891, \"time-step\": 1219}, {\"accuracy\": 1.0, \"loss\": 0.028561905026435852, \"time-step\": 1220}, {\"accuracy\": 1.0, \"loss\": 0.023988991975784302, \"time-step\": 1221}, {\"accuracy\": 1.0, \"loss\": 0.0284406878054142, \"time-step\": 1222}, {\"accuracy\": 1.0, \"loss\": 0.02390229143202305, \"time-step\": 1223}, {\"accuracy\": 1.0, \"loss\": 0.028319815173745155, \"time-step\": 1224}, {\"accuracy\": 1.0, \"loss\": 0.023815933614969254, \"time-step\": 1225}, {\"accuracy\": 1.0, \"loss\": 0.02819938212633133, \"time-step\": 1226}, {\"accuracy\": 1.0, \"loss\": 0.023729931563138962, \"time-step\": 1227}, {\"accuracy\": 1.0, \"loss\": 0.02807939052581787, \"time-step\": 1228}, {\"accuracy\": 1.0, \"loss\": 0.02364415116608143, \"time-step\": 1229}, {\"accuracy\": 1.0, \"loss\": 0.02795969322323799, \"time-step\": 1230}, {\"accuracy\": 1.0, \"loss\": 0.023558666929602623, \"time-step\": 1231}, {\"accuracy\": 1.0, \"loss\": 0.027840469032526016, \"time-step\": 1232}, {\"accuracy\": 1.0, \"loss\": 0.023473519831895828, \"time-step\": 1233}, {\"accuracy\": 1.0, \"loss\": 0.027721691876649857, \"time-step\": 1234}, {\"accuracy\": 1.0, \"loss\": 0.02338869869709015, \"time-step\": 1235}, {\"accuracy\": 1.0, \"loss\": 0.027603330090641975, \"time-step\": 1236}, {\"accuracy\": 1.0, \"loss\": 0.023304123431444168, \"time-step\": 1237}, {\"accuracy\": 1.0, \"loss\": 0.027485491707921028, \"time-step\": 1238}, {\"accuracy\": 1.0, \"loss\": 0.023219965398311615, \"time-step\": 1239}, {\"accuracy\": 1.0, \"loss\": 0.027368148788809776, \"time-step\": 1240}, {\"accuracy\": 1.0, \"loss\": 0.023136012256145477, \"time-step\": 1241}, {\"accuracy\": 1.0, \"loss\": 0.027251116931438446, \"time-step\": 1242}, {\"accuracy\": 1.0, \"loss\": 0.02305237017571926, \"time-step\": 1243}, {\"accuracy\": 1.0, \"loss\": 0.02713456004858017, \"time-step\": 1244}, {\"accuracy\": 1.0, \"loss\": 0.022968990728259087, \"time-step\": 1245}, {\"accuracy\": 1.0, \"loss\": 0.027018416672945023, \"time-step\": 1246}, {\"accuracy\": 1.0, \"loss\": 0.022885965183377266, \"time-step\": 1247}, {\"accuracy\": 1.0, \"loss\": 0.026902811601758003, \"time-step\": 1248}, {\"accuracy\": 1.0, \"loss\": 0.022803233936429024, \"time-step\": 1249}, {\"accuracy\": 1.0, \"loss\": 0.02678765542805195, \"time-step\": 1250}, {\"accuracy\": 1.0, \"loss\": 0.022720780223608017, \"time-step\": 1251}, {\"accuracy\": 1.0, \"loss\": 0.026673024520277977, \"time-step\": 1252}, {\"accuracy\": 1.0, \"loss\": 0.02263864502310753, \"time-step\": 1253}, {\"accuracy\": 1.0, \"loss\": 0.026558753103017807, \"time-step\": 1254}, {\"accuracy\": 1.0, \"loss\": 0.022556805983185768, \"time-step\": 1255}, {\"accuracy\": 1.0, \"loss\": 0.026445042341947556, \"time-step\": 1256}, {\"accuracy\": 1.0, \"loss\": 0.02247520349919796, \"time-step\": 1257}, {\"accuracy\": 1.0, \"loss\": 0.026331709697842598, \"time-step\": 1258}, {\"accuracy\": 1.0, \"loss\": 0.02239396423101425, \"time-step\": 1259}, {\"accuracy\": 1.0, \"loss\": 0.02621900849044323, \"time-step\": 1260}, {\"accuracy\": 1.0, \"loss\": 0.02231311984360218, \"time-step\": 1261}, {\"accuracy\": 1.0, \"loss\": 0.026106836274266243, \"time-step\": 1262}, {\"accuracy\": 1.0, \"loss\": 0.022232426330447197, \"time-step\": 1263}, {\"accuracy\": 1.0, \"loss\": 0.025994978845119476, \"time-step\": 1264}, {\"accuracy\": 1.0, \"loss\": 0.02215208113193512, \"time-step\": 1265}, {\"accuracy\": 1.0, \"loss\": 0.02588372863829136, \"time-step\": 1266}, {\"accuracy\": 1.0, \"loss\": 0.0220720823854208, \"time-step\": 1267}, {\"accuracy\": 1.0, \"loss\": 0.02577297016978264, \"time-step\": 1268}, {\"accuracy\": 1.0, \"loss\": 0.02199234440922737, \"time-step\": 1269}, {\"accuracy\": 1.0, \"loss\": 0.025662608444690704, \"time-step\": 1270}, {\"accuracy\": 1.0, \"loss\": 0.0219128355383873, \"time-step\": 1271}, {\"accuracy\": 1.0, \"loss\": 0.025552699342370033, \"time-step\": 1272}, {\"accuracy\": 1.0, \"loss\": 0.021833665668964386, \"time-step\": 1273}, {\"accuracy\": 1.0, \"loss\": 0.02544335275888443, \"time-step\": 1274}, {\"accuracy\": 1.0, \"loss\": 0.021754838526248932, \"time-step\": 1275}, {\"accuracy\": 1.0, \"loss\": 0.025334561243653297, \"time-step\": 1276}, {\"accuracy\": 1.0, \"loss\": 0.021676240488886833, \"time-step\": 1277}, {\"accuracy\": 1.0, \"loss\": 0.02522621676325798, \"time-step\": 1278}, {\"accuracy\": 1.0, \"loss\": 0.021598005667328835, \"time-step\": 1279}, {\"accuracy\": 1.0, \"loss\": 0.025118328630924225, \"time-step\": 1280}, {\"accuracy\": 1.0, \"loss\": 0.021519968286156654, \"time-step\": 1281}, {\"accuracy\": 1.0, \"loss\": 0.025010891258716583, \"time-step\": 1282}, {\"accuracy\": 1.0, \"loss\": 0.021442223340272903, \"time-step\": 1283}, {\"accuracy\": 1.0, \"loss\": 0.024903949350118637, \"time-step\": 1284}, {\"accuracy\": 1.0, \"loss\": 0.021364856511354446, \"time-step\": 1285}, {\"accuracy\": 1.0, \"loss\": 0.02479763701558113, \"time-step\": 1286}, {\"accuracy\": 1.0, \"loss\": 0.02128777466714382, \"time-step\": 1287}, {\"accuracy\": 1.0, \"loss\": 0.02469174563884735, \"time-step\": 1288}, {\"accuracy\": 1.0, \"loss\": 0.021210884675383568, \"time-step\": 1289}, {\"accuracy\": 1.0, \"loss\": 0.02458622306585312, \"time-step\": 1290}, {\"accuracy\": 1.0, \"loss\": 0.02113434299826622, \"time-step\": 1291}, {\"accuracy\": 1.0, \"loss\": 0.024481285363435745, \"time-step\": 1292}, {\"accuracy\": 1.0, \"loss\": 0.02105802670121193, \"time-step\": 1293}, {\"accuracy\": 1.0, \"loss\": 0.02437678724527359, \"time-step\": 1294}, {\"accuracy\": 1.0, \"loss\": 0.020982027053833008, \"time-step\": 1295}, {\"accuracy\": 1.0, \"loss\": 0.02427286095917225, \"time-step\": 1296}, {\"accuracy\": 1.0, \"loss\": 0.020906398072838783, \"time-step\": 1297}, {\"accuracy\": 1.0, \"loss\": 0.02416939288377762, \"time-step\": 1298}, {\"accuracy\": 1.0, \"loss\": 0.02083096280694008, \"time-step\": 1299}, {\"accuracy\": 1.0, \"loss\": 0.024066399782896042, \"time-step\": 1300}, {\"accuracy\": 1.0, \"loss\": 0.020755857229232788, \"time-step\": 1301}, {\"accuracy\": 1.0, \"loss\": 0.02396385185420513, \"time-step\": 1302}, {\"accuracy\": 1.0, \"loss\": 0.02068093605339527, \"time-step\": 1303}, {\"accuracy\": 1.0, \"loss\": 0.023861749097704887, \"time-step\": 1304}, {\"accuracy\": 1.0, \"loss\": 0.0206063874065876, \"time-step\": 1305}, {\"accuracy\": 1.0, \"loss\": 0.023760203272104263, \"time-step\": 1306}, {\"accuracy\": 1.0, \"loss\": 0.020532039925456047, \"time-step\": 1307}, {\"accuracy\": 1.0, \"loss\": 0.023659072816371918, \"time-step\": 1308}, {\"accuracy\": 1.0, \"loss\": 0.02045804262161255, \"time-step\": 1309}, {\"accuracy\": 1.0, \"loss\": 0.02355855144560337, \"time-step\": 1310}, {\"accuracy\": 1.0, \"loss\": 0.02038433961570263, \"time-step\": 1311}, {\"accuracy\": 1.0, \"loss\": 0.02345842868089676, \"time-step\": 1312}, {\"accuracy\": 1.0, \"loss\": 0.02031087689101696, \"time-step\": 1313}, {\"accuracy\": 1.0, \"loss\": 0.023358788341283798, \"time-step\": 1314}, {\"accuracy\": 1.0, \"loss\": 0.020237674936652184, \"time-step\": 1315}, {\"accuracy\": 1.0, \"loss\": 0.02325957641005516, \"time-step\": 1316}, {\"accuracy\": 1.0, \"loss\": 0.020164769142866135, \"time-step\": 1317}, {\"accuracy\": 1.0, \"loss\": 0.02316087856888771, \"time-step\": 1318}, {\"accuracy\": 1.0, \"loss\": 0.02009212225675583, \"time-step\": 1319}, {\"accuracy\": 1.0, \"loss\": 0.02306266874074936, \"time-step\": 1320}, {\"accuracy\": 1.0, \"loss\": 0.020019780844449997, \"time-step\": 1321}, {\"accuracy\": 1.0, \"loss\": 0.022964898496866226, \"time-step\": 1322}, {\"accuracy\": 1.0, \"loss\": 0.019947681576013565, \"time-step\": 1323}, {\"accuracy\": 1.0, \"loss\": 0.022867579013109207, \"time-step\": 1324}, {\"accuracy\": 1.0, \"loss\": 0.019875824451446533, \"time-step\": 1325}, {\"accuracy\": 1.0, \"loss\": 0.022770727053284645, \"time-step\": 1326}, {\"accuracy\": 1.0, \"loss\": 0.019804270938038826, \"time-step\": 1327}, {\"accuracy\": 1.0, \"loss\": 0.02267433889210224, \"time-step\": 1328}, {\"accuracy\": 1.0, \"loss\": 0.01973298378288746, \"time-step\": 1329}, {\"accuracy\": 1.0, \"loss\": 0.02257835492491722, \"time-step\": 1330}, {\"accuracy\": 1.0, \"loss\": 0.019661827012896538, \"time-step\": 1331}, {\"accuracy\": 1.0, \"loss\": 0.022482749074697495, \"time-step\": 1332}, {\"accuracy\": 1.0, \"loss\": 0.01959099993109703, \"time-step\": 1333}, {\"accuracy\": 1.0, \"loss\": 0.02238771691918373, \"time-step\": 1334}, {\"accuracy\": 1.0, \"loss\": 0.01952051930129528, \"time-step\": 1335}, {\"accuracy\": 1.0, \"loss\": 0.022293182089924812, \"time-step\": 1336}, {\"accuracy\": 1.0, \"loss\": 0.019450321793556213, \"time-step\": 1337}, {\"accuracy\": 1.0, \"loss\": 0.02219908870756626, \"time-step\": 1338}, {\"accuracy\": 1.0, \"loss\": 0.019380338490009308, \"time-step\": 1339}, {\"accuracy\": 1.0, \"loss\": 0.022105425596237183, \"time-step\": 1340}, {\"accuracy\": 1.0, \"loss\": 0.019310589879751205, \"time-step\": 1341}, {\"accuracy\": 1.0, \"loss\": 0.022012146189808846, \"time-step\": 1342}, {\"accuracy\": 1.0, \"loss\": 0.019241079688072205, \"time-step\": 1343}, {\"accuracy\": 1.0, \"loss\": 0.021919352933764458, \"time-step\": 1344}, {\"accuracy\": 1.0, \"loss\": 0.01917189359664917, \"time-step\": 1345}, {\"accuracy\": 1.0, \"loss\": 0.021827001124620438, \"time-step\": 1346}, {\"accuracy\": 1.0, \"loss\": 0.019102878868579865, \"time-step\": 1347}, {\"accuracy\": 1.0, \"loss\": 0.021735060960054398, \"time-step\": 1348}, {\"accuracy\": 1.0, \"loss\": 0.019034145399928093, \"time-step\": 1349}, {\"accuracy\": 1.0, \"loss\": 0.02164357155561447, \"time-step\": 1350}, {\"accuracy\": 1.0, \"loss\": 0.018965689465403557, \"time-step\": 1351}, {\"accuracy\": 1.0, \"loss\": 0.0215525534003973, \"time-step\": 1352}, {\"accuracy\": 1.0, \"loss\": 0.01889750361442566, \"time-step\": 1353}, {\"accuracy\": 1.0, \"loss\": 0.02146189846098423, \"time-step\": 1354}, {\"accuracy\": 1.0, \"loss\": 0.018829533830285072, \"time-step\": 1355}, {\"accuracy\": 1.0, \"loss\": 0.021371757611632347, \"time-step\": 1356}, {\"accuracy\": 1.0, \"loss\": 0.018761875107884407, \"time-step\": 1357}, {\"accuracy\": 1.0, \"loss\": 0.021282045170664787, \"time-step\": 1358}, {\"accuracy\": 1.0, \"loss\": 0.018694471567869186, \"time-step\": 1359}, {\"accuracy\": 1.0, \"loss\": 0.021192779764533043, \"time-step\": 1360}, {\"accuracy\": 1.0, \"loss\": 0.018627211451530457, \"time-step\": 1361}, {\"accuracy\": 1.0, \"loss\": 0.02110375463962555, \"time-step\": 1362}, {\"accuracy\": 1.0, \"loss\": 0.018560199066996574, \"time-step\": 1363}, {\"accuracy\": 1.0, \"loss\": 0.021015282720327377, \"time-step\": 1364}, {\"accuracy\": 1.0, \"loss\": 0.018493494018912315, \"time-step\": 1365}, {\"accuracy\": 1.0, \"loss\": 0.020927229896187782, \"time-step\": 1366}, {\"accuracy\": 1.0, \"loss\": 0.01842712238430977, \"time-step\": 1367}, {\"accuracy\": 1.0, \"loss\": 0.020839707925915718, \"time-step\": 1368}, {\"accuracy\": 1.0, \"loss\": 0.018360942602157593, \"time-step\": 1369}, {\"accuracy\": 1.0, \"loss\": 0.020752480253577232, \"time-step\": 1370}, {\"accuracy\": 1.0, \"loss\": 0.018294909968972206, \"time-step\": 1371}, {\"accuracy\": 1.0, \"loss\": 0.020665623247623444, \"time-step\": 1372}, {\"accuracy\": 1.0, \"loss\": 0.018229210749268532, \"time-step\": 1373}, {\"accuracy\": 1.0, \"loss\": 0.0205792635679245, \"time-step\": 1374}, {\"accuracy\": 1.0, \"loss\": 0.018163712695240974, \"time-step\": 1375}, {\"accuracy\": 1.0, \"loss\": 0.020493298768997192, \"time-step\": 1376}, {\"accuracy\": 1.0, \"loss\": 0.01809847354888916, \"time-step\": 1377}, {\"accuracy\": 1.0, \"loss\": 0.020407766103744507, \"time-step\": 1378}, {\"accuracy\": 1.0, \"loss\": 0.018033519387245178, \"time-step\": 1379}, {\"accuracy\": 1.0, \"loss\": 0.020322585478425026, \"time-step\": 1380}, {\"accuracy\": 1.0, \"loss\": 0.017968716099858284, \"time-step\": 1381}, {\"accuracy\": 1.0, \"loss\": 0.020237786695361137, \"time-step\": 1382}, {\"accuracy\": 1.0, \"loss\": 0.017904222011566162, \"time-step\": 1383}, {\"accuracy\": 1.0, \"loss\": 0.020153433084487915, \"time-step\": 1384}, {\"accuracy\": 1.0, \"loss\": 0.01783984713256359, \"time-step\": 1385}, {\"accuracy\": 1.0, \"loss\": 0.02006937377154827, \"time-step\": 1386}, {\"accuracy\": 1.0, \"loss\": 0.01777580939233303, \"time-step\": 1387}, {\"accuracy\": 1.0, \"loss\": 0.019985901191830635, \"time-step\": 1388}, {\"accuracy\": 1.0, \"loss\": 0.017712002620100975, \"time-step\": 1389}, {\"accuracy\": 1.0, \"loss\": 0.019902680069208145, \"time-step\": 1390}, {\"accuracy\": 1.0, \"loss\": 0.017648380249738693, \"time-step\": 1391}, {\"accuracy\": 1.0, \"loss\": 0.019819872453808784, \"time-step\": 1392}, {\"accuracy\": 1.0, \"loss\": 0.01758502423763275, \"time-step\": 1393}, {\"accuracy\": 1.0, \"loss\": 0.019737474620342255, \"time-step\": 1394}, {\"accuracy\": 1.0, \"loss\": 0.017521923407912254, \"time-step\": 1395}, {\"accuracy\": 1.0, \"loss\": 0.019655529409646988, \"time-step\": 1396}, {\"accuracy\": 1.0, \"loss\": 0.01745905913412571, \"time-step\": 1397}, {\"accuracy\": 1.0, \"loss\": 0.019573932513594627, \"time-step\": 1398}, {\"accuracy\": 1.0, \"loss\": 0.01739642769098282, \"time-step\": 1399}, {\"accuracy\": 1.0, \"loss\": 0.019492723047733307, \"time-step\": 1400}, {\"accuracy\": 1.0, \"loss\": 0.017333997413516045, \"time-step\": 1401}, {\"accuracy\": 1.0, \"loss\": 0.019411878660321236, \"time-step\": 1402}, {\"accuracy\": 1.0, \"loss\": 0.017271822318434715, \"time-step\": 1403}, {\"accuracy\": 1.0, \"loss\": 0.01933145895600319, \"time-step\": 1404}, {\"accuracy\": 1.0, \"loss\": 0.017209816724061966, \"time-step\": 1405}, {\"accuracy\": 1.0, \"loss\": 0.019251354038715363, \"time-step\": 1406}, {\"accuracy\": 1.0, \"loss\": 0.017148131504654884, \"time-step\": 1407}, {\"accuracy\": 1.0, \"loss\": 0.019171686843037605, \"time-step\": 1408}, {\"accuracy\": 1.0, \"loss\": 0.01708655059337616, \"time-step\": 1409}, {\"accuracy\": 1.0, \"loss\": 0.019092269241809845, \"time-step\": 1410}, {\"accuracy\": 1.0, \"loss\": 0.017025265842676163, \"time-step\": 1411}, {\"accuracy\": 1.0, \"loss\": 0.01901336759328842, \"time-step\": 1412}, {\"accuracy\": 1.0, \"loss\": 0.016964249312877655, \"time-step\": 1413}, {\"accuracy\": 1.0, \"loss\": 0.01893479749560356, \"time-step\": 1414}, {\"accuracy\": 1.0, \"loss\": 0.016903391107916832, \"time-step\": 1415}, {\"accuracy\": 1.0, \"loss\": 0.018856622278690338, \"time-step\": 1416}, {\"accuracy\": 1.0, \"loss\": 0.016842808574438095, \"time-step\": 1417}, {\"accuracy\": 1.0, \"loss\": 0.018778754398226738, \"time-step\": 1418}, {\"accuracy\": 1.0, \"loss\": 0.016782408580183983, \"time-step\": 1419}, {\"accuracy\": 1.0, \"loss\": 0.01870124042034149, \"time-step\": 1420}, {\"accuracy\": 1.0, \"loss\": 0.016722148284316063, \"time-step\": 1421}, {\"accuracy\": 1.0, \"loss\": 0.0186240803450346, \"time-step\": 1422}, {\"accuracy\": 1.0, \"loss\": 0.016662271693348885, \"time-step\": 1423}, {\"accuracy\": 1.0, \"loss\": 0.01854741759598255, \"time-step\": 1424}, {\"accuracy\": 1.0, \"loss\": 0.016602544113993645, \"time-step\": 1425}, {\"accuracy\": 1.0, \"loss\": 0.0184710081666708, \"time-step\": 1426}, {\"accuracy\": 1.0, \"loss\": 0.01654297485947609, \"time-step\": 1427}, {\"accuracy\": 1.0, \"loss\": 0.018394887447357178, \"time-step\": 1428}, {\"accuracy\": 1.0, \"loss\": 0.01648365519940853, \"time-step\": 1429}, {\"accuracy\": 1.0, \"loss\": 0.018319198861718178, \"time-step\": 1430}, {\"accuracy\": 1.0, \"loss\": 0.01642456278204918, \"time-step\": 1431}, {\"accuracy\": 1.0, \"loss\": 0.01824386790394783, \"time-step\": 1432}, {\"accuracy\": 1.0, \"loss\": 0.01636573299765587, \"time-step\": 1433}, {\"accuracy\": 1.0, \"loss\": 0.01816890388727188, \"time-step\": 1434}, {\"accuracy\": 1.0, \"loss\": 0.016307029873132706, \"time-step\": 1435}, {\"accuracy\": 1.0, \"loss\": 0.018094222992658615, \"time-step\": 1436}, {\"accuracy\": 1.0, \"loss\": 0.016248615458607674, \"time-step\": 1437}, {\"accuracy\": 1.0, \"loss\": 0.018020015209913254, \"time-step\": 1438}, {\"accuracy\": 1.0, \"loss\": 0.01619044318795204, \"time-step\": 1439}, {\"accuracy\": 1.0, \"loss\": 0.017946122214198112, \"time-step\": 1440}, {\"accuracy\": 1.0, \"loss\": 0.016132425516843796, \"time-step\": 1441}, {\"accuracy\": 1.0, \"loss\": 0.017872458323836327, \"time-step\": 1442}, {\"accuracy\": 1.0, \"loss\": 0.016074562445282936, \"time-step\": 1443}, {\"accuracy\": 1.0, \"loss\": 0.01779918745160103, \"time-step\": 1444}, {\"accuracy\": 1.0, \"loss\": 0.0160170067101717, \"time-step\": 1445}, {\"accuracy\": 1.0, \"loss\": 0.017726296558976173, \"time-step\": 1446}, {\"accuracy\": 1.0, \"loss\": 0.015959622338414192, \"time-step\": 1447}, {\"accuracy\": 1.0, \"loss\": 0.017653707414865494, \"time-step\": 1448}, {\"accuracy\": 1.0, \"loss\": 0.01590241864323616, \"time-step\": 1449}, {\"accuracy\": 1.0, \"loss\": 0.017581479623913765, \"time-step\": 1450}, {\"accuracy\": 1.0, \"loss\": 0.01584547571837902, \"time-step\": 1451}, {\"accuracy\": 1.0, \"loss\": 0.017509566619992256, \"time-step\": 1452}, {\"accuracy\": 1.0, \"loss\": 0.015788761898875237, \"time-step\": 1453}, {\"accuracy\": 1.0, \"loss\": 0.01743803545832634, \"time-step\": 1454}, {\"accuracy\": 1.0, \"loss\": 0.015732239931821823, \"time-step\": 1455}, {\"accuracy\": 1.0, \"loss\": 0.017366867512464523, \"time-step\": 1456}, {\"accuracy\": 1.0, \"loss\": 0.015675928443670273, \"time-step\": 1457}, {\"accuracy\": 1.0, \"loss\": 0.01729602739214897, \"time-step\": 1458}, {\"accuracy\": 1.0, \"loss\": 0.01561982836574316, \"time-step\": 1459}, {\"accuracy\": 1.0, \"loss\": 0.017225472256541252, \"time-step\": 1460}, {\"accuracy\": 1.0, \"loss\": 0.015563876368105412, \"time-step\": 1461}, {\"accuracy\": 1.0, \"loss\": 0.01715514063835144, \"time-step\": 1462}, {\"accuracy\": 1.0, \"loss\": 0.015508139505982399, \"time-step\": 1463}, {\"accuracy\": 1.0, \"loss\": 0.017085272818803787, \"time-step\": 1464}, {\"accuracy\": 1.0, \"loss\": 0.015452686697244644, \"time-step\": 1465}, {\"accuracy\": 1.0, \"loss\": 0.01701570302248001, \"time-step\": 1466}, {\"accuracy\": 1.0, \"loss\": 0.015397327020764351, \"time-step\": 1467}, {\"accuracy\": 1.0, \"loss\": 0.01694643869996071, \"time-step\": 1468}, {\"accuracy\": 1.0, \"loss\": 0.01534225046634674, \"time-step\": 1469}, {\"accuracy\": 1.0, \"loss\": 0.016877535730600357, \"time-step\": 1470}, {\"accuracy\": 1.0, \"loss\": 0.015287394635379314, \"time-step\": 1471}, {\"accuracy\": 1.0, \"loss\": 0.016808925196528435, \"time-step\": 1472}, {\"accuracy\": 1.0, \"loss\": 0.015232700854539871, \"time-step\": 1473}, {\"accuracy\": 1.0, \"loss\": 0.016740625724196434, \"time-step\": 1474}, {\"accuracy\": 1.0, \"loss\": 0.01517820917069912, \"time-step\": 1475}, {\"accuracy\": 1.0, \"loss\": 0.016672682017087936, \"time-step\": 1476}, {\"accuracy\": 1.0, \"loss\": 0.015123971737921238, \"time-step\": 1477}, {\"accuracy\": 1.0, \"loss\": 0.01660502329468727, \"time-step\": 1478}, {\"accuracy\": 1.0, \"loss\": 0.015069892629981041, \"time-step\": 1479}, {\"accuracy\": 1.0, \"loss\": 0.016537729650735855, \"time-step\": 1480}, {\"accuracy\": 1.0, \"loss\": 0.015015954151749611, \"time-step\": 1481}, {\"accuracy\": 1.0, \"loss\": 0.016470598056912422, \"time-step\": 1482}, {\"accuracy\": 1.0, \"loss\": 0.014962218701839447, \"time-step\": 1483}, {\"accuracy\": 1.0, \"loss\": 0.016403822228312492, \"time-step\": 1484}, {\"accuracy\": 1.0, \"loss\": 0.014908695593476295, \"time-step\": 1485}, {\"accuracy\": 1.0, \"loss\": 0.016337363049387932, \"time-step\": 1486}, {\"accuracy\": 1.0, \"loss\": 0.014855436980724335, \"time-step\": 1487}, {\"accuracy\": 1.0, \"loss\": 0.01627127267420292, \"time-step\": 1488}, {\"accuracy\": 1.0, \"loss\": 0.014802252873778343, \"time-step\": 1489}, {\"accuracy\": 1.0, \"loss\": 0.01620541140437126, \"time-step\": 1490}, {\"accuracy\": 1.0, \"loss\": 0.014749359339475632, \"time-step\": 1491}, {\"accuracy\": 1.0, \"loss\": 0.01613987423479557, \"time-step\": 1492}, {\"accuracy\": 1.0, \"loss\": 0.01469656452536583, \"time-step\": 1493}, {\"accuracy\": 1.0, \"loss\": 0.016074616461992264, \"time-step\": 1494}, {\"accuracy\": 1.0, \"loss\": 0.014644077979028225, \"time-step\": 1495}, {\"accuracy\": 1.0, \"loss\": 0.016009779646992683, \"time-step\": 1496}, {\"accuracy\": 1.0, \"loss\": 0.014591773971915245, \"time-step\": 1497}, {\"accuracy\": 1.0, \"loss\": 0.01594516448676586, \"time-step\": 1498}, {\"accuracy\": 1.0, \"loss\": 0.014539660885930061, \"time-step\": 1499}, {\"accuracy\": 1.0, \"loss\": 0.015880892053246498, \"time-step\": 1500}, {\"accuracy\": 1.0, \"loss\": 0.014487712644040585, \"time-step\": 1501}, {\"accuracy\": 1.0, \"loss\": 0.015816885977983475, \"time-step\": 1502}, {\"accuracy\": 1.0, \"loss\": 0.014435974881052971, \"time-step\": 1503}, {\"accuracy\": 1.0, \"loss\": 0.01575322076678276, \"time-step\": 1504}, {\"accuracy\": 1.0, \"loss\": 0.014384478330612183, \"time-step\": 1505}, {\"accuracy\": 1.0, \"loss\": 0.015689855441451073, \"time-step\": 1506}, {\"accuracy\": 1.0, \"loss\": 0.014333155937492847, \"time-step\": 1507}, {\"accuracy\": 1.0, \"loss\": 0.01562676578760147, \"time-step\": 1508}, {\"accuracy\": 1.0, \"loss\": 0.014282011426985264, \"time-step\": 1509}, {\"accuracy\": 1.0, \"loss\": 0.015563988126814365, \"time-step\": 1510}, {\"accuracy\": 1.0, \"loss\": 0.014231054112315178, \"time-step\": 1511}, {\"accuracy\": 1.0, \"loss\": 0.0155014144256711, \"time-step\": 1512}, {\"accuracy\": 1.0, \"loss\": 0.014180183410644531, \"time-step\": 1513}, {\"accuracy\": 1.0, \"loss\": 0.015439072623848915, \"time-step\": 1514}, {\"accuracy\": 1.0, \"loss\": 0.014129582792520523, \"time-step\": 1515}, {\"accuracy\": 1.0, \"loss\": 0.015377131290733814, \"time-step\": 1516}, {\"accuracy\": 1.0, \"loss\": 0.014079204760491848, \"time-step\": 1517}, {\"accuracy\": 1.0, \"loss\": 0.015315471217036247, \"time-step\": 1518}, {\"accuracy\": 1.0, \"loss\": 0.014028920792043209, \"time-step\": 1519}, {\"accuracy\": 1.0, \"loss\": 0.015253983438014984, \"time-step\": 1520}, {\"accuracy\": 1.0, \"loss\": 0.013978819362819195, \"time-step\": 1521}, {\"accuracy\": 1.0, \"loss\": 0.0151927899569273, \"time-step\": 1522}, {\"accuracy\": 1.0, \"loss\": 0.013928917236626148, \"time-step\": 1523}, {\"accuracy\": 1.0, \"loss\": 0.015131952241063118, \"time-step\": 1524}, {\"accuracy\": 1.0, \"loss\": 0.013879240490496159, \"time-step\": 1525}, {\"accuracy\": 1.0, \"loss\": 0.015071452595293522, \"time-step\": 1526}, {\"accuracy\": 1.0, \"loss\": 0.013829859904944897, \"time-step\": 1527}, {\"accuracy\": 1.0, \"loss\": 0.01501127053052187, \"time-step\": 1528}, {\"accuracy\": 1.0, \"loss\": 0.013780616223812103, \"time-step\": 1529}, {\"accuracy\": 1.0, \"loss\": 0.014951279386878014, \"time-step\": 1530}, {\"accuracy\": 1.0, \"loss\": 0.013731485232710838, \"time-step\": 1531}, {\"accuracy\": 1.0, \"loss\": 0.01489159744232893, \"time-step\": 1532}, {\"accuracy\": 1.0, \"loss\": 0.013682611286640167, \"time-step\": 1533}, {\"accuracy\": 1.0, \"loss\": 0.014832187443971634, \"time-step\": 1534}, {\"accuracy\": 1.0, \"loss\": 0.013633891940116882, \"time-step\": 1535}, {\"accuracy\": 1.0, \"loss\": 0.014773078262805939, \"time-step\": 1536}, {\"accuracy\": 1.0, \"loss\": 0.013585338369011879, \"time-step\": 1537}, {\"accuracy\": 1.0, \"loss\": 0.01471409760415554, \"time-step\": 1538}, {\"accuracy\": 1.0, \"loss\": 0.013536826707422733, \"time-step\": 1539}, {\"accuracy\": 1.0, \"loss\": 0.014655394479632378, \"time-step\": 1540}, {\"accuracy\": 1.0, \"loss\": 0.013488641940057278, \"time-step\": 1541}, {\"accuracy\": 1.0, \"loss\": 0.014597037807106972, \"time-step\": 1542}, {\"accuracy\": 1.0, \"loss\": 0.01344066858291626, \"time-step\": 1543}, {\"accuracy\": 1.0, \"loss\": 0.014539000578224659, \"time-step\": 1544}, {\"accuracy\": 1.0, \"loss\": 0.013392830267548561, \"time-step\": 1545}, {\"accuracy\": 1.0, \"loss\": 0.01448114775121212, \"time-step\": 1546}, {\"accuracy\": 1.0, \"loss\": 0.01334511861205101, \"time-step\": 1547}, {\"accuracy\": 1.0, \"loss\": 0.014423585496842861, \"time-step\": 1548}, {\"accuracy\": 1.0, \"loss\": 0.013297615572810173, \"time-step\": 1549}, {\"accuracy\": 1.0, \"loss\": 0.014366261661052704, \"time-step\": 1550}, {\"accuracy\": 1.0, \"loss\": 0.01325029693543911, \"time-step\": 1551}, {\"accuracy\": 1.0, \"loss\": 0.01430920697748661, \"time-step\": 1552}, {\"accuracy\": 1.0, \"loss\": 0.013203159905970097, \"time-step\": 1553}, {\"accuracy\": 1.0, \"loss\": 0.014252479188144207, \"time-step\": 1554}, {\"accuracy\": 1.0, \"loss\": 0.013156306929886341, \"time-step\": 1555}, {\"accuracy\": 1.0, \"loss\": 0.014196033589541912, \"time-step\": 1556}, {\"accuracy\": 1.0, \"loss\": 0.013109520077705383, \"time-step\": 1557}, {\"accuracy\": 1.0, \"loss\": 0.014139821752905846, \"time-step\": 1558}, {\"accuracy\": 1.0, \"loss\": 0.013062936253845692, \"time-step\": 1559}, {\"accuracy\": 1.0, \"loss\": 0.014083772897720337, \"time-step\": 1560}, {\"accuracy\": 1.0, \"loss\": 0.013016492128372192, \"time-step\": 1561}, {\"accuracy\": 1.0, \"loss\": 0.014027981087565422, \"time-step\": 1562}, {\"accuracy\": 1.0, \"loss\": 0.012970157898962498, \"time-step\": 1563}, {\"accuracy\": 1.0, \"loss\": 0.013972459360957146, \"time-step\": 1564}, {\"accuracy\": 1.0, \"loss\": 0.012924104928970337, \"time-step\": 1565}, {\"accuracy\": 1.0, \"loss\": 0.013917279429733753, \"time-step\": 1566}, {\"accuracy\": 1.0, \"loss\": 0.012878242880105972, \"time-step\": 1567}, {\"accuracy\": 1.0, \"loss\": 0.013862352818250656, \"time-step\": 1568}, {\"accuracy\": 1.0, \"loss\": 0.012832501903176308, \"time-step\": 1569}, {\"accuracy\": 1.0, \"loss\": 0.013807648792862892, \"time-step\": 1570}, {\"accuracy\": 1.0, \"loss\": 0.012787005864083767, \"time-step\": 1571}, {\"accuracy\": 1.0, \"loss\": 0.01375325582921505, \"time-step\": 1572}, {\"accuracy\": 1.0, \"loss\": 0.012741648592054844, \"time-step\": 1573}, {\"accuracy\": 1.0, \"loss\": 0.013698997907340527, \"time-step\": 1574}, {\"accuracy\": 1.0, \"loss\": 0.012696430087089539, \"time-step\": 1575}, {\"accuracy\": 1.0, \"loss\": 0.013645105995237827, \"time-step\": 1576}, {\"accuracy\": 1.0, \"loss\": 0.012651436030864716, \"time-step\": 1577}, {\"accuracy\": 1.0, \"loss\": 0.013591360300779343, \"time-step\": 1578}, {\"accuracy\": 1.0, \"loss\": 0.012606472708284855, \"time-step\": 1579}, {\"accuracy\": 1.0, \"loss\": 0.01353782694786787, \"time-step\": 1580}, {\"accuracy\": 1.0, \"loss\": 0.012561832554638386, \"time-step\": 1581}, {\"accuracy\": 1.0, \"loss\": 0.013484605588018894, \"time-step\": 1582}, {\"accuracy\": 1.0, \"loss\": 0.01251728180795908, \"time-step\": 1583}, {\"accuracy\": 1.0, \"loss\": 0.013431616127490997, \"time-step\": 1584}, {\"accuracy\": 1.0, \"loss\": 0.012472914531826973, \"time-step\": 1585}, {\"accuracy\": 1.0, \"loss\": 0.013378913514316082, \"time-step\": 1586}, {\"accuracy\": 1.0, \"loss\": 0.012428765185177326, \"time-step\": 1587}, {\"accuracy\": 1.0, \"loss\": 0.013326430693268776, \"time-step\": 1588}, {\"accuracy\": 1.0, \"loss\": 0.012384729459881783, \"time-step\": 1589}, {\"accuracy\": 1.0, \"loss\": 0.013274195604026318, \"time-step\": 1590}, {\"accuracy\": 1.0, \"loss\": 0.012340916320681572, \"time-step\": 1591}, {\"accuracy\": 1.0, \"loss\": 0.013222165405750275, \"time-step\": 1592}, {\"accuracy\": 1.0, \"loss\": 0.012297236360609531, \"time-step\": 1593}, {\"accuracy\": 1.0, \"loss\": 0.013170425780117512, \"time-step\": 1594}, {\"accuracy\": 1.0, \"loss\": 0.012253767810761929, \"time-step\": 1595}, {\"accuracy\": 1.0, \"loss\": 0.013118930160999298, \"time-step\": 1596}, {\"accuracy\": 1.0, \"loss\": 0.0122104212641716, \"time-step\": 1597}, {\"accuracy\": 1.0, \"loss\": 0.013067617081105709, \"time-step\": 1598}, {\"accuracy\": 1.0, \"loss\": 0.012167224660515785, \"time-step\": 1599}, {\"accuracy\": 1.0, \"loss\": 0.013016547076404095, \"time-step\": 1600}, {\"accuracy\": 1.0, \"loss\": 0.01212423574179411, \"time-step\": 1601}, {\"accuracy\": 1.0, \"loss\": 0.012965732254087925, \"time-step\": 1602}, {\"accuracy\": 1.0, \"loss\": 0.012081331573426723, \"time-step\": 1603}, {\"accuracy\": 1.0, \"loss\": 0.01291513629257679, \"time-step\": 1604}, {\"accuracy\": 1.0, \"loss\": 0.012038717046380043, \"time-step\": 1605}, {\"accuracy\": 1.0, \"loss\": 0.012864826247096062, \"time-step\": 1606}, {\"accuracy\": 1.0, \"loss\": 0.011996209621429443, \"time-step\": 1607}, {\"accuracy\": 1.0, \"loss\": 0.012814700603485107, \"time-step\": 1608}, {\"accuracy\": 1.0, \"loss\": 0.011953867971897125, \"time-step\": 1609}, {\"accuracy\": 1.0, \"loss\": 0.012764772400259972, \"time-step\": 1610}, {\"accuracy\": 1.0, \"loss\": 0.011911618523299694, \"time-step\": 1611}, {\"accuracy\": 1.0, \"loss\": 0.01271507702767849, \"time-step\": 1612}, {\"accuracy\": 1.0, \"loss\": 0.01186957024037838, \"time-step\": 1613}, {\"accuracy\": 1.0, \"loss\": 0.012665623798966408, \"time-step\": 1614}, {\"accuracy\": 1.0, \"loss\": 0.01182771846652031, \"time-step\": 1615}, {\"accuracy\": 1.0, \"loss\": 0.012616443447768688, \"time-step\": 1616}, {\"accuracy\": 1.0, \"loss\": 0.011786055751144886, \"time-step\": 1617}, {\"accuracy\": 1.0, \"loss\": 0.012567475438117981, \"time-step\": 1618}, {\"accuracy\": 1.0, \"loss\": 0.011744476854801178, \"time-step\": 1619}, {\"accuracy\": 1.0, \"loss\": 0.01251864992082119, \"time-step\": 1620}, {\"accuracy\": 1.0, \"loss\": 0.01170302927494049, \"time-step\": 1621}, {\"accuracy\": 1.0, \"loss\": 0.012470067478716373, \"time-step\": 1622}, {\"accuracy\": 1.0, \"loss\": 0.011661826632916927, \"time-step\": 1623}, {\"accuracy\": 1.0, \"loss\": 0.012421770952641964, \"time-step\": 1624}, {\"accuracy\": 1.0, \"loss\": 0.011620782315731049, \"time-step\": 1625}, {\"accuracy\": 1.0, \"loss\": 0.012373726814985275, \"time-step\": 1626}, {\"accuracy\": 1.0, \"loss\": 0.011579882353544235, \"time-step\": 1627}, {\"accuracy\": 1.0, \"loss\": 0.012325873598456383, \"time-step\": 1628}, {\"accuracy\": 1.0, \"loss\": 0.011539117433130741, \"time-step\": 1629}, {\"accuracy\": 1.0, \"loss\": 0.012278186157345772, \"time-step\": 1630}, {\"accuracy\": 1.0, \"loss\": 0.011498527601361275, \"time-step\": 1631}, {\"accuracy\": 1.0, \"loss\": 0.012230792082846165, \"time-step\": 1632}, {\"accuracy\": 1.0, \"loss\": 0.011458111926913261, \"time-step\": 1633}, {\"accuracy\": 1.0, \"loss\": 0.01218356192111969, \"time-step\": 1634}, {\"accuracy\": 1.0, \"loss\": 0.011417856439948082, \"time-step\": 1635}, {\"accuracy\": 1.0, \"loss\": 0.01213662326335907, \"time-step\": 1636}, {\"accuracy\": 1.0, \"loss\": 0.011377759277820587, \"time-step\": 1637}, {\"accuracy\": 1.0, \"loss\": 0.012089846655726433, \"time-step\": 1638}, {\"accuracy\": 1.0, \"loss\": 0.01133782509714365, \"time-step\": 1639}, {\"accuracy\": 1.0, \"loss\": 0.012043291702866554, \"time-step\": 1640}, {\"accuracy\": 1.0, \"loss\": 0.011297984048724174, \"time-step\": 1641}, {\"accuracy\": 1.0, \"loss\": 0.011996876448392868, \"time-step\": 1642}, {\"accuracy\": 1.0, \"loss\": 0.011258276179432869, \"time-step\": 1643}, {\"accuracy\": 1.0, \"loss\": 0.011950649321079254, \"time-step\": 1644}, {\"accuracy\": 1.0, \"loss\": 0.011218744330108166, \"time-step\": 1645}, {\"accuracy\": 1.0, \"loss\": 0.011904720216989517, \"time-step\": 1646}, {\"accuracy\": 1.0, \"loss\": 0.011179391294717789, \"time-step\": 1647}, {\"accuracy\": 1.0, \"loss\": 0.01185902114957571, \"time-step\": 1648}, {\"accuracy\": 1.0, \"loss\": 0.01114021334797144, \"time-step\": 1649}, {\"accuracy\": 1.0, \"loss\": 0.01181358564645052, \"time-step\": 1650}, {\"accuracy\": 1.0, \"loss\": 0.011101260781288147, \"time-step\": 1651}, {\"accuracy\": 1.0, \"loss\": 0.01176831778138876, \"time-step\": 1652}, {\"accuracy\": 1.0, \"loss\": 0.011062365025281906, \"time-step\": 1653}, {\"accuracy\": 1.0, \"loss\": 0.01172318123281002, \"time-step\": 1654}, {\"accuracy\": 1.0, \"loss\": 0.011023596860468388, \"time-step\": 1655}, {\"accuracy\": 1.0, \"loss\": 0.011678295210003853, \"time-step\": 1656}, {\"accuracy\": 1.0, \"loss\": 0.010985004715621471, \"time-step\": 1657}, {\"accuracy\": 1.0, \"loss\": 0.011633647605776787, \"time-step\": 1658}, {\"accuracy\": 1.0, \"loss\": 0.010946592316031456, \"time-step\": 1659}, {\"accuracy\": 1.0, \"loss\": 0.01158914715051651, \"time-step\": 1660}, {\"accuracy\": 1.0, \"loss\": 0.010908364318311214, \"time-step\": 1661}, {\"accuracy\": 1.0, \"loss\": 0.011544973589479923, \"time-step\": 1662}, {\"accuracy\": 1.0, \"loss\": 0.010870265774428844, \"time-step\": 1663}, {\"accuracy\": 1.0, \"loss\": 0.011500900611281395, \"time-step\": 1664}, {\"accuracy\": 1.0, \"loss\": 0.010832270607352257, \"time-step\": 1665}, {\"accuracy\": 1.0, \"loss\": 0.011456998065114021, \"time-step\": 1666}, {\"accuracy\": 1.0, \"loss\": 0.010794364847242832, \"time-step\": 1667}, {\"accuracy\": 1.0, \"loss\": 0.011413254775106907, \"time-step\": 1668}, {\"accuracy\": 1.0, \"loss\": 0.010756637901067734, \"time-step\": 1669}, {\"accuracy\": 1.0, \"loss\": 0.01136975921690464, \"time-step\": 1670}, {\"accuracy\": 1.0, \"loss\": 0.01071912981569767, \"time-step\": 1671}, {\"accuracy\": 1.0, \"loss\": 0.011326532810926437, \"time-step\": 1672}, {\"accuracy\": 1.0, \"loss\": 0.010681717656552792, \"time-step\": 1673}, {\"accuracy\": 1.0, \"loss\": 0.011283478699624538, \"time-step\": 1674}, {\"accuracy\": 1.0, \"loss\": 0.01064455509185791, \"time-step\": 1675}, {\"accuracy\": 1.0, \"loss\": 0.011240672320127487, \"time-step\": 1676}, {\"accuracy\": 1.0, \"loss\": 0.010607396252453327, \"time-step\": 1677}, {\"accuracy\": 1.0, \"loss\": 0.011197932995855808, \"time-step\": 1678}, {\"accuracy\": 1.0, \"loss\": 0.010570451617240906, \"time-step\": 1679}, {\"accuracy\": 1.0, \"loss\": 0.011155447922647, \"time-step\": 1680}, {\"accuracy\": 1.0, \"loss\": 0.010533631779253483, \"time-step\": 1681}, {\"accuracy\": 1.0, \"loss\": 0.011113142594695091, \"time-step\": 1682}, {\"accuracy\": 1.0, \"loss\": 0.010496939532458782, \"time-step\": 1683}, {\"accuracy\": 1.0, \"loss\": 0.011071027256548405, \"time-step\": 1684}, {\"accuracy\": 1.0, \"loss\": 0.010460424236953259, \"time-step\": 1685}, {\"accuracy\": 1.0, \"loss\": 0.011029166169464588, \"time-step\": 1686}, {\"accuracy\": 1.0, \"loss\": 0.010424096137285233, \"time-step\": 1687}, {\"accuracy\": 1.0, \"loss\": 0.010987498797476292, \"time-step\": 1688}, {\"accuracy\": 1.0, \"loss\": 0.010387898422777653, \"time-step\": 1689}, {\"accuracy\": 1.0, \"loss\": 0.010945998132228851, \"time-step\": 1690}, {\"accuracy\": 1.0, \"loss\": 0.01035179290920496, \"time-step\": 1691}, {\"accuracy\": 1.0, \"loss\": 0.010904655791819096, \"time-step\": 1692}, {\"accuracy\": 1.0, \"loss\": 0.01031583733856678, \"time-step\": 1693}, {\"accuracy\": 1.0, \"loss\": 0.010863522067666054, \"time-step\": 1694}, {\"accuracy\": 1.0, \"loss\": 0.010280036367475986, \"time-step\": 1695}, {\"accuracy\": 1.0, \"loss\": 0.010822540149092674, \"time-step\": 1696}, {\"accuracy\": 1.0, \"loss\": 0.010244358330965042, \"time-step\": 1697}, {\"accuracy\": 1.0, \"loss\": 0.010781805962324142, \"time-step\": 1698}, {\"accuracy\": 1.0, \"loss\": 0.010208847932517529, \"time-step\": 1699}, {\"accuracy\": 1.0, \"loss\": 0.010741219855844975, \"time-step\": 1700}, {\"accuracy\": 1.0, \"loss\": 0.010173463262617588, \"time-step\": 1701}, {\"accuracy\": 1.0, \"loss\": 0.010700799524784088, \"time-step\": 1702}, {\"accuracy\": 1.0, \"loss\": 0.010138212703168392, \"time-step\": 1703}, {\"accuracy\": 1.0, \"loss\": 0.01066066324710846, \"time-step\": 1704}, {\"accuracy\": 1.0, \"loss\": 0.01010318286716938, \"time-step\": 1705}, {\"accuracy\": 1.0, \"loss\": 0.01062062755227089, \"time-step\": 1706}, {\"accuracy\": 1.0, \"loss\": 0.01006814744323492, \"time-step\": 1707}, {\"accuracy\": 1.0, \"loss\": 0.010580734349787235, \"time-step\": 1708}, {\"accuracy\": 1.0, \"loss\": 0.010033324360847473, \"time-step\": 1709}, {\"accuracy\": 1.0, \"loss\": 0.010541166178882122, \"time-step\": 1710}, {\"accuracy\": 1.0, \"loss\": 0.009998725727200508, \"time-step\": 1711}, {\"accuracy\": 1.0, \"loss\": 0.010501750744879246, \"time-step\": 1712}, {\"accuracy\": 1.0, \"loss\": 0.009964222088456154, \"time-step\": 1713}, {\"accuracy\": 1.0, \"loss\": 0.01046249270439148, \"time-step\": 1714}, {\"accuracy\": 1.0, \"loss\": 0.009929823689162731, \"time-step\": 1715}, {\"accuracy\": 1.0, \"loss\": 0.010423436760902405, \"time-step\": 1716}, {\"accuracy\": 1.0, \"loss\": 0.009895591996610165, \"time-step\": 1717}, {\"accuracy\": 1.0, \"loss\": 0.01038446370512247, \"time-step\": 1718}, {\"accuracy\": 1.0, \"loss\": 0.009861433878540993, \"time-step\": 1719}, {\"accuracy\": 1.0, \"loss\": 0.010345706716179848, \"time-step\": 1720}, {\"accuracy\": 1.0, \"loss\": 0.009827409870922565, \"time-step\": 1721}, {\"accuracy\": 1.0, \"loss\": 0.01030709594488144, \"time-step\": 1722}, {\"accuracy\": 1.0, \"loss\": 0.009793545119464397, \"time-step\": 1723}, {\"accuracy\": 1.0, \"loss\": 0.010268734768033028, \"time-step\": 1724}, {\"accuracy\": 1.0, \"loss\": 0.009759863838553429, \"time-step\": 1725}, {\"accuracy\": 1.0, \"loss\": 0.010230551473796368, \"time-step\": 1726}, {\"accuracy\": 1.0, \"loss\": 0.009726288728415966, \"time-step\": 1727}, {\"accuracy\": 1.0, \"loss\": 0.01019255630671978, \"time-step\": 1728}, {\"accuracy\": 1.0, \"loss\": 0.009692879393696785, \"time-step\": 1729}, {\"accuracy\": 1.0, \"loss\": 0.01015465147793293, \"time-step\": 1730}, {\"accuracy\": 1.0, \"loss\": 0.00965953804552555, \"time-step\": 1731}, {\"accuracy\": 1.0, \"loss\": 0.010116969235241413, \"time-step\": 1732}, {\"accuracy\": 1.0, \"loss\": 0.009626359678804874, \"time-step\": 1733}, {\"accuracy\": 1.0, \"loss\": 0.010079476051032543, \"time-step\": 1734}, {\"accuracy\": 1.0, \"loss\": 0.009593313559889793, \"time-step\": 1735}, {\"accuracy\": 1.0, \"loss\": 0.010042082518339157, \"time-step\": 1736}, {\"accuracy\": 1.0, \"loss\": 0.009560327976942062, \"time-step\": 1737}, {\"accuracy\": 1.0, \"loss\": 0.010004847310483456, \"time-step\": 1738}, {\"accuracy\": 1.0, \"loss\": 0.009527537040412426, \"time-step\": 1739}, {\"accuracy\": 1.0, \"loss\": 0.009967860765755177, \"time-step\": 1740}, {\"accuracy\": 1.0, \"loss\": 0.009494876489043236, \"time-step\": 1741}, {\"accuracy\": 1.0, \"loss\": 0.009931019507348537, \"time-step\": 1742}, {\"accuracy\": 1.0, \"loss\": 0.009462352842092514, \"time-step\": 1743}, {\"accuracy\": 1.0, \"loss\": 0.009894308634102345, \"time-step\": 1744}, {\"accuracy\": 1.0, \"loss\": 0.009429964236915112, \"time-step\": 1745}, {\"accuracy\": 1.0, \"loss\": 0.009857822209596634, \"time-step\": 1746}, {\"accuracy\": 1.0, \"loss\": 0.00939769484102726, \"time-step\": 1747}, {\"accuracy\": 1.0, \"loss\": 0.00982155092060566, \"time-step\": 1748}, {\"accuracy\": 1.0, \"loss\": 0.009365648962557316, \"time-step\": 1749}, {\"accuracy\": 1.0, \"loss\": 0.009785441681742668, \"time-step\": 1750}, {\"accuracy\": 1.0, \"loss\": 0.009333662688732147, \"time-step\": 1751}, {\"accuracy\": 1.0, \"loss\": 0.009749449789524078, \"time-step\": 1752}, {\"accuracy\": 1.0, \"loss\": 0.009301791898906231, \"time-step\": 1753}, {\"accuracy\": 1.0, \"loss\": 0.009713608771562576, \"time-step\": 1754}, {\"accuracy\": 1.0, \"loss\": 0.0092700170353055, \"time-step\": 1755}, {\"accuracy\": 1.0, \"loss\": 0.009677896276116371, \"time-step\": 1756}, {\"accuracy\": 1.0, \"loss\": 0.009238367900252342, \"time-step\": 1757}, {\"accuracy\": 1.0, \"loss\": 0.009642324410378933, \"time-step\": 1758}, {\"accuracy\": 1.0, \"loss\": 0.009206822142004967, \"time-step\": 1759}, {\"accuracy\": 1.0, \"loss\": 0.009606908075511456, \"time-step\": 1760}, {\"accuracy\": 1.0, \"loss\": 0.009175437502563, \"time-step\": 1761}, {\"accuracy\": 1.0, \"loss\": 0.009571781381964684, \"time-step\": 1762}, {\"accuracy\": 1.0, \"loss\": 0.00914421584457159, \"time-step\": 1763}, {\"accuracy\": 1.0, \"loss\": 0.009536714293062687, \"time-step\": 1764}, {\"accuracy\": 1.0, \"loss\": 0.009113115258514881, \"time-step\": 1765}, {\"accuracy\": 1.0, \"loss\": 0.009501887485384941, \"time-step\": 1766}, {\"accuracy\": 1.0, \"loss\": 0.009082144126296043, \"time-step\": 1767}, {\"accuracy\": 1.0, \"loss\": 0.00946720875799656, \"time-step\": 1768}, {\"accuracy\": 1.0, \"loss\": 0.009051285684108734, \"time-step\": 1769}, {\"accuracy\": 1.0, \"loss\": 0.009432678110897541, \"time-step\": 1770}, {\"accuracy\": 1.0, \"loss\": 0.00902058556675911, \"time-step\": 1771}, {\"accuracy\": 1.0, \"loss\": 0.00939828623086214, \"time-step\": 1772}, {\"accuracy\": 1.0, \"loss\": 0.00898993294686079, \"time-step\": 1773}, {\"accuracy\": 1.0, \"loss\": 0.009364031255245209, \"time-step\": 1774}, {\"accuracy\": 1.0, \"loss\": 0.00895945355296135, \"time-step\": 1775}, {\"accuracy\": 1.0, \"loss\": 0.009329969063401222, \"time-step\": 1776}, {\"accuracy\": 1.0, \"loss\": 0.008929100818932056, \"time-step\": 1777}, {\"accuracy\": 1.0, \"loss\": 0.009296047501266003, \"time-step\": 1778}, {\"accuracy\": 1.0, \"loss\": 0.00889887660741806, \"time-step\": 1779}, {\"accuracy\": 1.0, \"loss\": 0.009262311272323132, \"time-step\": 1780}, {\"accuracy\": 1.0, \"loss\": 0.008868756704032421, \"time-step\": 1781}, {\"accuracy\": 1.0, \"loss\": 0.009228707291185856, \"time-step\": 1782}, {\"accuracy\": 1.0, \"loss\": 0.008838756009936333, \"time-step\": 1783}, {\"accuracy\": 1.0, \"loss\": 0.009195193648338318, \"time-step\": 1784}, {\"accuracy\": 1.0, \"loss\": 0.008808787912130356, \"time-step\": 1785}, {\"accuracy\": 1.0, \"loss\": 0.009161875583231449, \"time-step\": 1786}, {\"accuracy\": 1.0, \"loss\": 0.008779099211096764, \"time-step\": 1787}, {\"accuracy\": 1.0, \"loss\": 0.009128735400736332, \"time-step\": 1788}, {\"accuracy\": 1.0, \"loss\": 0.008749411441385746, \"time-step\": 1789}, {\"accuracy\": 1.0, \"loss\": 0.00909571535885334, \"time-step\": 1790}, {\"accuracy\": 1.0, \"loss\": 0.008719897828996181, \"time-step\": 1791}, {\"accuracy\": 1.0, \"loss\": 0.009062876924872398, \"time-step\": 1792}, {\"accuracy\": 1.0, \"loss\": 0.008690545335412025, \"time-step\": 1793}, {\"accuracy\": 1.0, \"loss\": 0.009030220098793507, \"time-step\": 1794}, {\"accuracy\": 1.0, \"loss\": 0.008661314845085144, \"time-step\": 1795}, {\"accuracy\": 1.0, \"loss\": 0.008997691795229912, \"time-step\": 1796}, {\"accuracy\": 1.0, \"loss\": 0.008632145822048187, \"time-step\": 1797}, {\"accuracy\": 1.0, \"loss\": 0.008965269662439823, \"time-step\": 1798}, {\"accuracy\": 1.0, \"loss\": 0.008603089489042759, \"time-step\": 1799}, {\"accuracy\": 1.0, \"loss\": 0.008933036588132381, \"time-step\": 1800}, {\"accuracy\": 1.0, \"loss\": 0.00857422687113285, \"time-step\": 1801}, {\"accuracy\": 1.0, \"loss\": 0.008900970220565796, \"time-step\": 1802}, {\"accuracy\": 1.0, \"loss\": 0.00854544062167406, \"time-step\": 1803}, {\"accuracy\": 1.0, \"loss\": 0.008869036100804806, \"time-step\": 1804}, {\"accuracy\": 1.0, \"loss\": 0.00851674098521471, \"time-step\": 1805}, {\"accuracy\": 1.0, \"loss\": 0.008837188594043255, \"time-step\": 1806}, {\"accuracy\": 1.0, \"loss\": 0.008488163352012634, \"time-step\": 1807}, {\"accuracy\": 1.0, \"loss\": 0.008805477991700172, \"time-step\": 1808}, {\"accuracy\": 1.0, \"loss\": 0.008459660224616528, \"time-step\": 1809}, {\"accuracy\": 1.0, \"loss\": 0.008773934096097946, \"time-step\": 1810}, {\"accuracy\": 1.0, \"loss\": 0.00843135081231594, \"time-step\": 1811}, {\"accuracy\": 1.0, \"loss\": 0.00874259416013956, \"time-step\": 1812}, {\"accuracy\": 1.0, \"loss\": 0.008403141051530838, \"time-step\": 1813}, {\"accuracy\": 1.0, \"loss\": 0.008711335249245167, \"time-step\": 1814}, {\"accuracy\": 1.0, \"loss\": 0.00837500486522913, \"time-step\": 1815}, {\"accuracy\": 1.0, \"loss\": 0.008680223487317562, \"time-step\": 1816}, {\"accuracy\": 1.0, \"loss\": 0.008347021415829659, \"time-step\": 1817}, {\"accuracy\": 1.0, \"loss\": 0.008649296127259731, \"time-step\": 1818}, {\"accuracy\": 1.0, \"loss\": 0.008319167420268059, \"time-step\": 1819}, {\"accuracy\": 1.0, \"loss\": 0.008618470281362534, \"time-step\": 1820}, {\"accuracy\": 1.0, \"loss\": 0.00829138420522213, \"time-step\": 1821}, {\"accuracy\": 1.0, \"loss\": 0.008587803691625595, \"time-step\": 1822}, {\"accuracy\": 1.0, \"loss\": 0.008263742551207542, \"time-step\": 1823}, {\"accuracy\": 1.0, \"loss\": 0.008557233959436417, \"time-step\": 1824}, {\"accuracy\": 1.0, \"loss\": 0.008236164227128029, \"time-step\": 1825}, {\"accuracy\": 1.0, \"loss\": 0.00852684024721384, \"time-step\": 1826}, {\"accuracy\": 1.0, \"loss\": 0.008208765648305416, \"time-step\": 1827}, {\"accuracy\": 1.0, \"loss\": 0.008496607653796673, \"time-step\": 1828}, {\"accuracy\": 1.0, \"loss\": 0.008181469514966011, \"time-step\": 1829}, {\"accuracy\": 1.0, \"loss\": 0.008466491475701332, \"time-step\": 1830}, {\"accuracy\": 1.0, \"loss\": 0.008154276758432388, \"time-step\": 1831}, {\"accuracy\": 1.0, \"loss\": 0.008436532691121101, \"time-step\": 1832}, {\"accuracy\": 1.0, \"loss\": 0.008127202279865742, \"time-step\": 1833}, {\"accuracy\": 1.0, \"loss\": 0.00840671919286251, \"time-step\": 1834}, {\"accuracy\": 1.0, \"loss\": 0.008100271224975586, \"time-step\": 1835}, {\"accuracy\": 1.0, \"loss\": 0.008377023972570896, \"time-step\": 1836}, {\"accuracy\": 1.0, \"loss\": 0.008073368109762669, \"time-step\": 1837}, {\"accuracy\": 1.0, \"loss\": 0.008347420021891594, \"time-step\": 1838}, {\"accuracy\": 1.0, \"loss\": 0.008046630769968033, \"time-step\": 1839}, {\"accuracy\": 1.0, \"loss\": 0.00831795483827591, \"time-step\": 1840}, {\"accuracy\": 1.0, \"loss\": 0.008019909262657166, \"time-step\": 1841}, {\"accuracy\": 1.0, \"loss\": 0.008288581855595112, \"time-step\": 1842}, {\"accuracy\": 1.0, \"loss\": 0.007993310689926147, \"time-step\": 1843}, {\"accuracy\": 1.0, \"loss\": 0.00825939979404211, \"time-step\": 1844}, {\"accuracy\": 1.0, \"loss\": 0.007966909557580948, \"time-step\": 1845}, {\"accuracy\": 1.0, \"loss\": 0.008230376057326794, \"time-step\": 1846}, {\"accuracy\": 1.0, \"loss\": 0.007940586656332016, \"time-step\": 1847}, {\"accuracy\": 1.0, \"loss\": 0.008201462216675282, \"time-step\": 1848}, {\"accuracy\": 1.0, \"loss\": 0.007914379239082336, \"time-step\": 1849}, {\"accuracy\": 1.0, \"loss\": 0.00817269366234541, \"time-step\": 1850}, {\"accuracy\": 1.0, \"loss\": 0.00788826309144497, \"time-step\": 1851}, {\"accuracy\": 1.0, \"loss\": 0.008144024759531021, \"time-step\": 1852}, {\"accuracy\": 1.0, \"loss\": 0.007862245664000511, \"time-step\": 1853}, {\"accuracy\": 1.0, \"loss\": 0.008115546777844429, \"time-step\": 1854}, {\"accuracy\": 1.0, \"loss\": 0.007836399599909782, \"time-step\": 1855}, {\"accuracy\": 1.0, \"loss\": 0.00808718428015709, \"time-step\": 1856}, {\"accuracy\": 1.0, \"loss\": 0.007810586132109165, \"time-step\": 1857}, {\"accuracy\": 1.0, \"loss\": 0.008058860898017883, \"time-step\": 1858}, {\"accuracy\": 1.0, \"loss\": 0.007784857880324125, \"time-step\": 1859}, {\"accuracy\": 1.0, \"loss\": 0.008030695840716362, \"time-step\": 1860}, {\"accuracy\": 1.0, \"loss\": 0.007759273983538151, \"time-step\": 1861}, {\"accuracy\": 1.0, \"loss\": 0.00800267606973648, \"time-step\": 1862}, {\"accuracy\": 1.0, \"loss\": 0.007733779959380627, \"time-step\": 1863}, {\"accuracy\": 1.0, \"loss\": 0.007974805310368538, \"time-step\": 1864}, {\"accuracy\": 1.0, \"loss\": 0.007708407007157803, \"time-step\": 1865}, {\"accuracy\": 1.0, \"loss\": 0.007947087287902832, \"time-step\": 1866}, {\"accuracy\": 1.0, \"loss\": 0.007683188188821077, \"time-step\": 1867}, {\"accuracy\": 1.0, \"loss\": 0.00791945867240429, \"time-step\": 1868}, {\"accuracy\": 1.0, \"loss\": 0.007658025249838829, \"time-step\": 1869}, {\"accuracy\": 1.0, \"loss\": 0.00789197813719511, \"time-step\": 1870}, {\"accuracy\": 1.0, \"loss\": 0.0076330117881298065, \"time-step\": 1871}, {\"accuracy\": 1.0, \"loss\": 0.007864649407565594, \"time-step\": 1872}, {\"accuracy\": 1.0, \"loss\": 0.007608057931065559, \"time-step\": 1873}, {\"accuracy\": 1.0, \"loss\": 0.007837366312742233, \"time-step\": 1874}, {\"accuracy\": 1.0, \"loss\": 0.007583173457533121, \"time-step\": 1875}, {\"accuracy\": 1.0, \"loss\": 0.0078102112747728825, \"time-step\": 1876}, {\"accuracy\": 1.0, \"loss\": 0.007558413781225681, \"time-step\": 1877}, {\"accuracy\": 1.0, \"loss\": 0.007783218752592802, \"time-step\": 1878}, {\"accuracy\": 1.0, \"loss\": 0.0075337584130465984, \"time-step\": 1879}, {\"accuracy\": 1.0, \"loss\": 0.007756280712783337, \"time-step\": 1880}, {\"accuracy\": 1.0, \"loss\": 0.007509130053222179, \"time-step\": 1881}, {\"accuracy\": 1.0, \"loss\": 0.007729462347924709, \"time-step\": 1882}, {\"accuracy\": 1.0, \"loss\": 0.007484687492251396, \"time-step\": 1883}, {\"accuracy\": 1.0, \"loss\": 0.0077028293162584305, \"time-step\": 1884}, {\"accuracy\": 1.0, \"loss\": 0.007460325490683317, \"time-step\": 1885}, {\"accuracy\": 1.0, \"loss\": 0.007676294539123774, \"time-step\": 1886}, {\"accuracy\": 1.0, \"loss\": 0.007436074316501617, \"time-step\": 1887}, {\"accuracy\": 1.0, \"loss\": 0.007649905048310757, \"time-step\": 1888}, {\"accuracy\": 1.0, \"loss\": 0.007411966100335121, \"time-step\": 1889}, {\"accuracy\": 1.0, \"loss\": 0.00762363662943244, \"time-step\": 1890}, {\"accuracy\": 1.0, \"loss\": 0.007387866266071796, \"time-step\": 1891}, {\"accuracy\": 1.0, \"loss\": 0.007597426883876324, \"time-step\": 1892}, {\"accuracy\": 1.0, \"loss\": 0.007363910786807537, \"time-step\": 1893}, {\"accuracy\": 1.0, \"loss\": 0.007571373600512743, \"time-step\": 1894}, {\"accuracy\": 1.0, \"loss\": 0.007340059150010347, \"time-step\": 1895}, {\"accuracy\": 1.0, \"loss\": 0.00754549540579319, \"time-step\": 1896}, {\"accuracy\": 1.0, \"loss\": 0.0073163434863090515, \"time-step\": 1897}, {\"accuracy\": 1.0, \"loss\": 0.007519686594605446, \"time-step\": 1898}, {\"accuracy\": 1.0, \"loss\": 0.007292683236300945, \"time-step\": 1899}, {\"accuracy\": 1.0, \"loss\": 0.007493963465094566, \"time-step\": 1900}, {\"accuracy\": 1.0, \"loss\": 0.0072690751403570175, \"time-step\": 1901}, {\"accuracy\": 1.0, \"loss\": 0.007468344643712044, \"time-step\": 1902}, {\"accuracy\": 1.0, \"loss\": 0.007245583925396204, \"time-step\": 1903}, {\"accuracy\": 1.0, \"loss\": 0.007442871108651161, \"time-step\": 1904}, {\"accuracy\": 1.0, \"loss\": 0.007222270127385855, \"time-step\": 1905}, {\"accuracy\": 1.0, \"loss\": 0.00741759967058897, \"time-step\": 1906}, {\"accuracy\": 1.0, \"loss\": 0.007199005223810673, \"time-step\": 1907}, {\"accuracy\": 1.0, \"loss\": 0.007392344530671835, \"time-step\": 1908}, {\"accuracy\": 1.0, \"loss\": 0.007175837643444538, \"time-step\": 1909}, {\"accuracy\": 1.0, \"loss\": 0.007367216050624847, \"time-step\": 1910}, {\"accuracy\": 1.0, \"loss\": 0.007152758538722992, \"time-step\": 1911}, {\"accuracy\": 1.0, \"loss\": 0.00734220864251256, \"time-step\": 1912}, {\"accuracy\": 1.0, \"loss\": 0.00712979631498456, \"time-step\": 1913}, {\"accuracy\": 1.0, \"loss\": 0.007317338604480028, \"time-step\": 1914}, {\"accuracy\": 1.0, \"loss\": 0.007106910925358534, \"time-step\": 1915}, {\"accuracy\": 1.0, \"loss\": 0.007292529102414846, \"time-step\": 1916}, {\"accuracy\": 1.0, \"loss\": 0.007084054872393608, \"time-step\": 1917}, {\"accuracy\": 1.0, \"loss\": 0.007267806679010391, \"time-step\": 1918}, {\"accuracy\": 1.0, \"loss\": 0.007061357144266367, \"time-step\": 1919}, {\"accuracy\": 1.0, \"loss\": 0.007243271451443434, \"time-step\": 1920}, {\"accuracy\": 1.0, \"loss\": 0.007038759533315897, \"time-step\": 1921}, {\"accuracy\": 1.0, \"loss\": 0.007218805141746998, \"time-step\": 1922}, {\"accuracy\": 1.0, \"loss\": 0.007016237825155258, \"time-step\": 1923}, {\"accuracy\": 1.0, \"loss\": 0.007194428239017725, \"time-step\": 1924}, {\"accuracy\": 1.0, \"loss\": 0.006993766874074936, \"time-step\": 1925}, {\"accuracy\": 1.0, \"loss\": 0.007170185912400484, \"time-step\": 1926}, {\"accuracy\": 1.0, \"loss\": 0.006971411872655153, \"time-step\": 1927}, {\"accuracy\": 1.0, \"loss\": 0.007146033924072981, \"time-step\": 1928}, {\"accuracy\": 1.0, \"loss\": 0.006949215196073055, \"time-step\": 1929}, {\"accuracy\": 1.0, \"loss\": 0.007122064009308815, \"time-step\": 1930}, {\"accuracy\": 1.0, \"loss\": 0.006927064619958401, \"time-step\": 1931}, {\"accuracy\": 1.0, \"loss\": 0.007098136469721794, \"time-step\": 1932}, {\"accuracy\": 1.0, \"loss\": 0.006904990412294865, \"time-step\": 1933}, {\"accuracy\": 1.0, \"loss\": 0.007074309047311544, \"time-step\": 1934}, {\"accuracy\": 1.0, \"loss\": 0.006883033085614443, \"time-step\": 1935}, {\"accuracy\": 1.0, \"loss\": 0.007050645537674427, \"time-step\": 1936}, {\"accuracy\": 1.0, \"loss\": 0.006861109286546707, \"time-step\": 1937}, {\"accuracy\": 1.0, \"loss\": 0.0070269848220050335, \"time-step\": 1938}, {\"accuracy\": 1.0, \"loss\": 0.006839324254542589, \"time-step\": 1939}, {\"accuracy\": 1.0, \"loss\": 0.00700351782143116, \"time-step\": 1940}, {\"accuracy\": 1.0, \"loss\": 0.006817622110247612, \"time-step\": 1941}, {\"accuracy\": 1.0, \"loss\": 0.006980110891163349, \"time-step\": 1942}, {\"accuracy\": 1.0, \"loss\": 0.006795984227210283, \"time-step\": 1943}, {\"accuracy\": 1.0, \"loss\": 0.006956843659281731, \"time-step\": 1944}, {\"accuracy\": 1.0, \"loss\": 0.006774488836526871, \"time-step\": 1945}, {\"accuracy\": 1.0, \"loss\": 0.006933627650141716, \"time-step\": 1946}, {\"accuracy\": 1.0, \"loss\": 0.006752979941666126, \"time-step\": 1947}, {\"accuracy\": 1.0, \"loss\": 0.006910487078130245, \"time-step\": 1948}, {\"accuracy\": 1.0, \"loss\": 0.006731576286256313, \"time-step\": 1949}, {\"accuracy\": 1.0, \"loss\": 0.006887521594762802, \"time-step\": 1950}, {\"accuracy\": 1.0, \"loss\": 0.006710369139909744, \"time-step\": 1951}, {\"accuracy\": 1.0, \"loss\": 0.00686468742787838, \"time-step\": 1952}, {\"accuracy\": 1.0, \"loss\": 0.0066892108879983425, \"time-step\": 1953}, {\"accuracy\": 1.0, \"loss\": 0.0068419440649449825, \"time-step\": 1954}, {\"accuracy\": 1.0, \"loss\": 0.006668149493634701, \"time-step\": 1955}, {\"accuracy\": 1.0, \"loss\": 0.00681929849088192, \"time-step\": 1956}, {\"accuracy\": 1.0, \"loss\": 0.006647147238254547, \"time-step\": 1957}, {\"accuracy\": 1.0, \"loss\": 0.006796724628657103, \"time-step\": 1958}, {\"accuracy\": 1.0, \"loss\": 0.0066262464970350266, \"time-step\": 1959}, {\"accuracy\": 1.0, \"loss\": 0.006774268113076687, \"time-step\": 1960}, {\"accuracy\": 1.0, \"loss\": 0.006605386734008789, \"time-step\": 1961}, {\"accuracy\": 1.0, \"loss\": 0.0067518665455281734, \"time-step\": 1962}, {\"accuracy\": 1.0, \"loss\": 0.006584628019481897, \"time-step\": 1963}, {\"accuracy\": 1.0, \"loss\": 0.006729611195623875, \"time-step\": 1964}, {\"accuracy\": 1.0, \"loss\": 0.006563958711922169, \"time-step\": 1965}, {\"accuracy\": 1.0, \"loss\": 0.00670738285407424, \"time-step\": 1966}, {\"accuracy\": 1.0, \"loss\": 0.006543324328958988, \"time-step\": 1967}, {\"accuracy\": 1.0, \"loss\": 0.006685296073555946, \"time-step\": 1968}, {\"accuracy\": 1.0, \"loss\": 0.006522851996123791, \"time-step\": 1969}, {\"accuracy\": 1.0, \"loss\": 0.006663323380053043, \"time-step\": 1970}, {\"accuracy\": 1.0, \"loss\": 0.0065024616196751595, \"time-step\": 1971}, {\"accuracy\": 1.0, \"loss\": 0.006641479674726725, \"time-step\": 1972}, {\"accuracy\": 1.0, \"loss\": 0.00648213317617774, \"time-step\": 1973}, {\"accuracy\": 1.0, \"loss\": 0.006619680672883987, \"time-step\": 1974}, {\"accuracy\": 1.0, \"loss\": 0.006461861077696085, \"time-step\": 1975}, {\"accuracy\": 1.0, \"loss\": 0.006597989704459906, \"time-step\": 1976}, {\"accuracy\": 1.0, \"loss\": 0.006441700272262096, \"time-step\": 1977}, {\"accuracy\": 1.0, \"loss\": 0.006576379761099815, \"time-step\": 1978}, {\"accuracy\": 1.0, \"loss\": 0.006421588361263275, \"time-step\": 1979}, {\"accuracy\": 1.0, \"loss\": 0.0065548657439649105, \"time-step\": 1980}, {\"accuracy\": 1.0, \"loss\": 0.006401606369763613, \"time-step\": 1981}, {\"accuracy\": 1.0, \"loss\": 0.006533449981361628, \"time-step\": 1982}, {\"accuracy\": 1.0, \"loss\": 0.006381634157150984, \"time-step\": 1983}, {\"accuracy\": 1.0, \"loss\": 0.006512125954031944, \"time-step\": 1984}, {\"accuracy\": 1.0, \"loss\": 0.006361796520650387, \"time-step\": 1985}, {\"accuracy\": 1.0, \"loss\": 0.006490867584943771, \"time-step\": 1986}, {\"accuracy\": 1.0, \"loss\": 0.006342017091810703, \"time-step\": 1987}, {\"accuracy\": 1.0, \"loss\": 0.006469763815402985, \"time-step\": 1988}, {\"accuracy\": 1.0, \"loss\": 0.006322366185486317, \"time-step\": 1989}, {\"accuracy\": 1.0, \"loss\": 0.0064487699419260025, \"time-step\": 1990}, {\"accuracy\": 1.0, \"loss\": 0.006302817724645138, \"time-step\": 1991}, {\"accuracy\": 1.0, \"loss\": 0.006427873857319355, \"time-step\": 1992}, {\"accuracy\": 1.0, \"loss\": 0.006283348426222801, \"time-step\": 1993}, {\"accuracy\": 1.0, \"loss\": 0.006407079752534628, \"time-step\": 1994}, {\"accuracy\": 1.0, \"loss\": 0.006263946183025837, \"time-step\": 1995}, {\"accuracy\": 1.0, \"loss\": 0.006386335473507643, \"time-step\": 1996}, {\"accuracy\": 1.0, \"loss\": 0.006244594696909189, \"time-step\": 1997}, {\"accuracy\": 1.0, \"loss\": 0.006365708075463772, \"time-step\": 1998}, {\"accuracy\": 1.0, \"loss\": 0.006225310266017914, \"time-step\": 1999}, {\"accuracy\": 1.0, \"loss\": 0.006345120258629322, \"time-step\": 2000}, {\"accuracy\": 1.0, \"loss\": 0.006206111516803503, \"time-step\": 2001}, {\"accuracy\": 1.0, \"loss\": 0.006324637681245804, \"time-step\": 2002}, {\"accuracy\": 1.0, \"loss\": 0.006186976563185453, \"time-step\": 2003}, {\"accuracy\": 1.0, \"loss\": 0.0063042351976037025, \"time-step\": 2004}, {\"accuracy\": 1.0, \"loss\": 0.006167899817228317, \"time-step\": 2005}, {\"accuracy\": 1.0, \"loss\": 0.006283904891461134, \"time-step\": 2006}, {\"accuracy\": 1.0, \"loss\": 0.006148975342512131, \"time-step\": 2007}, {\"accuracy\": 1.0, \"loss\": 0.006263714283704758, \"time-step\": 2008}, {\"accuracy\": 1.0, \"loss\": 0.006130051799118519, \"time-step\": 2009}, {\"accuracy\": 1.0, \"loss\": 0.00624358095228672, \"time-step\": 2010}, {\"accuracy\": 1.0, \"loss\": 0.006111229304224253, \"time-step\": 2011}, {\"accuracy\": 1.0, \"loss\": 0.006223530508577824, \"time-step\": 2012}, {\"accuracy\": 1.0, \"loss\": 0.0060924869030714035, \"time-step\": 2013}, {\"accuracy\": 1.0, \"loss\": 0.006203577388077974, \"time-step\": 2014}, {\"accuracy\": 1.0, \"loss\": 0.006073847878724337, \"time-step\": 2015}, {\"accuracy\": 1.0, \"loss\": 0.006183759309351444, \"time-step\": 2016}, {\"accuracy\": 1.0, \"loss\": 0.006055300123989582, \"time-step\": 2017}, {\"accuracy\": 1.0, \"loss\": 0.006163998506963253, \"time-step\": 2018}, {\"accuracy\": 1.0, \"loss\": 0.006036762613803148, \"time-step\": 2019}, {\"accuracy\": 1.0, \"loss\": 0.006144313141703606, \"time-step\": 2020}, {\"accuracy\": 1.0, \"loss\": 0.006018369924277067, \"time-step\": 2021}, {\"accuracy\": 1.0, \"loss\": 0.006124740932136774, \"time-step\": 2022}, {\"accuracy\": 1.0, \"loss\": 0.006000017747282982, \"time-step\": 2023}, {\"accuracy\": 1.0, \"loss\": 0.0061052106320858, \"time-step\": 2024}, {\"accuracy\": 1.0, \"loss\": 0.005981756839901209, \"time-step\": 2025}, {\"accuracy\": 1.0, \"loss\": 0.006085831206291914, \"time-step\": 2026}, {\"accuracy\": 1.0, \"loss\": 0.005963563919067383, \"time-step\": 2027}, {\"accuracy\": 1.0, \"loss\": 0.00606645829975605, \"time-step\": 2028}, {\"accuracy\": 1.0, \"loss\": 0.0059454115107655525, \"time-step\": 2029}, {\"accuracy\": 1.0, \"loss\": 0.006047168746590614, \"time-step\": 2030}, {\"accuracy\": 1.0, \"loss\": 0.005927293561398983, \"time-step\": 2031}, {\"accuracy\": 1.0, \"loss\": 0.006027943920344114, \"time-step\": 2032}, {\"accuracy\": 1.0, \"loss\": 0.005909244064241648, \"time-step\": 2033}, {\"accuracy\": 1.0, \"loss\": 0.0060088071040809155, \"time-step\": 2034}, {\"accuracy\": 1.0, \"loss\": 0.005891352891921997, \"time-step\": 2035}, {\"accuracy\": 1.0, \"loss\": 0.005989814642816782, \"time-step\": 2036}, {\"accuracy\": 1.0, \"loss\": 0.005873472895473242, \"time-step\": 2037}, {\"accuracy\": 1.0, \"loss\": 0.005970857106149197, \"time-step\": 2038}, {\"accuracy\": 1.0, \"loss\": 0.0058557577431201935, \"time-step\": 2039}, {\"accuracy\": 1.0, \"loss\": 0.005952070467174053, \"time-step\": 2040}, {\"accuracy\": 1.0, \"loss\": 0.005838069599121809, \"time-step\": 2041}, {\"accuracy\": 1.0, \"loss\": 0.005933303851634264, \"time-step\": 2042}, {\"accuracy\": 1.0, \"loss\": 0.005820471793413162, \"time-step\": 2043}, {\"accuracy\": 1.0, \"loss\": 0.005914666224271059, \"time-step\": 2044}, {\"accuracy\": 1.0, \"loss\": 0.005802904721349478, \"time-step\": 2045}, {\"accuracy\": 1.0, \"loss\": 0.005896029062569141, \"time-step\": 2046}, {\"accuracy\": 1.0, \"loss\": 0.005785360001027584, \"time-step\": 2047}, {\"accuracy\": 1.0, \"loss\": 0.005877463612705469, \"time-step\": 2048}, {\"accuracy\": 1.0, \"loss\": 0.005767961964011192, \"time-step\": 2049}, {\"accuracy\": 1.0, \"loss\": 0.00585902389138937, \"time-step\": 2050}, {\"accuracy\": 1.0, \"loss\": 0.005750603973865509, \"time-step\": 2051}, {\"accuracy\": 1.0, \"loss\": 0.005840699188411236, \"time-step\": 2052}, {\"accuracy\": 1.0, \"loss\": 0.005733385682106018, \"time-step\": 2053}, {\"accuracy\": 1.0, \"loss\": 0.005822456907480955, \"time-step\": 2054}, {\"accuracy\": 1.0, \"loss\": 0.005716205574572086, \"time-step\": 2055}, {\"accuracy\": 1.0, \"loss\": 0.005804232321679592, \"time-step\": 2056}, {\"accuracy\": 1.0, \"loss\": 0.0056990706361830235, \"time-step\": 2057}, {\"accuracy\": 1.0, \"loss\": 0.005786116700619459, \"time-step\": 2058}, {\"accuracy\": 1.0, \"loss\": 0.005682001356035471, \"time-step\": 2059}, {\"accuracy\": 1.0, \"loss\": 0.005768083035945892, \"time-step\": 2060}, {\"accuracy\": 1.0, \"loss\": 0.005665022414177656, \"time-step\": 2061}, {\"accuracy\": 1.0, \"loss\": 0.005750128533691168, \"time-step\": 2062}, {\"accuracy\": 1.0, \"loss\": 0.005648073274642229, \"time-step\": 2063}, {\"accuracy\": 1.0, \"loss\": 0.005732204765081406, \"time-step\": 2064}, {\"accuracy\": 1.0, \"loss\": 0.005631194915622473, \"time-step\": 2065}, {\"accuracy\": 1.0, \"loss\": 0.005714393220841885, \"time-step\": 2066}, {\"accuracy\": 1.0, \"loss\": 0.005614462774246931, \"time-step\": 2067}, {\"accuracy\": 1.0, \"loss\": 0.005696702748537064, \"time-step\": 2068}, {\"accuracy\": 1.0, \"loss\": 0.005597766023129225, \"time-step\": 2069}, {\"accuracy\": 1.0, \"loss\": 0.005679083988070488, \"time-step\": 2070}, {\"accuracy\": 1.0, \"loss\": 0.005581140052527189, \"time-step\": 2071}, {\"accuracy\": 1.0, \"loss\": 0.00566151412203908, \"time-step\": 2072}, {\"accuracy\": 1.0, \"loss\": 0.005564522929489613, \"time-step\": 2073}, {\"accuracy\": 1.0, \"loss\": 0.005643955431878567, \"time-step\": 2074}, {\"accuracy\": 1.0, \"loss\": 0.005547946318984032, \"time-step\": 2075}, {\"accuracy\": 1.0, \"loss\": 0.005626491736620665, \"time-step\": 2076}, {\"accuracy\": 1.0, \"loss\": 0.005531522445380688, \"time-step\": 2077}, {\"accuracy\": 1.0, \"loss\": 0.0056091658771038055, \"time-step\": 2078}, {\"accuracy\": 1.0, \"loss\": 0.005515153054147959, \"time-step\": 2079}, {\"accuracy\": 1.0, \"loss\": 0.005591896362602711, \"time-step\": 2080}, {\"accuracy\": 1.0, \"loss\": 0.005498834419995546, \"time-step\": 2081}, {\"accuracy\": 1.0, \"loss\": 0.005574685521423817, \"time-step\": 2082}, {\"accuracy\": 1.0, \"loss\": 0.005482593551278114, \"time-step\": 2083}, {\"accuracy\": 1.0, \"loss\": 0.005557587835937738, \"time-step\": 2084}, {\"accuracy\": 1.0, \"loss\": 0.005466413218528032, \"time-step\": 2085}, {\"accuracy\": 1.0, \"loss\": 0.005540487356483936, \"time-step\": 2086}, {\"accuracy\": 1.0, \"loss\": 0.005450247786939144, \"time-step\": 2087}, {\"accuracy\": 1.0, \"loss\": 0.005523479077965021, \"time-step\": 2088}, {\"accuracy\": 1.0, \"loss\": 0.005434185266494751, \"time-step\": 2089}, {\"accuracy\": 1.0, \"loss\": 0.005506571382284164, \"time-step\": 2090}, {\"accuracy\": 1.0, \"loss\": 0.0054181586019694805, \"time-step\": 2091}, {\"accuracy\": 1.0, \"loss\": 0.005489706061780453, \"time-step\": 2092}, {\"accuracy\": 1.0, \"loss\": 0.005402262322604656, \"time-step\": 2093}, {\"accuracy\": 1.0, \"loss\": 0.005472949240356684, \"time-step\": 2094}, {\"accuracy\": 1.0, \"loss\": 0.005386370234191418, \"time-step\": 2095}, {\"accuracy\": 1.0, \"loss\": 0.00545622268691659, \"time-step\": 2096}, {\"accuracy\": 1.0, \"loss\": 0.005370555445551872, \"time-step\": 2097}, {\"accuracy\": 1.0, \"loss\": 0.005439596250653267, \"time-step\": 2098}, {\"accuracy\": 1.0, \"loss\": 0.00535479886457324, \"time-step\": 2099}, {\"accuracy\": 1.0, \"loss\": 0.005423033144325018, \"time-step\": 2100}, {\"accuracy\": 1.0, \"loss\": 0.005339138209819794, \"time-step\": 2101}, {\"accuracy\": 1.0, \"loss\": 0.005406518932431936, \"time-step\": 2102}, {\"accuracy\": 1.0, \"loss\": 0.005323498509824276, \"time-step\": 2103}, {\"accuracy\": 1.0, \"loss\": 0.005390121601521969, \"time-step\": 2104}, {\"accuracy\": 1.0, \"loss\": 0.005307971965521574, \"time-step\": 2105}, {\"accuracy\": 1.0, \"loss\": 0.005373809486627579, \"time-step\": 2106}, {\"accuracy\": 1.0, \"loss\": 0.00529249245300889, \"time-step\": 2107}, {\"accuracy\": 1.0, \"loss\": 0.005357454065233469, \"time-step\": 2108}, {\"accuracy\": 1.0, \"loss\": 0.005277012474834919, \"time-step\": 2109}, {\"accuracy\": 1.0, \"loss\": 0.0053412653505802155, \"time-step\": 2110}, {\"accuracy\": 1.0, \"loss\": 0.005261670332401991, \"time-step\": 2111}, {\"accuracy\": 1.0, \"loss\": 0.005325144622474909, \"time-step\": 2112}, {\"accuracy\": 1.0, \"loss\": 0.005246362648904324, \"time-step\": 2113}, {\"accuracy\": 1.0, \"loss\": 0.005309063941240311, \"time-step\": 2114}, {\"accuracy\": 1.0, \"loss\": 0.005231109447777271, \"time-step\": 2115}, {\"accuracy\": 1.0, \"loss\": 0.0052930391393601894, \"time-step\": 2116}, {\"accuracy\": 1.0, \"loss\": 0.005215931683778763, \"time-step\": 2117}, {\"accuracy\": 1.0, \"loss\": 0.005277110263705254, \"time-step\": 2118}, {\"accuracy\": 1.0, \"loss\": 0.005200807936489582, \"time-step\": 2119}, {\"accuracy\": 1.0, \"loss\": 0.005261257756501436, \"time-step\": 2120}, {\"accuracy\": 1.0, \"loss\": 0.005185727030038834, \"time-step\": 2121}, {\"accuracy\": 1.0, \"loss\": 0.005245448090136051, \"time-step\": 2122}, {\"accuracy\": 1.0, \"loss\": 0.00517073180526495, \"time-step\": 2123}, {\"accuracy\": 1.0, \"loss\": 0.005229711998254061, \"time-step\": 2124}, {\"accuracy\": 1.0, \"loss\": 0.005155797116458416, \"time-step\": 2125}, {\"accuracy\": 1.0, \"loss\": 0.005214042961597443, \"time-step\": 2126}, {\"accuracy\": 1.0, \"loss\": 0.0051409173756837845, \"time-step\": 2127}, {\"accuracy\": 1.0, \"loss\": 0.005198458209633827, \"time-step\": 2128}, {\"accuracy\": 1.0, \"loss\": 0.005126094911247492, \"time-step\": 2129}, {\"accuracy\": 1.0, \"loss\": 0.005182893015444279, \"time-step\": 2130}, {\"accuracy\": 1.0, \"loss\": 0.005111325066536665, \"time-step\": 2131}, {\"accuracy\": 1.0, \"loss\": 0.0051674311980605125, \"time-step\": 2132}, {\"accuracy\": 1.0, \"loss\": 0.0050966073758900166, \"time-step\": 2133}, {\"accuracy\": 1.0, \"loss\": 0.0051519847474992275, \"time-step\": 2134}, {\"accuracy\": 1.0, \"loss\": 0.005081892944872379, \"time-step\": 2135}, {\"accuracy\": 1.0, \"loss\": 0.005136582069098949, \"time-step\": 2136}, {\"accuracy\": 1.0, \"loss\": 0.005067277234047651, \"time-step\": 2137}, {\"accuracy\": 1.0, \"loss\": 0.005121320486068726, \"time-step\": 2138}, {\"accuracy\": 1.0, \"loss\": 0.005052743013948202, \"time-step\": 2139}, {\"accuracy\": 1.0, \"loss\": 0.0051060826517641544, \"time-step\": 2140}, {\"accuracy\": 1.0, \"loss\": 0.005038246978074312, \"time-step\": 2141}, {\"accuracy\": 1.0, \"loss\": 0.005090933293104172, \"time-step\": 2142}, {\"accuracy\": 1.0, \"loss\": 0.005023860838264227, \"time-step\": 2143}, {\"accuracy\": 1.0, \"loss\": 0.0050758542492985725, \"time-step\": 2144}, {\"accuracy\": 1.0, \"loss\": 0.005009484943002462, \"time-step\": 2145}, {\"accuracy\": 1.0, \"loss\": 0.005060823634266853, \"time-step\": 2146}, {\"accuracy\": 1.0, \"loss\": 0.0049951449036598206, \"time-step\": 2147}, {\"accuracy\": 1.0, \"loss\": 0.0050458298064768314, \"time-step\": 2148}, {\"accuracy\": 1.0, \"loss\": 0.004980916623026133, \"time-step\": 2149}, {\"accuracy\": 1.0, \"loss\": 0.00503094308078289, \"time-step\": 2150}, {\"accuracy\": 1.0, \"loss\": 0.004966725595295429, \"time-step\": 2151}, {\"accuracy\": 1.0, \"loss\": 0.0050161173567175865, \"time-step\": 2152}, {\"accuracy\": 1.0, \"loss\": 0.004952599760144949, \"time-step\": 2153}, {\"accuracy\": 1.0, \"loss\": 0.005001355893909931, \"time-step\": 2154}, {\"accuracy\": 1.0, \"loss\": 0.004938539583235979, \"time-step\": 2155}, {\"accuracy\": 1.0, \"loss\": 0.004986652173101902, \"time-step\": 2156}, {\"accuracy\": 1.0, \"loss\": 0.0049245283007621765, \"time-step\": 2157}, {\"accuracy\": 1.0, \"loss\": 0.004972022958099842, \"time-step\": 2158}, {\"accuracy\": 1.0, \"loss\": 0.0049105542711913586, \"time-step\": 2159}, {\"accuracy\": 1.0, \"loss\": 0.004957425408065319, \"time-step\": 2160}, {\"accuracy\": 1.0, \"loss\": 0.004896638449281454, \"time-step\": 2161}, {\"accuracy\": 1.0, \"loss\": 0.00494290329515934, \"time-step\": 2162}, {\"accuracy\": 1.0, \"loss\": 0.0048827603459358215, \"time-step\": 2163}, {\"accuracy\": 1.0, \"loss\": 0.004928414709866047, \"time-step\": 2164}, {\"accuracy\": 1.0, \"loss\": 0.00486897025257349, \"time-step\": 2165}, {\"accuracy\": 1.0, \"loss\": 0.004913998302072287, \"time-step\": 2166}, {\"accuracy\": 1.0, \"loss\": 0.004855198785662651, \"time-step\": 2167}, {\"accuracy\": 1.0, \"loss\": 0.004899626597762108, \"time-step\": 2168}, {\"accuracy\": 1.0, \"loss\": 0.0048414864577353, \"time-step\": 2169}, {\"accuracy\": 1.0, \"loss\": 0.004885321483016014, \"time-step\": 2170}, {\"accuracy\": 1.0, \"loss\": 0.0048278020694851875, \"time-step\": 2171}, {\"accuracy\": 1.0, \"loss\": 0.004871054086834192, \"time-step\": 2172}, {\"accuracy\": 1.0, \"loss\": 0.004814174026250839, \"time-step\": 2173}, {\"accuracy\": 1.0, \"loss\": 0.0048568397760391235, \"time-step\": 2174}, {\"accuracy\": 1.0, \"loss\": 0.0048006330616772175, \"time-step\": 2175}, {\"accuracy\": 1.0, \"loss\": 0.004842719063162804, \"time-step\": 2176}, {\"accuracy\": 1.0, \"loss\": 0.0047871265560388565, \"time-step\": 2177}, {\"accuracy\": 1.0, \"loss\": 0.00482866819947958, \"time-step\": 2178}, {\"accuracy\": 1.0, \"loss\": 0.004773750901222229, \"time-step\": 2179}, {\"accuracy\": 1.0, \"loss\": 0.004814718384295702, \"time-step\": 2180}, {\"accuracy\": 1.0, \"loss\": 0.0047603752464056015, \"time-step\": 2181}, {\"accuracy\": 1.0, \"loss\": 0.004800783004611731, \"time-step\": 2182}, {\"accuracy\": 1.0, \"loss\": 0.0047470517456531525, \"time-step\": 2183}, {\"accuracy\": 1.0, \"loss\": 0.004786856006830931, \"time-step\": 2184}, {\"accuracy\": 1.0, \"loss\": 0.004733739420771599, \"time-step\": 2185}, {\"accuracy\": 1.0, \"loss\": 0.004773052409291267, \"time-step\": 2186}, {\"accuracy\": 1.0, \"loss\": 0.0047205230221152306, \"time-step\": 2187}, {\"accuracy\": 1.0, \"loss\": 0.004759244155138731, \"time-step\": 2188}, {\"accuracy\": 1.0, \"loss\": 0.004707373678684235, \"time-step\": 2189}, {\"accuracy\": 1.0, \"loss\": 0.004745589103549719, \"time-step\": 2190}, {\"accuracy\": 1.0, \"loss\": 0.004694262519478798, \"time-step\": 2191}, {\"accuracy\": 1.0, \"loss\": 0.004731937311589718, \"time-step\": 2192}, {\"accuracy\": 1.0, \"loss\": 0.004681185353547335, \"time-step\": 2193}, {\"accuracy\": 1.0, \"loss\": 0.00471834558993578, \"time-step\": 2194}, {\"accuracy\": 1.0, \"loss\": 0.004668162204325199, \"time-step\": 2195}, {\"accuracy\": 1.0, \"loss\": 0.0047047752887010574, \"time-step\": 2196}, {\"accuracy\": 1.0, \"loss\": 0.004655197262763977, \"time-step\": 2197}, {\"accuracy\": 1.0, \"loss\": 0.004691267851740122, \"time-step\": 2198}, {\"accuracy\": 1.0, \"loss\": 0.004642252344638109, \"time-step\": 2199}, {\"accuracy\": 1.0, \"loss\": 0.004677834454923868, \"time-step\": 2200}, {\"accuracy\": 1.0, \"loss\": 0.004629385657608509, \"time-step\": 2201}, {\"accuracy\": 1.0, \"loss\": 0.004664449021220207, \"time-step\": 2202}, {\"accuracy\": 1.0, \"loss\": 0.0046165501698851585, \"time-step\": 2203}, {\"accuracy\": 1.0, \"loss\": 0.004651093855500221, \"time-step\": 2204}, {\"accuracy\": 1.0, \"loss\": 0.004603771027177572, \"time-step\": 2205}, {\"accuracy\": 1.0, \"loss\": 0.004637759644538164, \"time-step\": 2206}, {\"accuracy\": 1.0, \"loss\": 0.004590979311615229, \"time-step\": 2207}, {\"accuracy\": 1.0, \"loss\": 0.004624516237527132, \"time-step\": 2208}, {\"accuracy\": 1.0, \"loss\": 0.004578280728310347, \"time-step\": 2209}, {\"accuracy\": 1.0, \"loss\": 0.004611330572515726, \"time-step\": 2210}, {\"accuracy\": 1.0, \"loss\": 0.004565650597214699, \"time-step\": 2211}, {\"accuracy\": 1.0, \"loss\": 0.004598222207278013, \"time-step\": 2212}, {\"accuracy\": 1.0, \"loss\": 0.00455308984965086, \"time-step\": 2213}, {\"accuracy\": 1.0, \"loss\": 0.004585167858749628, \"time-step\": 2214}, {\"accuracy\": 1.0, \"loss\": 0.004540553316473961, \"time-step\": 2215}, {\"accuracy\": 1.0, \"loss\": 0.004572155885398388, \"time-step\": 2216}, {\"accuracy\": 1.0, \"loss\": 0.00452808290719986, \"time-step\": 2217}, {\"accuracy\": 1.0, \"loss\": 0.0045592146925628185, \"time-step\": 2218}, {\"accuracy\": 1.0, \"loss\": 0.0045156339183449745, \"time-step\": 2219}, {\"accuracy\": 1.0, \"loss\": 0.004546281415969133, \"time-step\": 2220}, {\"accuracy\": 1.0, \"loss\": 0.0045032501220703125, \"time-step\": 2221}, {\"accuracy\": 1.0, \"loss\": 0.004533437080681324, \"time-step\": 2222}, {\"accuracy\": 1.0, \"loss\": 0.004490940365940332, \"time-step\": 2223}, {\"accuracy\": 1.0, \"loss\": 0.00452068354934454, \"time-step\": 2224}, {\"accuracy\": 1.0, \"loss\": 0.004478659480810165, \"time-step\": 2225}, {\"accuracy\": 1.0, \"loss\": 0.004507926758378744, \"time-step\": 2226}, {\"accuracy\": 1.0, \"loss\": 0.004466412588953972, \"time-step\": 2227}, {\"accuracy\": 1.0, \"loss\": 0.004495246335864067, \"time-step\": 2228}, {\"accuracy\": 1.0, \"loss\": 0.004454245790839195, \"time-step\": 2229}, {\"accuracy\": 1.0, \"loss\": 0.004482615273445845, \"time-step\": 2230}, {\"accuracy\": 1.0, \"loss\": 0.0044420743361115456, \"time-step\": 2231}, {\"accuracy\": 1.0, \"loss\": 0.004469984211027622, \"time-step\": 2232}, {\"accuracy\": 1.0, \"loss\": 0.004429957363754511, \"time-step\": 2233}, {\"accuracy\": 1.0, \"loss\": 0.004457393195480108, \"time-step\": 2234}, {\"accuracy\": 1.0, \"loss\": 0.0044178650714457035, \"time-step\": 2235}, {\"accuracy\": 1.0, \"loss\": 0.0044448962435126305, \"time-step\": 2236}, {\"accuracy\": 1.0, \"loss\": 0.0044058579951524734, \"time-step\": 2237}, {\"accuracy\": 1.0, \"loss\": 0.00443245517089963, \"time-step\": 2238}, {\"accuracy\": 1.0, \"loss\": 0.004393878858536482, \"time-step\": 2239}, {\"accuracy\": 1.0, \"loss\": 0.004420044831931591, \"time-step\": 2240}, {\"accuracy\": 1.0, \"loss\": 0.004381946288049221, \"time-step\": 2241}, {\"accuracy\": 1.0, \"loss\": 0.0044076950289309025, \"time-step\": 2242}, {\"accuracy\": 1.0, \"loss\": 0.004370079841464758, \"time-step\": 2243}, {\"accuracy\": 1.0, \"loss\": 0.004395387135446072, \"time-step\": 2244}, {\"accuracy\": 1.0, \"loss\": 0.004358219914138317, \"time-step\": 2245}, {\"accuracy\": 1.0, \"loss\": 0.004383107181638479, \"time-step\": 2246}, {\"accuracy\": 1.0, \"loss\": 0.004346431698650122, \"time-step\": 2247}, {\"accuracy\": 1.0, \"loss\": 0.004370903130620718, \"time-step\": 2248}, {\"accuracy\": 1.0, \"loss\": 0.004334662109613419, \"time-step\": 2249}, {\"accuracy\": 1.0, \"loss\": 0.004358734469860792, \"time-step\": 2250}, {\"accuracy\": 1.0, \"loss\": 0.004322986584156752, \"time-step\": 2251}, {\"accuracy\": 1.0, \"loss\": 0.004346692468971014, \"time-step\": 2252}, {\"accuracy\": 1.0, \"loss\": 0.004311359021812677, \"time-step\": 2253}, {\"accuracy\": 1.0, \"loss\": 0.00433461694046855, \"time-step\": 2254}, {\"accuracy\": 1.0, \"loss\": 0.004299727734178305, \"time-step\": 2255}, {\"accuracy\": 1.0, \"loss\": 0.004322563298046589, \"time-step\": 2256}, {\"accuracy\": 1.0, \"loss\": 0.004288112744688988, \"time-step\": 2257}, {\"accuracy\": 1.0, \"loss\": 0.004310606047511101, \"time-step\": 2258}, {\"accuracy\": 1.0, \"loss\": 0.004276589956134558, \"time-step\": 2259}, {\"accuracy\": 1.0, \"loss\": 0.00429869070649147, \"time-step\": 2260}, {\"accuracy\": 1.0, \"loss\": 0.004265138879418373, \"time-step\": 2261}, {\"accuracy\": 1.0, \"loss\": 0.004286793991923332, \"time-step\": 2262}, {\"accuracy\": 1.0, \"loss\": 0.00425363564863801, \"time-step\": 2263}, {\"accuracy\": 1.0, \"loss\": 0.004274945706129074, \"time-step\": 2264}, {\"accuracy\": 1.0, \"loss\": 0.00424223905429244, \"time-step\": 2265}, {\"accuracy\": 1.0, \"loss\": 0.004263159818947315, \"time-step\": 2266}, {\"accuracy\": 1.0, \"loss\": 0.004230894614011049, \"time-step\": 2267}, {\"accuracy\": 1.0, \"loss\": 0.004251427948474884, \"time-step\": 2268}, {\"accuracy\": 1.0, \"loss\": 0.004219566937536001, \"time-step\": 2269}, {\"accuracy\": 1.0, \"loss\": 0.0042397817596793175, \"time-step\": 2270}, {\"accuracy\": 1.0, \"loss\": 0.004208365920931101, \"time-step\": 2271}, {\"accuracy\": 1.0, \"loss\": 0.004228188190609217, \"time-step\": 2272}, {\"accuracy\": 1.0, \"loss\": 0.0041971649043262005, \"time-step\": 2273}, {\"accuracy\": 1.0, \"loss\": 0.004216591361910105, \"time-step\": 2274}, {\"accuracy\": 1.0, \"loss\": 0.004185977391898632, \"time-step\": 2275}, {\"accuracy\": 1.0, \"loss\": 0.00420505553483963, \"time-step\": 2276}, {\"accuracy\": 1.0, \"loss\": 0.004174846690148115, \"time-step\": 2277}, {\"accuracy\": 1.0, \"loss\": 0.00419356394559145, \"time-step\": 2278}, {\"accuracy\": 1.0, \"loss\": 0.00416375370696187, \"time-step\": 2279}, {\"accuracy\": 1.0, \"loss\": 0.004182079806923866, \"time-step\": 2280}, {\"accuracy\": 1.0, \"loss\": 0.00415265467017889, \"time-step\": 2281}, {\"accuracy\": 1.0, \"loss\": 0.004170677158981562, \"time-step\": 2282}, {\"accuracy\": 1.0, \"loss\": 0.004141639452427626, \"time-step\": 2283}, {\"accuracy\": 1.0, \"loss\": 0.004159281961619854, \"time-step\": 2284}, {\"accuracy\": 1.0, \"loss\": 0.0041306233033537865, \"time-step\": 2285}, {\"accuracy\": 1.0, \"loss\": 0.004147905856370926, \"time-step\": 2286}, {\"accuracy\": 1.0, \"loss\": 0.004119659774005413, \"time-step\": 2287}, {\"accuracy\": 1.0, \"loss\": 0.004136630333960056, \"time-step\": 2288}, {\"accuracy\": 1.0, \"loss\": 0.004108768422156572, \"time-step\": 2289}, {\"accuracy\": 1.0, \"loss\": 0.004125392064452171, \"time-step\": 2290}, {\"accuracy\": 1.0, \"loss\": 0.004097946919500828, \"time-step\": 2291}, {\"accuracy\": 1.0, \"loss\": 0.004114230629056692, \"time-step\": 2292}, {\"accuracy\": 1.0, \"loss\": 0.004087093286216259, \"time-step\": 2293}, {\"accuracy\": 1.0, \"loss\": 0.00410302123054862, \"time-step\": 2294}, {\"accuracy\": 1.0, \"loss\": 0.004076266661286354, \"time-step\": 2295}, {\"accuracy\": 1.0, \"loss\": 0.0040918514132499695, \"time-step\": 2296}, {\"accuracy\": 1.0, \"loss\": 0.004065486136823893, \"time-step\": 2297}, {\"accuracy\": 1.0, \"loss\": 0.004080779850482941, \"time-step\": 2298}, {\"accuracy\": 1.0, \"loss\": 0.00405478710308671, \"time-step\": 2299}, {\"accuracy\": 1.0, \"loss\": 0.004069735761731863, \"time-step\": 2300}, {\"accuracy\": 1.0, \"loss\": 0.0040441351011395454, \"time-step\": 2301}, {\"accuracy\": 1.0, \"loss\": 0.004058788064867258, \"time-step\": 2302}, {\"accuracy\": 1.0, \"loss\": 0.004033524077385664, \"time-step\": 2303}, {\"accuracy\": 1.0, \"loss\": 0.0040478273294866085, \"time-step\": 2304}, {\"accuracy\": 1.0, \"loss\": 0.004022910725325346, \"time-step\": 2305}, {\"accuracy\": 1.0, \"loss\": 0.004036933183670044, \"time-step\": 2306}, {\"accuracy\": 1.0, \"loss\": 0.004012379329651594, \"time-step\": 2307}, {\"accuracy\": 1.0, \"loss\": 0.004026089329272509, \"time-step\": 2308}, {\"accuracy\": 1.0, \"loss\": 0.004001888446509838, \"time-step\": 2309}, {\"accuracy\": 1.0, \"loss\": 0.0040152231231331825, \"time-step\": 2310}, {\"accuracy\": 1.0, \"loss\": 0.003991383593529463, \"time-step\": 2311}, {\"accuracy\": 1.0, \"loss\": 0.004004483576864004, \"time-step\": 2312}, {\"accuracy\": 1.0, \"loss\": 0.003980968147516251, \"time-step\": 2313}, {\"accuracy\": 1.0, \"loss\": 0.003993726335465908, \"time-step\": 2314}, {\"accuracy\": 1.0, \"loss\": 0.003970569930970669, \"time-step\": 2315}, {\"accuracy\": 1.0, \"loss\": 0.0039830394089221954, \"time-step\": 2316}, {\"accuracy\": 1.0, \"loss\": 0.003960207104682922, \"time-step\": 2317}, {\"accuracy\": 1.0, \"loss\": 0.003972351551055908, \"time-step\": 2318}, {\"accuracy\": 1.0, \"loss\": 0.003949877806007862, \"time-step\": 2319}, {\"accuracy\": 1.0, \"loss\": 0.003961741924285889, \"time-step\": 2320}, {\"accuracy\": 1.0, \"loss\": 0.003939573187381029, \"time-step\": 2321}, {\"accuracy\": 1.0, \"loss\": 0.003951141145080328, \"time-step\": 2322}, {\"accuracy\": 1.0, \"loss\": 0.00392930768430233, \"time-step\": 2323}, {\"accuracy\": 1.0, \"loss\": 0.003940606489777565, \"time-step\": 2324}, {\"accuracy\": 1.0, \"loss\": 0.003919119015336037, \"time-step\": 2325}, {\"accuracy\": 1.0, \"loss\": 0.003930141683667898, \"time-step\": 2326}, {\"accuracy\": 1.0, \"loss\": 0.0039089578203856945, \"time-step\": 2327}, {\"accuracy\": 1.0, \"loss\": 0.003919678740203381, \"time-step\": 2328}, {\"accuracy\": 1.0, \"loss\": 0.0038988403975963593, \"time-step\": 2329}, {\"accuracy\": 1.0, \"loss\": 0.003909297753125429, \"time-step\": 2330}, {\"accuracy\": 1.0, \"loss\": 0.0038887308910489082, \"time-step\": 2331}, {\"accuracy\": 1.0, \"loss\": 0.0038988797459751368, \"time-step\": 2332}, {\"accuracy\": 1.0, \"loss\": 0.0038786304648965597, \"time-step\": 2333}, {\"accuracy\": 1.0, \"loss\": 0.0038884789682924747, \"time-step\": 2334}, {\"accuracy\": 1.0, \"loss\": 0.003868542145937681, \"time-step\": 2335}, {\"accuracy\": 1.0, \"loss\": 0.0038781235925853252, \"time-step\": 2336}, {\"accuracy\": 1.0, \"loss\": 0.0038585131987929344, \"time-step\": 2337}, {\"accuracy\": 1.0, \"loss\": 0.003867814550176263, \"time-step\": 2338}, {\"accuracy\": 1.0, \"loss\": 0.0038485098630189896, \"time-step\": 2339}, {\"accuracy\": 1.0, \"loss\": 0.003857589792460203, \"time-step\": 2340}, {\"accuracy\": 1.0, \"loss\": 0.0038385905791074038, \"time-step\": 2341}, {\"accuracy\": 1.0, \"loss\": 0.003847368760034442, \"time-step\": 2342}, {\"accuracy\": 1.0, \"loss\": 0.0038286822382360697, \"time-step\": 2343}, {\"accuracy\": 1.0, \"loss\": 0.0038372152484953403, \"time-step\": 2344}, {\"accuracy\": 1.0, \"loss\": 0.0038188081234693527, \"time-step\": 2345}, {\"accuracy\": 1.0, \"loss\": 0.0038270624354481697, \"time-step\": 2346}, {\"accuracy\": 1.0, \"loss\": 0.003808970097452402, \"time-step\": 2347}, {\"accuracy\": 1.0, \"loss\": 0.003816966200247407, \"time-step\": 2348}, {\"accuracy\": 1.0, \"loss\": 0.003799146506935358, \"time-step\": 2349}, {\"accuracy\": 1.0, \"loss\": 0.0038068995345383883, \"time-step\": 2350}, {\"accuracy\": 1.0, \"loss\": 0.003789417678490281, \"time-step\": 2351}, {\"accuracy\": 1.0, \"loss\": 0.0037969232071191072, \"time-step\": 2352}, {\"accuracy\": 1.0, \"loss\": 0.003779694205150008, \"time-step\": 2353}, {\"accuracy\": 1.0, \"loss\": 0.0037869377993047237, \"time-step\": 2354}, {\"accuracy\": 1.0, \"loss\": 0.0037699751555919647, \"time-step\": 2355}, {\"accuracy\": 1.0, \"loss\": 0.0037769810296595097, \"time-step\": 2356}, {\"accuracy\": 1.0, \"loss\": 0.0037603178061544895, \"time-step\": 2357}, {\"accuracy\": 1.0, \"loss\": 0.003767054993659258, \"time-step\": 2358}, {\"accuracy\": 1.0, \"loss\": 0.003750663483515382, \"time-step\": 2359}, {\"accuracy\": 1.0, \"loss\": 0.0037571671418845654, \"time-step\": 2360}, {\"accuracy\": 1.0, \"loss\": 0.0037410841323435307, \"time-step\": 2361}, {\"accuracy\": 1.0, \"loss\": 0.0037473468109965324, \"time-step\": 2362}, {\"accuracy\": 1.0, \"loss\": 0.003731498261913657, \"time-step\": 2363}, {\"accuracy\": 1.0, \"loss\": 0.0037375143729150295, \"time-step\": 2364}, {\"accuracy\": 1.0, \"loss\": 0.0037219710648059845, \"time-step\": 2365}, {\"accuracy\": 1.0, \"loss\": 0.0037277517840266228, \"time-step\": 2366}, {\"accuracy\": 1.0, \"loss\": 0.0037124562077224255, \"time-step\": 2367}, {\"accuracy\": 1.0, \"loss\": 0.003718032967299223, \"time-step\": 2368}, {\"accuracy\": 1.0, \"loss\": 0.0037029925733804703, \"time-step\": 2369}, {\"accuracy\": 1.0, \"loss\": 0.003708341158926487, \"time-step\": 2370}, {\"accuracy\": 1.0, \"loss\": 0.0036935494281351566, \"time-step\": 2371}, {\"accuracy\": 1.0, \"loss\": 0.0036986302584409714, \"time-step\": 2372}, {\"accuracy\": 1.0, \"loss\": 0.003684095572680235, \"time-step\": 2373}, {\"accuracy\": 1.0, \"loss\": 0.0036889463663101196, \"time-step\": 2374}, {\"accuracy\": 1.0, \"loss\": 0.003674685023725033, \"time-step\": 2375}, {\"accuracy\": 1.0, \"loss\": 0.00367930019274354, \"time-step\": 2376}, {\"accuracy\": 1.0, \"loss\": 0.003665281692519784, \"time-step\": 2377}, {\"accuracy\": 1.0, \"loss\": 0.003669699653983116, \"time-step\": 2378}, {\"accuracy\": 1.0, \"loss\": 0.0036559617146849632, \"time-step\": 2379}, {\"accuracy\": 1.0, \"loss\": 0.003660169430077076, \"time-step\": 2380}, {\"accuracy\": 1.0, \"loss\": 0.0036466645542532206, \"time-step\": 2381}, {\"accuracy\": 1.0, \"loss\": 0.0036506380420178175, \"time-step\": 2382}, {\"accuracy\": 1.0, \"loss\": 0.003637386951595545, \"time-step\": 2383}, {\"accuracy\": 1.0, \"loss\": 0.0036411467008292675, \"time-step\": 2384}, {\"accuracy\": 1.0, \"loss\": 0.00362817058339715, \"time-step\": 2385}, {\"accuracy\": 1.0, \"loss\": 0.003631698666140437, \"time-step\": 2386}, {\"accuracy\": 1.0, \"loss\": 0.0036189304664731026, \"time-step\": 2387}, {\"accuracy\": 1.0, \"loss\": 0.003622243879362941, \"time-step\": 2388}, {\"accuracy\": 1.0, \"loss\": 0.003609702456742525, \"time-step\": 2389}, {\"accuracy\": 1.0, \"loss\": 0.0036128084175288677, \"time-step\": 2390}, {\"accuracy\": 1.0, \"loss\": 0.0036005349829792976, \"time-step\": 2391}, {\"accuracy\": 1.0, \"loss\": 0.0036034637596458197, \"time-step\": 2392}, {\"accuracy\": 1.0, \"loss\": 0.003591405227780342, \"time-step\": 2393}, {\"accuracy\": 1.0, \"loss\": 0.003594097448512912, \"time-step\": 2394}, {\"accuracy\": 1.0, \"loss\": 0.003582307603210211, \"time-step\": 2395}, {\"accuracy\": 1.0, \"loss\": 0.003584827994927764, \"time-step\": 2396}, {\"accuracy\": 1.0, \"loss\": 0.003573280991986394, \"time-step\": 2397}, {\"accuracy\": 1.0, \"loss\": 0.0035755906719714403, \"time-step\": 2398}, {\"accuracy\": 1.0, \"loss\": 0.003564247628673911, \"time-step\": 2399}, {\"accuracy\": 1.0, \"loss\": 0.0035663479939103127, \"time-step\": 2400}, {\"accuracy\": 1.0, \"loss\": 0.0035551812034100294, \"time-step\": 2401}, {\"accuracy\": 1.0, \"loss\": 0.0035570813342928886, \"time-step\": 2402}, {\"accuracy\": 1.0, \"loss\": 0.003546204185113311, \"time-step\": 2403}, {\"accuracy\": 1.0, \"loss\": 0.0035478821955621243, \"time-step\": 2404}, {\"accuracy\": 1.0, \"loss\": 0.003537161508575082, \"time-step\": 2405}, {\"accuracy\": 1.0, \"loss\": 0.0035386879462748766, \"time-step\": 2406}, {\"accuracy\": 1.0, \"loss\": 0.0035282380413264036, \"time-step\": 2407}, {\"accuracy\": 1.0, \"loss\": 0.003529590554535389, \"time-step\": 2408}, {\"accuracy\": 1.0, \"loss\": 0.0035193185321986675, \"time-step\": 2409}, {\"accuracy\": 1.0, \"loss\": 0.003520464990288019, \"time-step\": 2410}, {\"accuracy\": 1.0, \"loss\": 0.003510409966111183, \"time-step\": 2411}, {\"accuracy\": 1.0, \"loss\": 0.0035113911144435406, \"time-step\": 2412}, {\"accuracy\": 1.0, \"loss\": 0.0035015419125556946, \"time-step\": 2413}, {\"accuracy\": 1.0, \"loss\": 0.003502295818179846, \"time-step\": 2414}, {\"accuracy\": 1.0, \"loss\": 0.003492655698210001, \"time-step\": 2415}, {\"accuracy\": 1.0, \"loss\": 0.0034932619892060757, \"time-step\": 2416}, {\"accuracy\": 1.0, \"loss\": 0.0034838642459362745, \"time-step\": 2417}, {\"accuracy\": 1.0, \"loss\": 0.003484271001070738, \"time-step\": 2418}, {\"accuracy\": 1.0, \"loss\": 0.0034750388003885746, \"time-step\": 2419}, {\"accuracy\": 1.0, \"loss\": 0.00347528257407248, \"time-step\": 2420}, {\"accuracy\": 1.0, \"loss\": 0.0034662873949855566, \"time-step\": 2421}, {\"accuracy\": 1.0, \"loss\": 0.003466350259259343, \"time-step\": 2422}, {\"accuracy\": 1.0, \"loss\": 0.0034575590398162603, \"time-step\": 2423}, {\"accuracy\": 1.0, \"loss\": 0.003457456361502409, \"time-step\": 2424}, {\"accuracy\": 1.0, \"loss\": 0.003448845585808158, \"time-step\": 2425}, {\"accuracy\": 1.0, \"loss\": 0.0034485631622374058, \"time-step\": 2426}, {\"accuracy\": 1.0, \"loss\": 0.00344009418040514, \"time-step\": 2427}, {\"accuracy\": 1.0, \"loss\": 0.0034396543633192778, \"time-step\": 2428}, {\"accuracy\": 1.0, \"loss\": 0.003431442892178893, \"time-step\": 2429}, {\"accuracy\": 1.0, \"loss\": 0.003430891316384077, \"time-step\": 2430}, {\"accuracy\": 1.0, \"loss\": 0.003422841429710388, \"time-step\": 2431}, {\"accuracy\": 1.0, \"loss\": 0.003422064008191228, \"time-step\": 2432}, {\"accuracy\": 1.0, \"loss\": 0.0034142020158469677, \"time-step\": 2433}, {\"accuracy\": 1.0, \"loss\": 0.0034132699947804213, \"time-step\": 2434}, {\"accuracy\": 1.0, \"loss\": 0.0034056054428219795, \"time-step\": 2435}, {\"accuracy\": 1.0, \"loss\": 0.0034044990316033363, \"time-step\": 2436}, {\"accuracy\": 1.0, \"loss\": 0.0033970128279179335, \"time-step\": 2437}, {\"accuracy\": 1.0, \"loss\": 0.0033957839477807283, \"time-step\": 2438}, {\"accuracy\": 1.0, \"loss\": 0.003388485638424754, \"time-step\": 2439}, {\"accuracy\": 1.0, \"loss\": 0.003387089353054762, \"time-step\": 2440}, {\"accuracy\": 1.0, \"loss\": 0.0033799505326896906, \"time-step\": 2441}, {\"accuracy\": 1.0, \"loss\": 0.003378399880602956, \"time-step\": 2442}, {\"accuracy\": 1.0, \"loss\": 0.003371421480551362, \"time-step\": 2443}, {\"accuracy\": 1.0, \"loss\": 0.0033697374165058136, \"time-step\": 2444}, {\"accuracy\": 1.0, \"loss\": 0.0033629557583481073, \"time-step\": 2445}, {\"accuracy\": 1.0, \"loss\": 0.0033611226826906204, \"time-step\": 2446}, {\"accuracy\": 1.0, \"loss\": 0.0033544888719916344, \"time-step\": 2447}, {\"accuracy\": 1.0, \"loss\": 0.003352539148181677, \"time-step\": 2448}, {\"accuracy\": 1.0, \"loss\": 0.003346112323924899, \"time-step\": 2449}, {\"accuracy\": 1.0, \"loss\": 0.003344035940244794, \"time-step\": 2450}, {\"accuracy\": 1.0, \"loss\": 0.0033377737272530794, \"time-step\": 2451}, {\"accuracy\": 1.0, \"loss\": 0.0033355168998241425, \"time-step\": 2452}, {\"accuracy\": 1.0, \"loss\": 0.0033294097520411015, \"time-step\": 2453}, {\"accuracy\": 1.0, \"loss\": 0.0033270670101046562, \"time-step\": 2454}, {\"accuracy\": 1.0, \"loss\": 0.003321101889014244, \"time-step\": 2455}, {\"accuracy\": 1.0, \"loss\": 0.0033185970969498158, \"time-step\": 2456}, {\"accuracy\": 1.0, \"loss\": 0.0033128135837614536, \"time-step\": 2457}, {\"accuracy\": 1.0, \"loss\": 0.0033102035522460938, \"time-step\": 2458}, {\"accuracy\": 1.0, \"loss\": 0.0033045883756130934, \"time-step\": 2459}, {\"accuracy\": 1.0, \"loss\": 0.0033018947578966618, \"time-step\": 2460}, {\"accuracy\": 1.0, \"loss\": 0.003296440467238426, \"time-step\": 2461}, {\"accuracy\": 1.0, \"loss\": 0.0032935969065874815, \"time-step\": 2462}, {\"accuracy\": 1.0, \"loss\": 0.003288313979282975, \"time-step\": 2463}, {\"accuracy\": 1.0, \"loss\": 0.0032853714656084776, \"time-step\": 2464}, {\"accuracy\": 1.0, \"loss\": 0.003280248725786805, \"time-step\": 2465}, {\"accuracy\": 1.0, \"loss\": 0.00327717955224216, \"time-step\": 2466}, {\"accuracy\": 1.0, \"loss\": 0.0032722316682338715, \"time-step\": 2467}, {\"accuracy\": 1.0, \"loss\": 0.0032690633088350296, \"time-step\": 2468}, {\"accuracy\": 1.0, \"loss\": 0.0032642812002450228, \"time-step\": 2469}, {\"accuracy\": 1.0, \"loss\": 0.0032610187772661448, \"time-step\": 2470}, {\"accuracy\": 1.0, \"loss\": 0.003256391966715455, \"time-step\": 2471}, {\"accuracy\": 1.0, \"loss\": 0.003252999857068062, \"time-step\": 2472}, {\"accuracy\": 1.0, \"loss\": 0.003248540684580803, \"time-step\": 2473}, {\"accuracy\": 1.0, \"loss\": 0.003245045430958271, \"time-step\": 2474}, {\"accuracy\": 1.0, \"loss\": 0.0032407541293650866, \"time-step\": 2475}, {\"accuracy\": 1.0, \"loss\": 0.0032371939159929752, \"time-step\": 2476}, {\"accuracy\": 1.0, \"loss\": 0.003233032999560237, \"time-step\": 2477}, {\"accuracy\": 1.0, \"loss\": 0.003229366149753332, \"time-step\": 2478}, {\"accuracy\": 1.0, \"loss\": 0.0032253731042146683, \"time-step\": 2479}, {\"accuracy\": 1.0, \"loss\": 0.003221612423658371, \"time-step\": 2480}, {\"accuracy\": 1.0, \"loss\": 0.003217805176973343, \"time-step\": 2481}, {\"accuracy\": 1.0, \"loss\": 0.003213941352441907, \"time-step\": 2482}, {\"accuracy\": 1.0, \"loss\": 0.003210279159247875, \"time-step\": 2483}, {\"accuracy\": 1.0, \"loss\": 0.0032063063699752092, \"time-step\": 2484}, {\"accuracy\": 1.0, \"loss\": 0.0032028069254010916, \"time-step\": 2485}, {\"accuracy\": 1.0, \"loss\": 0.003198751248419285, \"time-step\": 2486}, {\"accuracy\": 1.0, \"loss\": 0.0031953901052474976, \"time-step\": 2487}, {\"accuracy\": 1.0, \"loss\": 0.003191236173734069, \"time-step\": 2488}, {\"accuracy\": 1.0, \"loss\": 0.0031879935413599014, \"time-step\": 2489}, {\"accuracy\": 1.0, \"loss\": 0.0031837308779358864, \"time-step\": 2490}, {\"accuracy\": 1.0, \"loss\": 0.0031806686893105507, \"time-step\": 2491}, {\"accuracy\": 1.0, \"loss\": 0.0031763436272740364, \"time-step\": 2492}, {\"accuracy\": 1.0, \"loss\": 0.003173429286107421, \"time-step\": 2493}, {\"accuracy\": 1.0, \"loss\": 0.0031689973548054695, \"time-step\": 2494}, {\"accuracy\": 1.0, \"loss\": 0.0031662210822105408, \"time-step\": 2495}, {\"accuracy\": 1.0, \"loss\": 0.003161713248118758, \"time-step\": 2496}, {\"accuracy\": 1.0, \"loss\": 0.003159087151288986, \"time-step\": 2497}, {\"accuracy\": 1.0, \"loss\": 0.0031544314697384834, \"time-step\": 2498}, {\"accuracy\": 1.0, \"loss\": 0.0031519487965852022, \"time-step\": 2499}, {\"accuracy\": 1.0, \"loss\": 0.003147243056446314, \"time-step\": 2500}, {\"accuracy\": 1.0, \"loss\": 0.003144897986203432, \"time-step\": 2501}, {\"accuracy\": 1.0, \"loss\": 0.0031400849111378193, \"time-step\": 2502}, {\"accuracy\": 1.0, \"loss\": 0.003137868596240878, \"time-step\": 2503}, {\"accuracy\": 1.0, \"loss\": 0.003132941434159875, \"time-step\": 2504}, {\"accuracy\": 1.0, \"loss\": 0.003130868077278137, \"time-step\": 2505}, {\"accuracy\": 1.0, \"loss\": 0.0031258398666977882, \"time-step\": 2506}, {\"accuracy\": 1.0, \"loss\": 0.0031238640658557415, \"time-step\": 2507}, {\"accuracy\": 1.0, \"loss\": 0.003118740161880851, \"time-step\": 2508}, {\"accuracy\": 1.0, \"loss\": 0.003116914536803961, \"time-step\": 2509}, {\"accuracy\": 1.0, \"loss\": 0.003111722646281123, \"time-step\": 2510}, {\"accuracy\": 1.0, \"loss\": 0.00311002298258245, \"time-step\": 2511}, {\"accuracy\": 1.0, \"loss\": 0.0031047258526086807, \"time-step\": 2512}, {\"accuracy\": 1.0, \"loss\": 0.003103183815255761, \"time-step\": 2513}, {\"accuracy\": 1.0, \"loss\": 0.003097774926573038, \"time-step\": 2514}, {\"accuracy\": 1.0, \"loss\": 0.003096325322985649, \"time-step\": 2515}, {\"accuracy\": 1.0, \"loss\": 0.003090800018981099, \"time-step\": 2516}, {\"accuracy\": 1.0, \"loss\": 0.003089462872594595, \"time-step\": 2517}, {\"accuracy\": 1.0, \"loss\": 0.0030838805250823498, \"time-step\": 2518}, {\"accuracy\": 1.0, \"loss\": 0.003082692390307784, \"time-step\": 2519}, {\"accuracy\": 1.0, \"loss\": 0.003077024593949318, \"time-step\": 2520}, {\"accuracy\": 1.0, \"loss\": 0.0030759612563997507, \"time-step\": 2521}, {\"accuracy\": 1.0, \"loss\": 0.0030701530631631613, \"time-step\": 2522}, {\"accuracy\": 1.0, \"loss\": 0.0030691740103065968, \"time-step\": 2523}, {\"accuracy\": 1.0, \"loss\": 0.0030632754787802696, \"time-step\": 2524}, {\"accuracy\": 1.0, \"loss\": 0.00306245987303555, \"time-step\": 2525}, {\"accuracy\": 1.0, \"loss\": 0.003056492656469345, \"time-step\": 2526}, {\"accuracy\": 1.0, \"loss\": 0.0030557741411030293, \"time-step\": 2527}, {\"accuracy\": 1.0, \"loss\": 0.0030496963299810886, \"time-step\": 2528}, {\"accuracy\": 1.0, \"loss\": 0.003049073973670602, \"time-step\": 2529}, {\"accuracy\": 1.0, \"loss\": 0.0030429151374846697, \"time-step\": 2530}, {\"accuracy\": 1.0, \"loss\": 0.0030424196738749743, \"time-step\": 2531}, {\"accuracy\": 1.0, \"loss\": 0.0030361642129719257, \"time-step\": 2532}, {\"accuracy\": 1.0, \"loss\": 0.0030357895884662867, \"time-step\": 2533}, {\"accuracy\": 1.0, \"loss\": 0.0030294423922896385, \"time-step\": 2534}, {\"accuracy\": 1.0, \"loss\": 0.0030291832517832518, \"time-step\": 2535}, {\"accuracy\": 1.0, \"loss\": 0.0030227454844862223, \"time-step\": 2536}, {\"accuracy\": 1.0, \"loss\": 0.0030226141680032015, \"time-step\": 2537}, {\"accuracy\": 1.0, \"loss\": 0.003016085596755147, \"time-step\": 2538}, {\"accuracy\": 1.0, \"loss\": 0.0030160353053361177, \"time-step\": 2539}, {\"accuracy\": 1.0, \"loss\": 0.0030094124376773834, \"time-step\": 2540}, {\"accuracy\": 1.0, \"loss\": 0.0030094680842012167, \"time-step\": 2541}, {\"accuracy\": 1.0, \"loss\": 0.0030027474276721478, \"time-step\": 2542}, {\"accuracy\": 1.0, \"loss\": 0.003002940444275737, \"time-step\": 2543}, {\"accuracy\": 1.0, \"loss\": 0.002996144350618124, \"time-step\": 2544}, {\"accuracy\": 1.0, \"loss\": 0.0029964670538902283, \"time-step\": 2545}, {\"accuracy\": 1.0, \"loss\": 0.0029895741026848555, \"time-step\": 2546}, {\"accuracy\": 1.0, \"loss\": 0.0029899962246418, \"time-step\": 2547}, {\"accuracy\": 1.0, \"loss\": 0.002983053447678685, \"time-step\": 2548}, {\"accuracy\": 1.0, \"loss\": 0.0029835570603609085, \"time-step\": 2549}, {\"accuracy\": 1.0, \"loss\": 0.0029765439685434103, \"time-step\": 2550}, {\"accuracy\": 1.0, \"loss\": 0.002977171912789345, \"time-step\": 2551}, {\"accuracy\": 1.0, \"loss\": 0.002970006549730897, \"time-step\": 2552}, {\"accuracy\": 1.0, \"loss\": 0.002970711560919881, \"time-step\": 2553}, {\"accuracy\": 1.0, \"loss\": 0.0029634935781359673, \"time-step\": 2554}, {\"accuracy\": 1.0, \"loss\": 0.002964335260912776, \"time-step\": 2555}, {\"accuracy\": 1.0, \"loss\": 0.002957048127427697, \"time-step\": 2556}, {\"accuracy\": 1.0, \"loss\": 0.002957974560558796, \"time-step\": 2557}, {\"accuracy\": 1.0, \"loss\": 0.0029505854472517967, \"time-step\": 2558}, {\"accuracy\": 1.0, \"loss\": 0.0029515819624066353, \"time-step\": 2559}, {\"accuracy\": 1.0, \"loss\": 0.0029441057704389095, \"time-step\": 2560}, {\"accuracy\": 1.0, \"loss\": 0.0029452359303832054, \"time-step\": 2561}, {\"accuracy\": 1.0, \"loss\": 0.0029377087485045195, \"time-step\": 2562}, {\"accuracy\": 1.0, \"loss\": 0.0029389429837465286, \"time-step\": 2563}, {\"accuracy\": 1.0, \"loss\": 0.002931330818682909, \"time-step\": 2564}, {\"accuracy\": 1.0, \"loss\": 0.002932673553004861, \"time-step\": 2565}, {\"accuracy\": 1.0, \"loss\": 0.0029249624349176884, \"time-step\": 2566}, {\"accuracy\": 1.0, \"loss\": 0.002926390618085861, \"time-step\": 2567}, {\"accuracy\": 1.0, \"loss\": 0.0029186150059103966, \"time-step\": 2568}, {\"accuracy\": 1.0, \"loss\": 0.002920137718319893, \"time-step\": 2569}, {\"accuracy\": 1.0, \"loss\": 0.0029122689738869667, \"time-step\": 2570}, {\"accuracy\": 1.0, \"loss\": 0.0029138843528926373, \"time-step\": 2571}, {\"accuracy\": 1.0, \"loss\": 0.002905944362282753, \"time-step\": 2572}, {\"accuracy\": 1.0, \"loss\": 0.0029076894279569387, \"time-step\": 2573}, {\"accuracy\": 1.0, \"loss\": 0.002899679820984602, \"time-step\": 2574}, {\"accuracy\": 1.0, \"loss\": 0.002901468425989151, \"time-step\": 2575}, {\"accuracy\": 1.0, \"loss\": 0.002893387572839856, \"time-step\": 2576}, {\"accuracy\": 1.0, \"loss\": 0.0028953026048839092, \"time-step\": 2577}, {\"accuracy\": 1.0, \"loss\": 0.0028871544636785984, \"time-step\": 2578}, {\"accuracy\": 1.0, \"loss\": 0.002889165887609124, \"time-step\": 2579}, {\"accuracy\": 1.0, \"loss\": 0.0028809551149606705, \"time-step\": 2580}, {\"accuracy\": 1.0, \"loss\": 0.0028830391820520163, \"time-step\": 2581}, {\"accuracy\": 1.0, \"loss\": 0.002874712459743023, \"time-step\": 2582}, {\"accuracy\": 1.0, \"loss\": 0.002876891987398267, \"time-step\": 2583}, {\"accuracy\": 1.0, \"loss\": 0.0028685168363153934, \"time-step\": 2584}, {\"accuracy\": 1.0, \"loss\": 0.0028708046302199364, \"time-step\": 2585}, {\"accuracy\": 1.0, \"loss\": 0.0028623640537261963, \"time-step\": 2586}, {\"accuracy\": 1.0, \"loss\": 0.0028647282160818577, \"time-step\": 2587}, {\"accuracy\": 1.0, \"loss\": 0.002856220118701458, \"time-step\": 2588}, {\"accuracy\": 1.0, \"loss\": 0.0028587111737579107, \"time-step\": 2589}, {\"accuracy\": 1.0, \"loss\": 0.002850132528692484, \"time-step\": 2590}, {\"accuracy\": 1.0, \"loss\": 0.0028527299873530865, \"time-step\": 2591}, {\"accuracy\": 1.0, \"loss\": 0.00284411758184433, \"time-step\": 2592}, {\"accuracy\": 1.0, \"loss\": 0.0028467969968914986, \"time-step\": 2593}, {\"accuracy\": 1.0, \"loss\": 0.002838080981746316, \"time-step\": 2594}, {\"accuracy\": 1.0, \"loss\": 0.0028408162761479616, \"time-step\": 2595}, {\"accuracy\": 1.0, \"loss\": 0.0028320166748017073, \"time-step\": 2596}, {\"accuracy\": 1.0, \"loss\": 0.0028348425403237343, \"time-step\": 2597}, {\"accuracy\": 1.0, \"loss\": 0.0028259954415261745, \"time-step\": 2598}, {\"accuracy\": 1.0, \"loss\": 0.0028289291076362133, \"time-step\": 2599}, {\"accuracy\": 1.0, \"loss\": 0.002820006338879466, \"time-step\": 2600}, {\"accuracy\": 1.0, \"loss\": 0.0028229833114892244, \"time-step\": 2601}, {\"accuracy\": 1.0, \"loss\": 0.0028139720670878887, \"time-step\": 2602}, {\"accuracy\": 1.0, \"loss\": 0.0028170219156891108, \"time-step\": 2603}, {\"accuracy\": 1.0, \"loss\": 0.002807960147038102, \"time-step\": 2604}, {\"accuracy\": 1.0, \"loss\": 0.0028111147694289684, \"time-step\": 2605}, {\"accuracy\": 1.0, \"loss\": 0.002801977563649416, \"time-step\": 2606}, {\"accuracy\": 1.0, \"loss\": 0.0028052187990397215, \"time-step\": 2607}, {\"accuracy\": 1.0, \"loss\": 0.002796051325276494, \"time-step\": 2608}, {\"accuracy\": 1.0, \"loss\": 0.002799385692924261, \"time-step\": 2609}, {\"accuracy\": 1.0, \"loss\": 0.0027901241555809975, \"time-step\": 2610}, {\"accuracy\": 1.0, \"loss\": 0.0027935646940022707, \"time-step\": 2611}, {\"accuracy\": 1.0, \"loss\": 0.0027842805720865726, \"time-step\": 2612}, {\"accuracy\": 1.0, \"loss\": 0.002787795150652528, \"time-step\": 2613}, {\"accuracy\": 1.0, \"loss\": 0.0027784372214227915, \"time-step\": 2614}, {\"accuracy\": 1.0, \"loss\": 0.0027820419054478407, \"time-step\": 2615}, {\"accuracy\": 1.0, \"loss\": 0.002772608771920204, \"time-step\": 2616}, {\"accuracy\": 1.0, \"loss\": 0.002776257460936904, \"time-step\": 2617}, {\"accuracy\": 1.0, \"loss\": 0.0027667616959661245, \"time-step\": 2618}, {\"accuracy\": 1.0, \"loss\": 0.002770512131974101, \"time-step\": 2619}, {\"accuracy\": 1.0, \"loss\": 0.0027609493117779493, \"time-step\": 2620}, {\"accuracy\": 1.0, \"loss\": 0.002764791017398238, \"time-step\": 2621}, {\"accuracy\": 1.0, \"loss\": 0.002755179535597563, \"time-step\": 2622}, {\"accuracy\": 1.0, \"loss\": 0.0027590792160481215, \"time-step\": 2623}, {\"accuracy\": 1.0, \"loss\": 0.002749413251876831, \"time-step\": 2624}, {\"accuracy\": 1.0, \"loss\": 0.0027534067630767822, \"time-step\": 2625}, {\"accuracy\": 1.0, \"loss\": 0.0027436716482043266, \"time-step\": 2626}, {\"accuracy\": 1.0, \"loss\": 0.0027477561961859465, \"time-step\": 2627}, {\"accuracy\": 1.0, \"loss\": 0.002737964503467083, \"time-step\": 2628}, {\"accuracy\": 1.0, \"loss\": 0.0027421119157224894, \"time-step\": 2629}, {\"accuracy\": 1.0, \"loss\": 0.0027322652749717236, \"time-step\": 2630}, {\"accuracy\": 1.0, \"loss\": 0.002736467868089676, \"time-step\": 2631}, {\"accuracy\": 1.0, \"loss\": 0.0027265651151537895, \"time-step\": 2632}, {\"accuracy\": 1.0, \"loss\": 0.0027308575809001923, \"time-step\": 2633}, {\"accuracy\": 1.0, \"loss\": 0.00272089964710176, \"time-step\": 2634}, {\"accuracy\": 1.0, \"loss\": 0.002725271973758936, \"time-step\": 2635}, {\"accuracy\": 1.0, \"loss\": 0.0027152695693075657, \"time-step\": 2636}, {\"accuracy\": 1.0, \"loss\": 0.00271970103494823, \"time-step\": 2637}, {\"accuracy\": 1.0, \"loss\": 0.002709611551836133, \"time-step\": 2638}, {\"accuracy\": 1.0, \"loss\": 0.00271410821005702, \"time-step\": 2639}, {\"accuracy\": 1.0, \"loss\": 0.00270399684086442, \"time-step\": 2640}, {\"accuracy\": 1.0, \"loss\": 0.002708598505705595, \"time-step\": 2641}, {\"accuracy\": 1.0, \"loss\": 0.002698424505069852, \"time-step\": 2642}, {\"accuracy\": 1.0, \"loss\": 0.002703102072700858, \"time-step\": 2643}, {\"accuracy\": 1.0, \"loss\": 0.002692870330065489, \"time-step\": 2644}, {\"accuracy\": 1.0, \"loss\": 0.0026976163499057293, \"time-step\": 2645}, {\"accuracy\": 1.0, \"loss\": 0.0026873392052948475, \"time-step\": 2646}, {\"accuracy\": 1.0, \"loss\": 0.002692130161449313, \"time-step\": 2647}, {\"accuracy\": 1.0, \"loss\": 0.002681792713701725, \"time-step\": 2648}, {\"accuracy\": 1.0, \"loss\": 0.0026866821572184563, \"time-step\": 2649}, {\"accuracy\": 1.0, \"loss\": 0.0026762892957776785, \"time-step\": 2650}, {\"accuracy\": 1.0, \"loss\": 0.002681228332221508, \"time-step\": 2651}, {\"accuracy\": 1.0, \"loss\": 0.0026707923971116543, \"time-step\": 2652}, {\"accuracy\": 1.0, \"loss\": 0.002675787080079317, \"time-step\": 2653}, {\"accuracy\": 1.0, \"loss\": 0.0026652682572603226, \"time-step\": 2654}, {\"accuracy\": 1.0, \"loss\": 0.0026703353505581617, \"time-step\": 2655}, {\"accuracy\": 1.0, \"loss\": 0.0026598067488521338, \"time-step\": 2656}, {\"accuracy\": 1.0, \"loss\": 0.0026649765204638243, \"time-step\": 2657}, {\"accuracy\": 1.0, \"loss\": 0.002654371317476034, \"time-step\": 2658}, {\"accuracy\": 1.0, \"loss\": 0.0026595669332891703, \"time-step\": 2659}, {\"accuracy\": 1.0, \"loss\": 0.0026489305309951305, \"time-step\": 2660}, {\"accuracy\": 1.0, \"loss\": 0.0026542090345174074, \"time-step\": 2661}, {\"accuracy\": 1.0, \"loss\": 0.002643533516675234, \"time-step\": 2662}, {\"accuracy\": 1.0, \"loss\": 0.0026488848961889744, \"time-step\": 2663}, {\"accuracy\": 1.0, \"loss\": 0.002638156060129404, \"time-step\": 2664}, {\"accuracy\": 1.0, \"loss\": 0.0026435512118041515, \"time-step\": 2665}, {\"accuracy\": 1.0, \"loss\": 0.002632784191519022, \"time-step\": 2666}, {\"accuracy\": 1.0, \"loss\": 0.002638254314661026, \"time-step\": 2667}, {\"accuracy\": 1.0, \"loss\": 0.0026274321135133505, \"time-step\": 2668}, {\"accuracy\": 1.0, \"loss\": 0.0026329767424613237, \"time-step\": 2669}, {\"accuracy\": 1.0, \"loss\": 0.0026221037842333317, \"time-step\": 2670}, {\"accuracy\": 1.0, \"loss\": 0.0026277038268744946, \"time-step\": 2671}, {\"accuracy\": 1.0, \"loss\": 0.00261678546667099, \"time-step\": 2672}, {\"accuracy\": 1.0, \"loss\": 0.002622464206069708, \"time-step\": 2673}, {\"accuracy\": 1.0, \"loss\": 0.0026114950887858868, \"time-step\": 2674}, {\"accuracy\": 1.0, \"loss\": 0.0026172276120632887, \"time-step\": 2675}, {\"accuracy\": 1.0, \"loss\": 0.0026062210090458393, \"time-step\": 2676}, {\"accuracy\": 1.0, \"loss\": 0.002612013602629304, \"time-step\": 2677}, {\"accuracy\": 1.0, \"loss\": 0.0026009632274508476, \"time-step\": 2678}, {\"accuracy\": 1.0, \"loss\": 0.0026068075094372034, \"time-step\": 2679}, {\"accuracy\": 1.0, \"loss\": 0.0025956938043236732, \"time-step\": 2680}, {\"accuracy\": 1.0, \"loss\": 0.0026016116607934237, \"time-step\": 2681}, {\"accuracy\": 1.0, \"loss\": 0.002590468619018793, \"time-step\": 2682}, {\"accuracy\": 1.0, \"loss\": 0.0025964502710849047, \"time-step\": 2683}, {\"accuracy\": 1.0, \"loss\": 0.0025852511171251535, \"time-step\": 2684}, {\"accuracy\": 1.0, \"loss\": 0.002591286087408662, \"time-step\": 2685}, {\"accuracy\": 1.0, \"loss\": 0.002580077853053808, \"time-step\": 2686}, {\"accuracy\": 1.0, \"loss\": 0.002586185932159424, \"time-step\": 2687}, {\"accuracy\": 1.0, \"loss\": 0.0025749343913048506, \"time-step\": 2688}, {\"accuracy\": 1.0, \"loss\": 0.002581121399998665, \"time-step\": 2689}, {\"accuracy\": 1.0, \"loss\": 0.0025697967503219843, \"time-step\": 2690}, {\"accuracy\": 1.0, \"loss\": 0.0025760007556527853, \"time-step\": 2691}, {\"accuracy\": 1.0, \"loss\": 0.002564632333815098, \"time-step\": 2692}, {\"accuracy\": 1.0, \"loss\": 0.0025709220208227634, \"time-step\": 2693}, {\"accuracy\": 1.0, \"loss\": 0.0025595128536224365, \"time-step\": 2694}, {\"accuracy\": 1.0, \"loss\": 0.0025658365339040756, \"time-step\": 2695}, {\"accuracy\": 1.0, \"loss\": 0.002554391510784626, \"time-step\": 2696}, {\"accuracy\": 1.0, \"loss\": 0.002560768276453018, \"time-step\": 2697}, {\"accuracy\": 1.0, \"loss\": 0.002549287397414446, \"time-step\": 2698}, {\"accuracy\": 1.0, \"loss\": 0.002555716782808304, \"time-step\": 2699}, {\"accuracy\": 1.0, \"loss\": 0.002544195158407092, \"time-step\": 2700}, {\"accuracy\": 1.0, \"loss\": 0.0025507016107439995, \"time-step\": 2701}, {\"accuracy\": 1.0, \"loss\": 0.0025391385424882174, \"time-step\": 2702}, {\"accuracy\": 1.0, \"loss\": 0.0025457185693085194, \"time-step\": 2703}, {\"accuracy\": 1.0, \"loss\": 0.0025341457221657038, \"time-step\": 2704}, {\"accuracy\": 1.0, \"loss\": 0.002540756016969681, \"time-step\": 2705}, {\"accuracy\": 1.0, \"loss\": 0.0025291331112384796, \"time-step\": 2706}, {\"accuracy\": 1.0, \"loss\": 0.0025358162820339203, \"time-step\": 2707}, {\"accuracy\": 1.0, \"loss\": 0.002524150302633643, \"time-step\": 2708}, {\"accuracy\": 1.0, \"loss\": 0.0025308928452432156, \"time-step\": 2709}, {\"accuracy\": 1.0, \"loss\": 0.0025192105676978827, \"time-step\": 2710}, {\"accuracy\": 1.0, \"loss\": 0.0025259966496378183, \"time-step\": 2711}, {\"accuracy\": 1.0, \"loss\": 0.0025142456870526075, \"time-step\": 2712}, {\"accuracy\": 1.0, \"loss\": 0.002521062269806862, \"time-step\": 2713}, {\"accuracy\": 1.0, \"loss\": 0.002509255660697818, \"time-step\": 2714}, {\"accuracy\": 1.0, \"loss\": 0.0025161351077258587, \"time-step\": 2715}, {\"accuracy\": 1.0, \"loss\": 0.0025043173227459192, \"time-step\": 2716}, {\"accuracy\": 1.0, \"loss\": 0.0025112612638622522, \"time-step\": 2717}, {\"accuracy\": 1.0, \"loss\": 0.0024994106497615576, \"time-step\": 2718}, {\"accuracy\": 1.0, \"loss\": 0.002506389981135726, \"time-step\": 2719}, {\"accuracy\": 1.0, \"loss\": 0.0024945067707449198, \"time-step\": 2720}, {\"accuracy\": 1.0, \"loss\": 0.002501542679965496, \"time-step\": 2721}, {\"accuracy\": 1.0, \"loss\": 0.002489596838131547, \"time-step\": 2722}, {\"accuracy\": 1.0, \"loss\": 0.0024966790806502104, \"time-step\": 2723}, {\"accuracy\": 1.0, \"loss\": 0.0024847236927598715, \"time-step\": 2724}, {\"accuracy\": 1.0, \"loss\": 0.0024918578565120697, \"time-step\": 2725}, {\"accuracy\": 1.0, \"loss\": 0.002479858696460724, \"time-step\": 2726}, {\"accuracy\": 1.0, \"loss\": 0.0024870517663657665, \"time-step\": 2727}, {\"accuracy\": 1.0, \"loss\": 0.0024750265292823315, \"time-step\": 2728}, {\"accuracy\": 1.0, \"loss\": 0.002482285024598241, \"time-step\": 2729}, {\"accuracy\": 1.0, \"loss\": 0.002470224630087614, \"time-step\": 2730}, {\"accuracy\": 1.0, \"loss\": 0.002477518515661359, \"time-step\": 2731}, {\"accuracy\": 1.0, \"loss\": 0.0024654266890138388, \"time-step\": 2732}, {\"accuracy\": 1.0, \"loss\": 0.0024727554991841316, \"time-step\": 2733}, {\"accuracy\": 1.0, \"loss\": 0.0024606038350611925, \"time-step\": 2734}, {\"accuracy\": 1.0, \"loss\": 0.0024679885245859623, \"time-step\": 2735}, {\"accuracy\": 1.0, \"loss\": 0.0024558145087212324, \"time-step\": 2736}, {\"accuracy\": 1.0, \"loss\": 0.00246324110776186, \"time-step\": 2737}, {\"accuracy\": 1.0, \"loss\": 0.0024510552175343037, \"time-step\": 2738}, {\"accuracy\": 1.0, \"loss\": 0.002458535134792328, \"time-step\": 2739}, {\"accuracy\": 1.0, \"loss\": 0.002446312690153718, \"time-step\": 2740}, {\"accuracy\": 1.0, \"loss\": 0.002453843131661415, \"time-step\": 2741}, {\"accuracy\": 1.0, \"loss\": 0.0024415794759988785, \"time-step\": 2742}, {\"accuracy\": 1.0, \"loss\": 0.002449165331199765, \"time-step\": 2743}, {\"accuracy\": 1.0, \"loss\": 0.0024368923623114824, \"time-step\": 2744}, {\"accuracy\": 1.0, \"loss\": 0.002444501267746091, \"time-step\": 2745}, {\"accuracy\": 1.0, \"loss\": 0.002432163106277585, \"time-step\": 2746}, {\"accuracy\": 1.0, \"loss\": 0.002439821371808648, \"time-step\": 2747}, {\"accuracy\": 1.0, \"loss\": 0.002427462488412857, \"time-step\": 2748}, {\"accuracy\": 1.0, \"loss\": 0.0024351938627660275, \"time-step\": 2749}, {\"accuracy\": 1.0, \"loss\": 0.0024228342808783054, \"time-step\": 2750}, {\"accuracy\": 1.0, \"loss\": 0.0024306043051183224, \"time-step\": 2751}, {\"accuracy\": 1.0, \"loss\": 0.002418214688077569, \"time-step\": 2752}, {\"accuracy\": 1.0, \"loss\": 0.0024260191712528467, \"time-step\": 2753}, {\"accuracy\": 1.0, \"loss\": 0.00241358345374465, \"time-step\": 2754}, {\"accuracy\": 1.0, \"loss\": 0.0024214359000325203, \"time-step\": 2755}, {\"accuracy\": 1.0, \"loss\": 0.002408974338322878, \"time-step\": 2756}, {\"accuracy\": 1.0, \"loss\": 0.0024168696254491806, \"time-step\": 2757}, {\"accuracy\": 1.0, \"loss\": 0.0024043796584010124, \"time-step\": 2758}, {\"accuracy\": 1.0, \"loss\": 0.002412309404462576, \"time-step\": 2759}, {\"accuracy\": 1.0, \"loss\": 0.0023997719399631023, \"time-step\": 2760}, {\"accuracy\": 1.0, \"loss\": 0.0024077552370727062, \"time-step\": 2761}, {\"accuracy\": 1.0, \"loss\": 0.002395212184637785, \"time-step\": 2762}, {\"accuracy\": 1.0, \"loss\": 0.002403222257271409, \"time-step\": 2763}, {\"accuracy\": 1.0, \"loss\": 0.002390634035691619, \"time-step\": 2764}, {\"accuracy\": 1.0, \"loss\": 0.002398681128397584, \"time-step\": 2765}, {\"accuracy\": 1.0, \"loss\": 0.0023860586807131767, \"time-step\": 2766}, {\"accuracy\": 1.0, \"loss\": 0.002394177718088031, \"time-step\": 2767}, {\"accuracy\": 1.0, \"loss\": 0.0023815312888473272, \"time-step\": 2768}, {\"accuracy\": 1.0, \"loss\": 0.00238966871984303, \"time-step\": 2769}, {\"accuracy\": 1.0, \"loss\": 0.0023769850376993418, \"time-step\": 2770}, {\"accuracy\": 1.0, \"loss\": 0.002385176485404372, \"time-step\": 2771}, {\"accuracy\": 1.0, \"loss\": 0.002372484188526869, \"time-step\": 2772}, {\"accuracy\": 1.0, \"loss\": 0.002380707301199436, \"time-step\": 2773}, {\"accuracy\": 1.0, \"loss\": 0.002367980545386672, \"time-step\": 2774}, {\"accuracy\": 1.0, \"loss\": 0.0023762318305671215, \"time-step\": 2775}, {\"accuracy\": 1.0, \"loss\": 0.0023634834215044975, \"time-step\": 2776}, {\"accuracy\": 1.0, \"loss\": 0.0023718010634183884, \"time-step\": 2777}, {\"accuracy\": 1.0, \"loss\": 0.0023590160999447107, \"time-step\": 2778}, {\"accuracy\": 1.0, \"loss\": 0.0023673411924391985, \"time-step\": 2779}, {\"accuracy\": 1.0, \"loss\": 0.0023545543663203716, \"time-step\": 2780}, {\"accuracy\": 1.0, \"loss\": 0.002362930215895176, \"time-step\": 2781}, {\"accuracy\": 1.0, \"loss\": 0.002350108465179801, \"time-step\": 2782}, {\"accuracy\": 1.0, \"loss\": 0.0023585122544318438, \"time-step\": 2783}, {\"accuracy\": 1.0, \"loss\": 0.002345672808587551, \"time-step\": 2784}, {\"accuracy\": 1.0, \"loss\": 0.0023541483096778393, \"time-step\": 2785}, {\"accuracy\": 1.0, \"loss\": 0.0023412832524627447, \"time-step\": 2786}, {\"accuracy\": 1.0, \"loss\": 0.002349790185689926, \"time-step\": 2787}, {\"accuracy\": 1.0, \"loss\": 0.0023368981201201677, \"time-step\": 2788}, {\"accuracy\": 1.0, \"loss\": 0.002345446962863207, \"time-step\": 2789}, {\"accuracy\": 1.0, \"loss\": 0.00233254861086607, \"time-step\": 2790}, {\"accuracy\": 1.0, \"loss\": 0.0023411326110363007, \"time-step\": 2791}, {\"accuracy\": 1.0, \"loss\": 0.0023281779140233994, \"time-step\": 2792}, {\"accuracy\": 1.0, \"loss\": 0.0023367826361209154, \"time-step\": 2793}, {\"accuracy\": 1.0, \"loss\": 0.002323838183656335, \"time-step\": 2794}, {\"accuracy\": 1.0, \"loss\": 0.0023324834182858467, \"time-step\": 2795}, {\"accuracy\": 1.0, \"loss\": 0.0023194930981844664, \"time-step\": 2796}, {\"accuracy\": 1.0, \"loss\": 0.002328191651031375, \"time-step\": 2797}, {\"accuracy\": 1.0, \"loss\": 0.0023151752538979053, \"time-step\": 2798}, {\"accuracy\": 1.0, \"loss\": 0.0023238887079060078, \"time-step\": 2799}, {\"accuracy\": 1.0, \"loss\": 0.002310853684321046, \"time-step\": 2800}, {\"accuracy\": 1.0, \"loss\": 0.002319596242159605, \"time-step\": 2801}, {\"accuracy\": 1.0, \"loss\": 0.0023065374698489904, \"time-step\": 2802}, {\"accuracy\": 1.0, \"loss\": 0.0023153505753725767, \"time-step\": 2803}, {\"accuracy\": 1.0, \"loss\": 0.002302283188328147, \"time-step\": 2804}, {\"accuracy\": 1.0, \"loss\": 0.0023110925685614347, \"time-step\": 2805}, {\"accuracy\": 1.0, \"loss\": 0.00229798280633986, \"time-step\": 2806}, {\"accuracy\": 1.0, \"loss\": 0.002306840382516384, \"time-step\": 2807}, {\"accuracy\": 1.0, \"loss\": 0.0022937231697142124, \"time-step\": 2808}, {\"accuracy\": 1.0, \"loss\": 0.0023026387207210064, \"time-step\": 2809}, {\"accuracy\": 1.0, \"loss\": 0.002289505209773779, \"time-step\": 2810}, {\"accuracy\": 1.0, \"loss\": 0.002298434730619192, \"time-step\": 2811}, {\"accuracy\": 1.0, \"loss\": 0.0022852683905512094, \"time-step\": 2812}, {\"accuracy\": 1.0, \"loss\": 0.0022942356299608946, \"time-step\": 2813}, {\"accuracy\": 1.0, \"loss\": 0.0022810758091509342, \"time-step\": 2814}, {\"accuracy\": 1.0, \"loss\": 0.002290072152391076, \"time-step\": 2815}, {\"accuracy\": 1.0, \"loss\": 0.002276868559420109, \"time-step\": 2816}, {\"accuracy\": 1.0, \"loss\": 0.002285891678184271, \"time-step\": 2817}, {\"accuracy\": 1.0, \"loss\": 0.002272660844027996, \"time-step\": 2818}, {\"accuracy\": 1.0, \"loss\": 0.002281717024743557, \"time-step\": 2819}, {\"accuracy\": 1.0, \"loss\": 0.0022684915456920862, \"time-step\": 2820}, {\"accuracy\": 1.0, \"loss\": 0.002277601743116975, \"time-step\": 2821}, {\"accuracy\": 1.0, \"loss\": 0.002264325274154544, \"time-step\": 2822}, {\"accuracy\": 1.0, \"loss\": 0.002273442456498742, \"time-step\": 2823}, {\"accuracy\": 1.0, \"loss\": 0.002260172041133046, \"time-step\": 2824}, {\"accuracy\": 1.0, \"loss\": 0.002269335091114044, \"time-step\": 2825}, {\"accuracy\": 1.0, \"loss\": 0.002256042556837201, \"time-step\": 2826}, {\"accuracy\": 1.0, \"loss\": 0.0022652309853583574, \"time-step\": 2827}, {\"accuracy\": 1.0, \"loss\": 0.0022519046906381845, \"time-step\": 2828}, {\"accuracy\": 1.0, \"loss\": 0.0022611222229897976, \"time-step\": 2829}, {\"accuracy\": 1.0, \"loss\": 0.002247805241495371, \"time-step\": 2830}, {\"accuracy\": 1.0, \"loss\": 0.0022570653818547726, \"time-step\": 2831}, {\"accuracy\": 1.0, \"loss\": 0.0022437232546508312, \"time-step\": 2832}, {\"accuracy\": 1.0, \"loss\": 0.002253003418445587, \"time-step\": 2833}, {\"accuracy\": 1.0, \"loss\": 0.002239614725112915, \"time-step\": 2834}, {\"accuracy\": 1.0, \"loss\": 0.0022489342372864485, \"time-step\": 2835}, {\"accuracy\": 1.0, \"loss\": 0.0022355576511472464, \"time-step\": 2836}, {\"accuracy\": 1.0, \"loss\": 0.002244888339191675, \"time-step\": 2837}, {\"accuracy\": 1.0, \"loss\": 0.002231488237157464, \"time-step\": 2838}, {\"accuracy\": 1.0, \"loss\": 0.0022408717777580023, \"time-step\": 2839}, {\"accuracy\": 1.0, \"loss\": 0.0022274598013609648, \"time-step\": 2840}, {\"accuracy\": 1.0, \"loss\": 0.0022368726786226034, \"time-step\": 2841}, {\"accuracy\": 1.0, \"loss\": 0.0022234327625483274, \"time-step\": 2842}, {\"accuracy\": 1.0, \"loss\": 0.002232857048511505, \"time-step\": 2843}, {\"accuracy\": 1.0, \"loss\": 0.002219398971647024, \"time-step\": 2844}, {\"accuracy\": 1.0, \"loss\": 0.0022288584150373936, \"time-step\": 2845}, {\"accuracy\": 1.0, \"loss\": 0.002215383108705282, \"time-step\": 2846}, {\"accuracy\": 1.0, \"loss\": 0.002224863739684224, \"time-step\": 2847}, {\"accuracy\": 1.0, \"loss\": 0.0022113891318440437, \"time-step\": 2848}, {\"accuracy\": 1.0, \"loss\": 0.0022209053859114647, \"time-step\": 2849}, {\"accuracy\": 1.0, \"loss\": 0.002207418903708458, \"time-step\": 2850}, {\"accuracy\": 1.0, \"loss\": 0.0022169656585901976, \"time-step\": 2851}, {\"accuracy\": 1.0, \"loss\": 0.0022034568246454, \"time-step\": 2852}, {\"accuracy\": 1.0, \"loss\": 0.0022130031138658524, \"time-step\": 2853}, {\"accuracy\": 1.0, \"loss\": 0.002199472626671195, \"time-step\": 2854}, {\"accuracy\": 1.0, \"loss\": 0.002209059428423643, \"time-step\": 2855}, {\"accuracy\": 1.0, \"loss\": 0.002195526845753193, \"time-step\": 2856}, {\"accuracy\": 1.0, \"loss\": 0.0022051676642149687, \"time-step\": 2857}, {\"accuracy\": 1.0, \"loss\": 0.002191595733165741, \"time-step\": 2858}, {\"accuracy\": 1.0, \"loss\": 0.00220125587657094, \"time-step\": 2859}, {\"accuracy\": 1.0, \"loss\": 0.0021876629907637835, \"time-step\": 2860}, {\"accuracy\": 1.0, \"loss\": 0.0021973170805722475, \"time-step\": 2861}, {\"accuracy\": 1.0, \"loss\": 0.002183707896620035, \"time-step\": 2862}, {\"accuracy\": 1.0, \"loss\": 0.00219340855255723, \"time-step\": 2863}, {\"accuracy\": 1.0, \"loss\": 0.00217980844900012, \"time-step\": 2864}, {\"accuracy\": 1.0, \"loss\": 0.002189546125009656, \"time-step\": 2865}, {\"accuracy\": 1.0, \"loss\": 0.002175932750105858, \"time-step\": 2866}, {\"accuracy\": 1.0, \"loss\": 0.002185707213357091, \"time-step\": 2867}, {\"accuracy\": 1.0, \"loss\": 0.0021720840595662594, \"time-step\": 2868}, {\"accuracy\": 1.0, \"loss\": 0.00218186411075294, \"time-step\": 2869}, {\"accuracy\": 1.0, \"loss\": 0.002168213715776801, \"time-step\": 2870}, {\"accuracy\": 1.0, \"loss\": 0.0021780263632535934, \"time-step\": 2871}, {\"accuracy\": 1.0, \"loss\": 0.0021643629297614098, \"time-step\": 2872}, {\"accuracy\": 1.0, \"loss\": 0.0021742028184235096, \"time-step\": 2873}, {\"accuracy\": 1.0, \"loss\": 0.00216052052564919, \"time-step\": 2874}, {\"accuracy\": 1.0, \"loss\": 0.0021703678648918867, \"time-step\": 2875}, {\"accuracy\": 1.0, \"loss\": 0.002156693022698164, \"time-step\": 2876}, {\"accuracy\": 1.0, \"loss\": 0.0021665971726179123, \"time-step\": 2877}, {\"accuracy\": 1.0, \"loss\": 0.0021529104560613632, \"time-step\": 2878}, {\"accuracy\": 1.0, \"loss\": 0.0021628327667713165, \"time-step\": 2879}, {\"accuracy\": 1.0, \"loss\": 0.0021491204388439655, \"time-step\": 2880}, {\"accuracy\": 1.0, \"loss\": 0.00215906067751348, \"time-step\": 2881}, {\"accuracy\": 1.0, \"loss\": 0.002145336475223303, \"time-step\": 2882}, {\"accuracy\": 1.0, \"loss\": 0.0021552867256104946, \"time-step\": 2883}, {\"accuracy\": 1.0, \"loss\": 0.0021415348164737225, \"time-step\": 2884}, {\"accuracy\": 1.0, \"loss\": 0.0021515008993446827, \"time-step\": 2885}, {\"accuracy\": 1.0, \"loss\": 0.002137732459232211, \"time-step\": 2886}, {\"accuracy\": 1.0, \"loss\": 0.0021477537229657173, \"time-step\": 2887}, {\"accuracy\": 1.0, \"loss\": 0.002133989240974188, \"time-step\": 2888}, {\"accuracy\": 1.0, \"loss\": 0.00214401469565928, \"time-step\": 2889}, {\"accuracy\": 1.0, \"loss\": 0.0021302185487002134, \"time-step\": 2890}, {\"accuracy\": 1.0, \"loss\": 0.0021402728743851185, \"time-step\": 2891}, {\"accuracy\": 1.0, \"loss\": 0.002126486739143729, \"time-step\": 2892}, {\"accuracy\": 1.0, \"loss\": 0.00213656690903008, \"time-step\": 2893}, {\"accuracy\": 1.0, \"loss\": 0.0021227803081274033, \"time-step\": 2894}, {\"accuracy\": 1.0, \"loss\": 0.002132866531610489, \"time-step\": 2895}, {\"accuracy\": 1.0, \"loss\": 0.0021190373227000237, \"time-step\": 2896}, {\"accuracy\": 1.0, \"loss\": 0.0021291570737957954, \"time-step\": 2897}, {\"accuracy\": 1.0, \"loss\": 0.0021153392735868692, \"time-step\": 2898}, {\"accuracy\": 1.0, \"loss\": 0.002125486731529236, \"time-step\": 2899}, {\"accuracy\": 1.0, \"loss\": 0.00211165240034461, \"time-step\": 2900}, {\"accuracy\": 1.0, \"loss\": 0.0021217975299805403, \"time-step\": 2901}, {\"accuracy\": 1.0, \"loss\": 0.002107930136844516, \"time-step\": 2902}, {\"accuracy\": 1.0, \"loss\": 0.0021181090269237757, \"time-step\": 2903}, {\"accuracy\": 1.0, \"loss\": 0.002104264684021473, \"time-step\": 2904}, {\"accuracy\": 1.0, \"loss\": 0.002114493167027831, \"time-step\": 2905}, {\"accuracy\": 1.0, \"loss\": 0.0021006499882787466, \"time-step\": 2906}, {\"accuracy\": 1.0, \"loss\": 0.0021109094377607107, \"time-step\": 2907}, {\"accuracy\": 1.0, \"loss\": 0.0020970574114471674, \"time-step\": 2908}, {\"accuracy\": 1.0, \"loss\": 0.0021073140669614077, \"time-step\": 2909}, {\"accuracy\": 1.0, \"loss\": 0.002093430608510971, \"time-step\": 2910}, {\"accuracy\": 1.0, \"loss\": 0.0021036898251622915, \"time-step\": 2911}, {\"accuracy\": 1.0, \"loss\": 0.002089802175760269, \"time-step\": 2912}, {\"accuracy\": 1.0, \"loss\": 0.002100096084177494, \"time-step\": 2913}, {\"accuracy\": 1.0, \"loss\": 0.0020861865486949682, \"time-step\": 2914}, {\"accuracy\": 1.0, \"loss\": 0.0020964902359992266, \"time-step\": 2915}, {\"accuracy\": 1.0, \"loss\": 0.002082565799355507, \"time-step\": 2916}, {\"accuracy\": 1.0, \"loss\": 0.002092893933877349, \"time-step\": 2917}, {\"accuracy\": 1.0, \"loss\": 0.0020789955742657185, \"time-step\": 2918}, {\"accuracy\": 1.0, \"loss\": 0.0020893511828035116, \"time-step\": 2919}, {\"accuracy\": 1.0, \"loss\": 0.0020754323340952396, \"time-step\": 2920}, {\"accuracy\": 1.0, \"loss\": 0.002085795160382986, \"time-step\": 2921}, {\"accuracy\": 1.0, \"loss\": 0.002071848837658763, \"time-step\": 2922}, {\"accuracy\": 1.0, \"loss\": 0.002082236809656024, \"time-step\": 2923}, {\"accuracy\": 1.0, \"loss\": 0.002068291651085019, \"time-step\": 2924}, {\"accuracy\": 1.0, \"loss\": 0.002078705932945013, \"time-step\": 2925}, {\"accuracy\": 1.0, \"loss\": 0.002064741915091872, \"time-step\": 2926}, {\"accuracy\": 1.0, \"loss\": 0.0020751620177179575, \"time-step\": 2927}, {\"accuracy\": 1.0, \"loss\": 0.0020612210500985384, \"time-step\": 2928}, {\"accuracy\": 1.0, \"loss\": 0.002071667928248644, \"time-step\": 2929}, {\"accuracy\": 1.0, \"loss\": 0.0020576752722263336, \"time-step\": 2930}, {\"accuracy\": 1.0, \"loss\": 0.0020681272726505995, \"time-step\": 2931}, {\"accuracy\": 1.0, \"loss\": 0.0020541450940072536, \"time-step\": 2932}, {\"accuracy\": 1.0, \"loss\": 0.002064642496407032, \"time-step\": 2933}, {\"accuracy\": 1.0, \"loss\": 0.0020506668370217085, \"time-step\": 2934}, {\"accuracy\": 1.0, \"loss\": 0.002061167499050498, \"time-step\": 2935}, {\"accuracy\": 1.0, \"loss\": 0.0020471783354878426, \"time-step\": 2936}, {\"accuracy\": 1.0, \"loss\": 0.002057707402855158, \"time-step\": 2937}, {\"accuracy\": 1.0, \"loss\": 0.0020437005441635847, \"time-step\": 2938}, {\"accuracy\": 1.0, \"loss\": 0.002054214710369706, \"time-step\": 2939}, {\"accuracy\": 1.0, \"loss\": 0.0020401899237185717, \"time-step\": 2940}, {\"accuracy\": 1.0, \"loss\": 0.0020507383160293102, \"time-step\": 2941}, {\"accuracy\": 1.0, \"loss\": 0.002036712598055601, \"time-step\": 2942}, {\"accuracy\": 1.0, \"loss\": 0.002047295216470957, \"time-step\": 2943}, {\"accuracy\": 1.0, \"loss\": 0.0020332783460617065, \"time-step\": 2944}, {\"accuracy\": 1.0, \"loss\": 0.0020438439678400755, \"time-step\": 2945}, {\"accuracy\": 1.0, \"loss\": 0.0020298033487051725, \"time-step\": 2946}, {\"accuracy\": 1.0, \"loss\": 0.0020404120441526175, \"time-step\": 2947}, {\"accuracy\": 1.0, \"loss\": 0.0020263760816305876, \"time-step\": 2948}, {\"accuracy\": 1.0, \"loss\": 0.0020369996782392263, \"time-step\": 2949}, {\"accuracy\": 1.0, \"loss\": 0.002022950677201152, \"time-step\": 2950}, {\"accuracy\": 1.0, \"loss\": 0.002033587545156479, \"time-step\": 2951}, {\"accuracy\": 1.0, \"loss\": 0.0020195282995700836, \"time-step\": 2952}, {\"accuracy\": 1.0, \"loss\": 0.0020301761105656624, \"time-step\": 2953}, {\"accuracy\": 1.0, \"loss\": 0.0020161019638180733, \"time-step\": 2954}, {\"accuracy\": 1.0, \"loss\": 0.002026746980845928, \"time-step\": 2955}, {\"accuracy\": 1.0, \"loss\": 0.002012668875977397, \"time-step\": 2956}, {\"accuracy\": 1.0, \"loss\": 0.0020233530085533857, \"time-step\": 2957}, {\"accuracy\": 1.0, \"loss\": 0.0020092909689992666, \"time-step\": 2958}, {\"accuracy\": 1.0, \"loss\": 0.00201999768614769, \"time-step\": 2959}, {\"accuracy\": 1.0, \"loss\": 0.0020059330854564905, \"time-step\": 2960}, {\"accuracy\": 1.0, \"loss\": 0.0020166554022580385, \"time-step\": 2961}, {\"accuracy\": 1.0, \"loss\": 0.002002573339268565, \"time-step\": 2962}, {\"accuracy\": 1.0, \"loss\": 0.0020133061334490776, \"time-step\": 2963}, {\"accuracy\": 1.0, \"loss\": 0.001999208703637123, \"time-step\": 2964}, {\"accuracy\": 1.0, \"loss\": 0.00200994242914021, \"time-step\": 2965}, {\"accuracy\": 1.0, \"loss\": 0.0019958456978201866, \"time-step\": 2966}, {\"accuracy\": 1.0, \"loss\": 0.0020065989810973406, \"time-step\": 2967}, {\"accuracy\": 1.0, \"loss\": 0.001992485485970974, \"time-step\": 2968}, {\"accuracy\": 1.0, \"loss\": 0.0020032841712236404, \"time-step\": 2969}, {\"accuracy\": 1.0, \"loss\": 0.001989186042919755, \"time-step\": 2970}, {\"accuracy\": 1.0, \"loss\": 0.001999987056478858, \"time-step\": 2971}, {\"accuracy\": 1.0, \"loss\": 0.001985891256481409, \"time-step\": 2972}, {\"accuracy\": 1.0, \"loss\": 0.0019966892432421446, \"time-step\": 2973}, {\"accuracy\": 1.0, \"loss\": 0.0019825901836156845, \"time-step\": 2974}, {\"accuracy\": 1.0, \"loss\": 0.0019934228621423244, \"time-step\": 2975}, {\"accuracy\": 1.0, \"loss\": 0.0019793135579675436, \"time-step\": 2976}, {\"accuracy\": 1.0, \"loss\": 0.001990146469324827, \"time-step\": 2977}, {\"accuracy\": 1.0, \"loss\": 0.001976016676053405, \"time-step\": 2978}, {\"accuracy\": 1.0, \"loss\": 0.0019868542440235615, \"time-step\": 2979}, {\"accuracy\": 1.0, \"loss\": 0.001972716301679611, \"time-step\": 2980}, {\"accuracy\": 1.0, \"loss\": 0.001983576687052846, \"time-step\": 2981}, {\"accuracy\": 1.0, \"loss\": 0.0019694494549185038, \"time-step\": 2982}, {\"accuracy\": 1.0, \"loss\": 0.001980332424864173, \"time-step\": 2983}, {\"accuracy\": 1.0, \"loss\": 0.0019661770202219486, \"time-step\": 2984}, {\"accuracy\": 1.0, \"loss\": 0.001977081410586834, \"time-step\": 2985}, {\"accuracy\": 1.0, \"loss\": 0.001962966052815318, \"time-step\": 2986}, {\"accuracy\": 1.0, \"loss\": 0.0019738865084946156, \"time-step\": 2987}, {\"accuracy\": 1.0, \"loss\": 0.0019597476348280907, \"time-step\": 2988}, {\"accuracy\": 1.0, \"loss\": 0.0019706683233380318, \"time-step\": 2989}, {\"accuracy\": 1.0, \"loss\": 0.001956517808139324, \"time-step\": 2990}, {\"accuracy\": 1.0, \"loss\": 0.0019674510695040226, \"time-step\": 2991}, {\"accuracy\": 1.0, \"loss\": 0.0019533238373696804, \"time-step\": 2992}, {\"accuracy\": 1.0, \"loss\": 0.0019642645493149757, \"time-step\": 2993}, {\"accuracy\": 1.0, \"loss\": 0.0019500873750075698, \"time-step\": 2994}, {\"accuracy\": 1.0, \"loss\": 0.001961030066013336, \"time-step\": 2995}, {\"accuracy\": 1.0, \"loss\": 0.001946846372447908, \"time-step\": 2996}, {\"accuracy\": 1.0, \"loss\": 0.001957816071808338, \"time-step\": 2997}, {\"accuracy\": 1.0, \"loss\": 0.001943656476214528, \"time-step\": 2998}, {\"accuracy\": 1.0, \"loss\": 0.0019546635448932648, \"time-step\": 2999}, {\"accuracy\": 1.0, \"loss\": 0.001940506394021213, \"time-step\": 3000}, {\"accuracy\": 1.0, \"loss\": 0.001951513229869306, \"time-step\": 3001}, {\"accuracy\": 1.0, \"loss\": 0.0019373476970940828, \"time-step\": 3002}, {\"accuracy\": 1.0, \"loss\": 0.001948341727256775, \"time-step\": 3003}, {\"accuracy\": 1.0, \"loss\": 0.0019341728184372187, \"time-step\": 3004}, {\"accuracy\": 1.0, \"loss\": 0.0019452018896117806, \"time-step\": 3005}, {\"accuracy\": 1.0, \"loss\": 0.0019310098141431808, \"time-step\": 3006}, {\"accuracy\": 1.0, \"loss\": 0.0019420413300395012, \"time-step\": 3007}, {\"accuracy\": 1.0, \"loss\": 0.0019278540275990963, \"time-step\": 3008}, {\"accuracy\": 1.0, \"loss\": 0.0019388983491808176, \"time-step\": 3009}, {\"accuracy\": 1.0, \"loss\": 0.0019247194286435843, \"time-step\": 3010}, {\"accuracy\": 1.0, \"loss\": 0.0019357622368261218, \"time-step\": 3011}, {\"accuracy\": 1.0, \"loss\": 0.0019215734209865332, \"time-step\": 3012}, {\"accuracy\": 1.0, \"loss\": 0.001932644285261631, \"time-step\": 3013}, {\"accuracy\": 1.0, \"loss\": 0.0019184646662324667, \"time-step\": 3014}, {\"accuracy\": 1.0, \"loss\": 0.0019295370439067483, \"time-step\": 3015}, {\"accuracy\": 1.0, \"loss\": 0.0019153602188453078, \"time-step\": 3016}, {\"accuracy\": 1.0, \"loss\": 0.0019264596048742533, \"time-step\": 3017}, {\"accuracy\": 1.0, \"loss\": 0.0019122817320749164, \"time-step\": 3018}, {\"accuracy\": 1.0, \"loss\": 0.0019233962520956993, \"time-step\": 3019}, {\"accuracy\": 1.0, \"loss\": 0.001909213257022202, \"time-step\": 3020}, {\"accuracy\": 1.0, \"loss\": 0.0019203226547688246, \"time-step\": 3021}, {\"accuracy\": 1.0, \"loss\": 0.001906124409288168, \"time-step\": 3022}, {\"accuracy\": 1.0, \"loss\": 0.001917244982905686, \"time-step\": 3023}, {\"accuracy\": 1.0, \"loss\": 0.0019030440598726273, \"time-step\": 3024}, {\"accuracy\": 1.0, \"loss\": 0.00191417895257473, \"time-step\": 3025}, {\"accuracy\": 1.0, \"loss\": 0.0018999666208401322, \"time-step\": 3026}, {\"accuracy\": 1.0, \"loss\": 0.0019111275905743241, \"time-step\": 3027}, {\"accuracy\": 1.0, \"loss\": 0.0018969333032146096, \"time-step\": 3028}, {\"accuracy\": 1.0, \"loss\": 0.0019080714555457234, \"time-step\": 3029}, {\"accuracy\": 1.0, \"loss\": 0.0018938424764201045, \"time-step\": 3030}, {\"accuracy\": 1.0, \"loss\": 0.001904980163089931, \"time-step\": 3031}, {\"accuracy\": 1.0, \"loss\": 0.00189075933303684, \"time-step\": 3032}, {\"accuracy\": 1.0, \"loss\": 0.0019019236788153648, \"time-step\": 3033}, {\"accuracy\": 1.0, \"loss\": 0.0018877239199355245, \"time-step\": 3034}, {\"accuracy\": 1.0, \"loss\": 0.0018989024683833122, \"time-step\": 3035}, {\"accuracy\": 1.0, \"loss\": 0.0018846915336325765, \"time-step\": 3036}, {\"accuracy\": 1.0, \"loss\": 0.0018958861473947763, \"time-step\": 3037}, {\"accuracy\": 1.0, \"loss\": 0.0018817039672285318, \"time-step\": 3038}, {\"accuracy\": 1.0, \"loss\": 0.0018929298967123032, \"time-step\": 3039}, {\"accuracy\": 1.0, \"loss\": 0.0018787309527397156, \"time-step\": 3040}, {\"accuracy\": 1.0, \"loss\": 0.0018899280112236738, \"time-step\": 3041}, {\"accuracy\": 1.0, \"loss\": 0.0018757106736302376, \"time-step\": 3042}, {\"accuracy\": 1.0, \"loss\": 0.0018869324121624231, \"time-step\": 3043}, {\"accuracy\": 1.0, \"loss\": 0.0018727189162746072, \"time-step\": 3044}, {\"accuracy\": 1.0, \"loss\": 0.001883958000689745, \"time-step\": 3045}, {\"accuracy\": 1.0, \"loss\": 0.0018697549821808934, \"time-step\": 3046}, {\"accuracy\": 1.0, \"loss\": 0.0018809970933943987, \"time-step\": 3047}, {\"accuracy\": 1.0, \"loss\": 0.001866791513748467, \"time-step\": 3048}, {\"accuracy\": 1.0, \"loss\": 0.0018780524842441082, \"time-step\": 3049}, {\"accuracy\": 1.0, \"loss\": 0.0018638429464772344, \"time-step\": 3050}, {\"accuracy\": 1.0, \"loss\": 0.0018751118332147598, \"time-step\": 3051}, {\"accuracy\": 1.0, \"loss\": 0.0018609027611091733, \"time-step\": 3052}, {\"accuracy\": 1.0, \"loss\": 0.0018721588421612978, \"time-step\": 3053}, {\"accuracy\": 1.0, \"loss\": 0.0018579454626888037, \"time-step\": 3054}, {\"accuracy\": 1.0, \"loss\": 0.0018692295998334885, \"time-step\": 3055}, {\"accuracy\": 1.0, \"loss\": 0.001855025882832706, \"time-step\": 3056}, {\"accuracy\": 1.0, \"loss\": 0.001866320613771677, \"time-step\": 3057}, {\"accuracy\": 1.0, \"loss\": 0.0018521123565733433, \"time-step\": 3058}, {\"accuracy\": 1.0, \"loss\": 0.0018633947474882007, \"time-step\": 3059}, {\"accuracy\": 1.0, \"loss\": 0.0018491744995117188, \"time-step\": 3060}, {\"accuracy\": 1.0, \"loss\": 0.0018604606157168746, \"time-step\": 3061}, {\"accuracy\": 1.0, \"loss\": 0.0018462584121152759, \"time-step\": 3062}, {\"accuracy\": 1.0, \"loss\": 0.0018575689755380154, \"time-step\": 3063}, {\"accuracy\": 1.0, \"loss\": 0.0018433448858559132, \"time-step\": 3064}, {\"accuracy\": 1.0, \"loss\": 0.001854659290984273, \"time-step\": 3065}, {\"accuracy\": 1.0, \"loss\": 0.0018404412548989058, \"time-step\": 3066}, {\"accuracy\": 1.0, \"loss\": 0.0018517754506319761, \"time-step\": 3067}, {\"accuracy\": 1.0, \"loss\": 0.0018375462386757135, \"time-step\": 3068}, {\"accuracy\": 1.0, \"loss\": 0.0018488753121346235, \"time-step\": 3069}, {\"accuracy\": 1.0, \"loss\": 0.0018346690339967608, \"time-step\": 3070}, {\"accuracy\": 1.0, \"loss\": 0.0018460068386048079, \"time-step\": 3071}, {\"accuracy\": 1.0, \"loss\": 0.001831787172704935, \"time-step\": 3072}, {\"accuracy\": 1.0, \"loss\": 0.0018431257922202349, \"time-step\": 3073}, {\"accuracy\": 1.0, \"loss\": 0.0018289181170985103, \"time-step\": 3074}, {\"accuracy\": 1.0, \"loss\": 0.0018402793211862445, \"time-step\": 3075}, {\"accuracy\": 1.0, \"loss\": 0.0018260717624798417, \"time-step\": 3076}, {\"accuracy\": 1.0, \"loss\": 0.0018374222563579679, \"time-step\": 3077}, {\"accuracy\": 1.0, \"loss\": 0.0018232252914458513, \"time-step\": 3078}, {\"accuracy\": 1.0, \"loss\": 0.0018345857970416546, \"time-step\": 3079}, {\"accuracy\": 1.0, \"loss\": 0.001820380799472332, \"time-step\": 3080}, {\"accuracy\": 1.0, \"loss\": 0.0018317557405680418, \"time-step\": 3081}, {\"accuracy\": 1.0, \"loss\": 0.0018175359582528472, \"time-step\": 3082}, {\"accuracy\": 1.0, \"loss\": 0.0018289255676791072, \"time-step\": 3083}, {\"accuracy\": 1.0, \"loss\": 0.001814728369936347, \"time-step\": 3084}, {\"accuracy\": 1.0, \"loss\": 0.0018261212389916182, \"time-step\": 3085}, {\"accuracy\": 1.0, \"loss\": 0.0018119056476280093, \"time-step\": 3086}, {\"accuracy\": 1.0, \"loss\": 0.001823296071961522, \"time-step\": 3087}, {\"accuracy\": 1.0, \"loss\": 0.0018090694211423397, \"time-step\": 3088}, {\"accuracy\": 1.0, \"loss\": 0.001820470904931426, \"time-step\": 3089}, {\"accuracy\": 1.0, \"loss\": 0.0018062704475596547, \"time-step\": 3090}, {\"accuracy\": 1.0, \"loss\": 0.0018176782177761197, \"time-step\": 3091}, {\"accuracy\": 1.0, \"loss\": 0.0018034770619124174, \"time-step\": 3092}, {\"accuracy\": 1.0, \"loss\": 0.0018148979870602489, \"time-step\": 3093}, {\"accuracy\": 1.0, \"loss\": 0.0018006917089223862, \"time-step\": 3094}, {\"accuracy\": 1.0, \"loss\": 0.0018121384782716632, \"time-step\": 3095}, {\"accuracy\": 1.0, \"loss\": 0.0017979472177103162, \"time-step\": 3096}, {\"accuracy\": 1.0, \"loss\": 0.0018093589460477233, \"time-step\": 3097}, {\"accuracy\": 1.0, \"loss\": 0.0017951587215065956, \"time-step\": 3098}, {\"accuracy\": 1.0, \"loss\": 0.0018066000193357468, \"time-step\": 3099}, {\"accuracy\": 1.0, \"loss\": 0.001792412018403411, \"time-step\": 3100}, {\"accuracy\": 1.0, \"loss\": 0.0018038556445389986, \"time-step\": 3101}, {\"accuracy\": 1.0, \"loss\": 0.001789657399058342, \"time-step\": 3102}, {\"accuracy\": 1.0, \"loss\": 0.0018010989297181368, \"time-step\": 3103}, {\"accuracy\": 1.0, \"loss\": 0.0017869058065116405, \"time-step\": 3104}, {\"accuracy\": 1.0, \"loss\": 0.0017983746947720647, \"time-step\": 3105}, {\"accuracy\": 1.0, \"loss\": 0.0017841788940131664, \"time-step\": 3106}, {\"accuracy\": 1.0, \"loss\": 0.0017956416122615337, \"time-step\": 3107}, {\"accuracy\": 1.0, \"loss\": 0.0017814477905631065, \"time-step\": 3108}, {\"accuracy\": 1.0, \"loss\": 0.0017929170280694962, \"time-step\": 3109}, {\"accuracy\": 1.0, \"loss\": 0.0017787161050364375, \"time-step\": 3110}, {\"accuracy\": 1.0, \"loss\": 0.001790158450603485, \"time-step\": 3111}, {\"accuracy\": 1.0, \"loss\": 0.0017759562470018864, \"time-step\": 3112}, {\"accuracy\": 1.0, \"loss\": 0.0017874276963993907, \"time-step\": 3113}, {\"accuracy\": 1.0, \"loss\": 0.001773237599991262, \"time-step\": 3114}, {\"accuracy\": 1.0, \"loss\": 0.001784721971489489, \"time-step\": 3115}, {\"accuracy\": 1.0, \"loss\": 0.0017705349018797278, \"time-step\": 3116}, {\"accuracy\": 1.0, \"loss\": 0.0017820082139223814, \"time-step\": 3117}, {\"accuracy\": 1.0, \"loss\": 0.001767834764905274, \"time-step\": 3118}, {\"accuracy\": 1.0, \"loss\": 0.0017793355509638786, \"time-step\": 3119}, {\"accuracy\": 1.0, \"loss\": 0.0017651605885475874, \"time-step\": 3120}, {\"accuracy\": 1.0, \"loss\": 0.0017766542732715607, \"time-step\": 3121}, {\"accuracy\": 1.0, \"loss\": 0.0017624814063310623, \"time-step\": 3122}, {\"accuracy\": 1.0, \"loss\": 0.001773985568434, \"time-step\": 3123}, {\"accuracy\": 1.0, \"loss\": 0.0017598120030015707, \"time-step\": 3124}, {\"accuracy\": 1.0, \"loss\": 0.0017713319975882769, \"time-step\": 3125}, {\"accuracy\": 1.0, \"loss\": 0.0017571478383615613, \"time-step\": 3126}, {\"accuracy\": 1.0, \"loss\": 0.001768648624420166, \"time-step\": 3127}, {\"accuracy\": 1.0, \"loss\": 0.001754501718096435, \"time-step\": 3128}, {\"accuracy\": 1.0, \"loss\": 0.0017660248558968306, \"time-step\": 3129}, {\"accuracy\": 1.0, \"loss\": 0.0017518519889563322, \"time-step\": 3130}, {\"accuracy\": 1.0, \"loss\": 0.001763370237313211, \"time-step\": 3131}, {\"accuracy\": 1.0, \"loss\": 0.0017492210026830435, \"time-step\": 3132}, {\"accuracy\": 1.0, \"loss\": 0.0017607557820156217, \"time-step\": 3133}, {\"accuracy\": 1.0, \"loss\": 0.0017466065473854542, \"time-step\": 3134}, {\"accuracy\": 1.0, \"loss\": 0.001758152968250215, \"time-step\": 3135}, {\"accuracy\": 1.0, \"loss\": 0.0017440177034586668, \"time-step\": 3136}, {\"accuracy\": 1.0, \"loss\": 0.0017555663362145424, \"time-step\": 3137}, {\"accuracy\": 1.0, \"loss\": 0.0017413843888789415, \"time-step\": 3138}, {\"accuracy\": 1.0, \"loss\": 0.0017529106698930264, \"time-step\": 3139}, {\"accuracy\": 1.0, \"loss\": 0.0017387430416420102, \"time-step\": 3140}, {\"accuracy\": 1.0, \"loss\": 0.0017502623377367854, \"time-step\": 3141}, {\"accuracy\": 1.0, \"loss\": 0.0017361021600663662, \"time-step\": 3142}, {\"accuracy\": 1.0, \"loss\": 0.0017476517241448164, \"time-step\": 3143}, {\"accuracy\": 1.0, \"loss\": 0.0017335120355710387, \"time-step\": 3144}, {\"accuracy\": 1.0, \"loss\": 0.0017450597370043397, \"time-step\": 3145}, {\"accuracy\": 1.0, \"loss\": 0.0017309206305071712, \"time-step\": 3146}, {\"accuracy\": 1.0, \"loss\": 0.0017424790421500802, \"time-step\": 3147}, {\"accuracy\": 1.0, \"loss\": 0.001728329691104591, \"time-step\": 3148}, {\"accuracy\": 1.0, \"loss\": 0.0017398789059370756, \"time-step\": 3149}, {\"accuracy\": 1.0, \"loss\": 0.001725733745843172, \"time-step\": 3150}, {\"accuracy\": 1.0, \"loss\": 0.0017372940201312304, \"time-step\": 3151}, {\"accuracy\": 1.0, \"loss\": 0.0017231509555131197, \"time-step\": 3152}, {\"accuracy\": 1.0, \"loss\": 0.0017347073880955577, \"time-step\": 3153}, {\"accuracy\": 1.0, \"loss\": 0.0017205793410539627, \"time-step\": 3154}, {\"accuracy\": 1.0, \"loss\": 0.0017321528866887093, \"time-step\": 3155}, {\"accuracy\": 1.0, \"loss\": 0.001718020997941494, \"time-step\": 3156}, {\"accuracy\": 1.0, \"loss\": 0.0017295914003625512, \"time-step\": 3157}, {\"accuracy\": 1.0, \"loss\": 0.0017154690576717257, \"time-step\": 3158}, {\"accuracy\": 1.0, \"loss\": 0.0017270490061491728, \"time-step\": 3159}, {\"accuracy\": 1.0, \"loss\": 0.0017129433108493686, \"time-step\": 3160}, {\"accuracy\": 1.0, \"loss\": 0.0017245144117623568, \"time-step\": 3161}, {\"accuracy\": 1.0, \"loss\": 0.001710378797724843, \"time-step\": 3162}, {\"accuracy\": 1.0, \"loss\": 0.0017219474539160728, \"time-step\": 3163}, {\"accuracy\": 1.0, \"loss\": 0.001707845600321889, \"time-step\": 3164}, {\"accuracy\": 1.0, \"loss\": 0.0017194468528032303, \"time-step\": 3165}, {\"accuracy\": 1.0, \"loss\": 0.0017053356859833002, \"time-step\": 3166}, {\"accuracy\": 1.0, \"loss\": 0.0017169236671179533, \"time-step\": 3167}, {\"accuracy\": 1.0, \"loss\": 0.001702828798443079, \"time-step\": 3168}, {\"accuracy\": 1.0, \"loss\": 0.0017144203884527087, \"time-step\": 3169}, {\"accuracy\": 1.0, \"loss\": 0.0017003173707053065, \"time-step\": 3170}, {\"accuracy\": 1.0, \"loss\": 0.0017119039548560977, \"time-step\": 3171}, {\"accuracy\": 1.0, \"loss\": 0.001697802566923201, \"time-step\": 3172}, {\"accuracy\": 1.0, \"loss\": 0.0017093875212594867, \"time-step\": 3173}, {\"accuracy\": 1.0, \"loss\": 0.0016953034792095423, \"time-step\": 3174}, {\"accuracy\": 1.0, \"loss\": 0.0017069088062271476, \"time-step\": 3175}, {\"accuracy\": 1.0, \"loss\": 0.0016928308177739382, \"time-step\": 3176}, {\"accuracy\": 1.0, \"loss\": 0.0017044248525053263, \"time-step\": 3177}, {\"accuracy\": 1.0, \"loss\": 0.001690329983830452, \"time-step\": 3178}, {\"accuracy\": 1.0, \"loss\": 0.0017019378719851375, \"time-step\": 3179}, {\"accuracy\": 1.0, \"loss\": 0.0016878669848665595, \"time-step\": 3180}, {\"accuracy\": 1.0, \"loss\": 0.001699472893960774, \"time-step\": 3181}, {\"accuracy\": 1.0, \"loss\": 0.0016853867564350367, \"time-step\": 3182}, {\"accuracy\": 1.0, \"loss\": 0.0016969854477792978, \"time-step\": 3183}, {\"accuracy\": 1.0, \"loss\": 0.0016829001251608133, \"time-step\": 3184}, {\"accuracy\": 1.0, \"loss\": 0.0016945145325735211, \"time-step\": 3185}, {\"accuracy\": 1.0, \"loss\": 0.001680442364886403, \"time-step\": 3186}, {\"accuracy\": 1.0, \"loss\": 0.001692045945674181, \"time-step\": 3187}, {\"accuracy\": 1.0, \"loss\": 0.0016779731959104538, \"time-step\": 3188}, {\"accuracy\": 1.0, \"loss\": 0.0016895981971174479, \"time-step\": 3189}, {\"accuracy\": 1.0, \"loss\": 0.0016755362739786506, \"time-step\": 3190}, {\"accuracy\": 1.0, \"loss\": 0.0016871476545929909, \"time-step\": 3191}, {\"accuracy\": 1.0, \"loss\": 0.0016730895731598139, \"time-step\": 3192}, {\"accuracy\": 1.0, \"loss\": 0.0016847171355038881, \"time-step\": 3193}, {\"accuracy\": 1.0, \"loss\": 0.0016706684837117791, \"time-step\": 3194}, {\"accuracy\": 1.0, \"loss\": 0.001682277419604361, \"time-step\": 3195}, {\"accuracy\": 1.0, \"loss\": 0.0016682419227436185, \"time-step\": 3196}, {\"accuracy\": 1.0, \"loss\": 0.001679871929809451, \"time-step\": 3197}, {\"accuracy\": 1.0, \"loss\": 0.001665838877670467, \"time-step\": 3198}, {\"accuracy\": 1.0, \"loss\": 0.0016774543328210711, \"time-step\": 3199}, {\"accuracy\": 1.0, \"loss\": 0.001663410454057157, \"time-step\": 3200}, {\"accuracy\": 1.0, \"loss\": 0.0016750465147197247, \"time-step\": 3201}, {\"accuracy\": 1.0, \"loss\": 0.001661032671108842, \"time-step\": 3202}, {\"accuracy\": 1.0, \"loss\": 0.001672673039138317, \"time-step\": 3203}, {\"accuracy\": 1.0, \"loss\": 0.0016586571000516415, \"time-step\": 3204}, {\"accuracy\": 1.0, \"loss\": 0.0016703014262020588, \"time-step\": 3205}, {\"accuracy\": 1.0, \"loss\": 0.0016563000390306115, \"time-step\": 3206}, {\"accuracy\": 1.0, \"loss\": 0.001667927484959364, \"time-step\": 3207}, {\"accuracy\": 1.0, \"loss\": 0.0016539107309654355, \"time-step\": 3208}, {\"accuracy\": 1.0, \"loss\": 0.0016655372455716133, \"time-step\": 3209}, {\"accuracy\": 1.0, \"loss\": 0.0016514970920979977, \"time-step\": 3210}, {\"accuracy\": 1.0, \"loss\": 0.001663128030486405, \"time-step\": 3211}, {\"accuracy\": 1.0, \"loss\": 0.0016491229180246592, \"time-step\": 3212}, {\"accuracy\": 1.0, \"loss\": 0.0016607724828645587, \"time-step\": 3213}, {\"accuracy\": 1.0, \"loss\": 0.0016467706300318241, \"time-step\": 3214}, {\"accuracy\": 1.0, \"loss\": 0.0016583926044404507, \"time-step\": 3215}, {\"accuracy\": 1.0, \"loss\": 0.001644402276724577, \"time-step\": 3216}, {\"accuracy\": 1.0, \"loss\": 0.0016560449730604887, \"time-step\": 3217}, {\"accuracy\": 1.0, \"loss\": 0.001642050570808351, \"time-step\": 3218}, {\"accuracy\": 1.0, \"loss\": 0.0016536952462047338, \"time-step\": 3219}, {\"accuracy\": 1.0, \"loss\": 0.0016396953724324703, \"time-step\": 3220}, {\"accuracy\": 1.0, \"loss\": 0.0016513268928974867, \"time-step\": 3221}, {\"accuracy\": 1.0, \"loss\": 0.0016373435501009226, \"time-step\": 3222}, {\"accuracy\": 1.0, \"loss\": 0.0016489820554852486, \"time-step\": 3223}, {\"accuracy\": 1.0, \"loss\": 0.001634997664950788, \"time-step\": 3224}, {\"accuracy\": 1.0, \"loss\": 0.00164663209579885, \"time-step\": 3225}, {\"accuracy\": 1.0, \"loss\": 0.001632652711123228, \"time-step\": 3226}, {\"accuracy\": 1.0, \"loss\": 0.0016442773630842566, \"time-step\": 3227}, {\"accuracy\": 1.0, \"loss\": 0.001630304497666657, \"time-step\": 3228}, {\"accuracy\": 1.0, \"loss\": 0.001641943585127592, \"time-step\": 3229}, {\"accuracy\": 1.0, \"loss\": 0.0016279809642583132, \"time-step\": 3230}, {\"accuracy\": 1.0, \"loss\": 0.0016396190039813519, \"time-step\": 3231}, {\"accuracy\": 1.0, \"loss\": 0.0016256714006885886, \"time-step\": 3232}, {\"accuracy\": 1.0, \"loss\": 0.0016373267862945795, \"time-step\": 3233}, {\"accuracy\": 1.0, \"loss\": 0.0016233902424573898, \"time-step\": 3234}, {\"accuracy\": 1.0, \"loss\": 0.0016350381774827838, \"time-step\": 3235}, {\"accuracy\": 1.0, \"loss\": 0.0016211025649681687, \"time-step\": 3236}, {\"accuracy\": 1.0, \"loss\": 0.0016327459597960114, \"time-step\": 3237}, {\"accuracy\": 1.0, \"loss\": 0.0016187970759347081, \"time-step\": 3238}, {\"accuracy\": 1.0, \"loss\": 0.0016304306918755174, \"time-step\": 3239}, {\"accuracy\": 1.0, \"loss\": 0.001616498688235879, \"time-step\": 3240}, {\"accuracy\": 1.0, \"loss\": 0.0016281392890959978, \"time-step\": 3241}, {\"accuracy\": 1.0, \"loss\": 0.001614206819795072, \"time-step\": 3242}, {\"accuracy\": 1.0, \"loss\": 0.0016258669784292579, \"time-step\": 3243}, {\"accuracy\": 1.0, \"loss\": 0.0016119652427732944, \"time-step\": 3244}, {\"accuracy\": 1.0, \"loss\": 0.0016236042138189077, \"time-step\": 3245}, {\"accuracy\": 1.0, \"loss\": 0.0016096754698082805, \"time-step\": 3246}, {\"accuracy\": 1.0, \"loss\": 0.0016213160706683993, \"time-step\": 3247}, {\"accuracy\": 1.0, \"loss\": 0.0016074135201051831, \"time-step\": 3248}, {\"accuracy\": 1.0, \"loss\": 0.0016190569149330258, \"time-step\": 3249}, {\"accuracy\": 1.0, \"loss\": 0.001605127821676433, \"time-step\": 3250}, {\"accuracy\": 1.0, \"loss\": 0.0016167645808309317, \"time-step\": 3251}, {\"accuracy\": 1.0, \"loss\": 0.0016028638929128647, \"time-step\": 3252}, {\"accuracy\": 1.0, \"loss\": 0.0016145282424986362, \"time-step\": 3253}, {\"accuracy\": 1.0, \"loss\": 0.0016006318619474769, \"time-step\": 3254}, {\"accuracy\": 1.0, \"loss\": 0.001612273626960814, \"time-step\": 3255}, {\"accuracy\": 1.0, \"loss\": 0.0015983885386958718, \"time-step\": 3256}, {\"accuracy\": 1.0, \"loss\": 0.0016100520733743906, \"time-step\": 3257}, {\"accuracy\": 1.0, \"loss\": 0.0015961696626618505, \"time-step\": 3258}, {\"accuracy\": 1.0, \"loss\": 0.0016078143380582333, \"time-step\": 3259}, {\"accuracy\": 1.0, \"loss\": 0.001593938795849681, \"time-step\": 3260}, {\"accuracy\": 1.0, \"loss\": 0.0016055733431130648, \"time-step\": 3261}, {\"accuracy\": 1.0, \"loss\": 0.0015916904667392373, \"time-step\": 3262}, {\"accuracy\": 1.0, \"loss\": 0.0016033366555348039, \"time-step\": 3263}, {\"accuracy\": 1.0, \"loss\": 0.001589477644301951, \"time-step\": 3264}, {\"accuracy\": 1.0, \"loss\": 0.0016011331463232636, \"time-step\": 3265}, {\"accuracy\": 1.0, \"loss\": 0.0015872764633968472, \"time-step\": 3266}, {\"accuracy\": 1.0, \"loss\": 0.0015989263774827123, \"time-step\": 3267}, {\"accuracy\": 1.0, \"loss\": 0.0015850886702537537, \"time-step\": 3268}, {\"accuracy\": 1.0, \"loss\": 0.0015967462677508593, \"time-step\": 3269}, {\"accuracy\": 1.0, \"loss\": 0.0015829228796064854, \"time-step\": 3270}, {\"accuracy\": 1.0, \"loss\": 0.0015945800114423037, \"time-step\": 3271}, {\"accuracy\": 1.0, \"loss\": 0.0015807372983545065, \"time-step\": 3272}, {\"accuracy\": 1.0, \"loss\": 0.0015923877945169806, \"time-step\": 3273}, {\"accuracy\": 1.0, \"loss\": 0.0015785453142598271, \"time-step\": 3274}, {\"accuracy\": 1.0, \"loss\": 0.0015901925507932901, \"time-step\": 3275}, {\"accuracy\": 1.0, \"loss\": 0.0015763709088787436, \"time-step\": 3276}, {\"accuracy\": 1.0, \"loss\": 0.0015880290884524584, \"time-step\": 3277}, {\"accuracy\": 1.0, \"loss\": 0.001574206748045981, \"time-step\": 3278}, {\"accuracy\": 1.0, \"loss\": 0.001585835823789239, \"time-step\": 3279}, {\"accuracy\": 1.0, \"loss\": 0.0015720147639513016, \"time-step\": 3280}, {\"accuracy\": 1.0, \"loss\": 0.0015836473321542144, \"time-step\": 3281}, {\"accuracy\": 1.0, \"loss\": 0.0015698176575824618, \"time-step\": 3282}, {\"accuracy\": 1.0, \"loss\": 0.0015814552316442132, \"time-step\": 3283}, {\"accuracy\": 1.0, \"loss\": 0.0015676378970965743, \"time-step\": 3284}, {\"accuracy\": 1.0, \"loss\": 0.0015792704652994871, \"time-step\": 3285}, {\"accuracy\": 1.0, \"loss\": 0.0015654738526791334, \"time-step\": 3286}, {\"accuracy\": 1.0, \"loss\": 0.0015771127073094249, \"time-step\": 3287}, {\"accuracy\": 1.0, \"loss\": 0.0015633145812898874, \"time-step\": 3288}, {\"accuracy\": 1.0, \"loss\": 0.0015749535523355007, \"time-step\": 3289}, {\"accuracy\": 1.0, \"loss\": 0.001561174402013421, \"time-step\": 3290}, {\"accuracy\": 1.0, \"loss\": 0.00157280417624861, \"time-step\": 3291}, {\"accuracy\": 1.0, \"loss\": 0.0015590329421684146, \"time-step\": 3292}, {\"accuracy\": 1.0, \"loss\": 0.0015706735430285335, \"time-step\": 3293}, {\"accuracy\": 1.0, \"loss\": 0.0015569082461297512, \"time-step\": 3294}, {\"accuracy\": 1.0, \"loss\": 0.0015685680555179715, \"time-step\": 3295}, {\"accuracy\": 1.0, \"loss\": 0.001554800313897431, \"time-step\": 3296}, {\"accuracy\": 1.0, \"loss\": 0.0015664236852899194, \"time-step\": 3297}, {\"accuracy\": 1.0, \"loss\": 0.001552653149701655, \"time-step\": 3298}, {\"accuracy\": 1.0, \"loss\": 0.0015642894431948662, \"time-step\": 3299}, {\"accuracy\": 1.0, \"loss\": 0.0015505403280258179, \"time-step\": 3300}, {\"accuracy\": 1.0, \"loss\": 0.001562178716994822, \"time-step\": 3301}, {\"accuracy\": 1.0, \"loss\": 0.0015484399627894163, \"time-step\": 3302}, {\"accuracy\": 1.0, \"loss\": 0.0015600643819198012, \"time-step\": 3303}, {\"accuracy\": 1.0, \"loss\": 0.0015463257441297174, \"time-step\": 3304}, {\"accuracy\": 1.0, \"loss\": 0.0015579639002680779, \"time-step\": 3305}, {\"accuracy\": 1.0, \"loss\": 0.0015442389994859695, \"time-step\": 3306}, {\"accuracy\": 1.0, \"loss\": 0.0015558856539428234, \"time-step\": 3307}, {\"accuracy\": 1.0, \"loss\": 0.001542133861221373, \"time-step\": 3308}, {\"accuracy\": 1.0, \"loss\": 0.001553744776174426, \"time-step\": 3309}, {\"accuracy\": 1.0, \"loss\": 0.001540014985948801, \"time-step\": 3310}, {\"accuracy\": 1.0, \"loss\": 0.0015516416169703007, \"time-step\": 3311}, {\"accuracy\": 1.0, \"loss\": 0.0015379241667687893, \"time-step\": 3312}, {\"accuracy\": 1.0, \"loss\": 0.0015495435800403357, \"time-step\": 3313}, {\"accuracy\": 1.0, \"loss\": 0.0015358352102339268, \"time-step\": 3314}, {\"accuracy\": 1.0, \"loss\": 0.0015474720858037472, \"time-step\": 3315}, {\"accuracy\": 1.0, \"loss\": 0.0015337755903601646, \"time-step\": 3316}, {\"accuracy\": 1.0, \"loss\": 0.0015454036183655262, \"time-step\": 3317}, {\"accuracy\": 1.0, \"loss\": 0.0015317066572606564, \"time-step\": 3318}, {\"accuracy\": 1.0, \"loss\": 0.0015433066291734576, \"time-step\": 3319}, {\"accuracy\": 1.0, \"loss\": 0.001529612229205668, \"time-step\": 3320}, {\"accuracy\": 1.0, \"loss\": 0.0015412268694490194, \"time-step\": 3321}, {\"accuracy\": 1.0, \"loss\": 0.0015275445766746998, \"time-step\": 3322}, {\"accuracy\": 1.0, \"loss\": 0.0015391585184261203, \"time-step\": 3323}, {\"accuracy\": 1.0, \"loss\": 0.0015254870522767305, \"time-step\": 3324}, {\"accuracy\": 1.0, \"loss\": 0.0015371001791208982, \"time-step\": 3325}, {\"accuracy\": 1.0, \"loss\": 0.001523416955024004, \"time-step\": 3326}, {\"accuracy\": 1.0, \"loss\": 0.0015350303146988153, \"time-step\": 3327}, {\"accuracy\": 1.0, \"loss\": 0.0015213710721582174, \"time-step\": 3328}, {\"accuracy\": 1.0, \"loss\": 0.001532972790300846, \"time-step\": 3329}, {\"accuracy\": 1.0, \"loss\": 0.001519326469860971, \"time-step\": 3330}, {\"accuracy\": 1.0, \"loss\": 0.0015309310983866453, \"time-step\": 3331}, {\"accuracy\": 1.0, \"loss\": 0.0015172867570072412, \"time-step\": 3332}, {\"accuracy\": 1.0, \"loss\": 0.00152888847514987, \"time-step\": 3333}, {\"accuracy\": 1.0, \"loss\": 0.0015152439009398222, \"time-step\": 3334}, {\"accuracy\": 1.0, \"loss\": 0.0015268486458808184, \"time-step\": 3335}, {\"accuracy\": 1.0, \"loss\": 0.0015132217667996883, \"time-step\": 3336}, {\"accuracy\": 1.0, \"loss\": 0.0015248210402205586, \"time-step\": 3337}, {\"accuracy\": 1.0, \"loss\": 0.0015112077817320824, \"time-step\": 3338}, {\"accuracy\": 1.0, \"loss\": 0.0015228043776005507, \"time-step\": 3339}, {\"accuracy\": 1.0, \"loss\": 0.0015091815730556846, \"time-step\": 3340}, {\"accuracy\": 1.0, \"loss\": 0.0015207899268716574, \"time-step\": 3341}, {\"accuracy\": 1.0, \"loss\": 0.001507206354290247, \"time-step\": 3342}, {\"accuracy\": 1.0, \"loss\": 0.0015188142424449325, \"time-step\": 3343}, {\"accuracy\": 1.0, \"loss\": 0.0015052203088998795, \"time-step\": 3344}, {\"accuracy\": 1.0, \"loss\": 0.0015168088721111417, \"time-step\": 3345}, {\"accuracy\": 1.0, \"loss\": 0.001503228791989386, \"time-step\": 3346}, {\"accuracy\": 1.0, \"loss\": 0.0015148217789828777, \"time-step\": 3347}, {\"accuracy\": 1.0, \"loss\": 0.0015012349467724562, \"time-step\": 3348}, {\"accuracy\": 1.0, \"loss\": 0.0015128140803426504, \"time-step\": 3349}, {\"accuracy\": 1.0, \"loss\": 0.001499236561357975, \"time-step\": 3350}, {\"accuracy\": 1.0, \"loss\": 0.0015108007937669754, \"time-step\": 3351}, {\"accuracy\": 1.0, \"loss\": 0.0014972238568589091, \"time-step\": 3352}, {\"accuracy\": 1.0, \"loss\": 0.0015088203363120556, \"time-step\": 3353}, {\"accuracy\": 1.0, \"loss\": 0.0014952573692426085, \"time-step\": 3354}, {\"accuracy\": 1.0, \"loss\": 0.0015068587381392717, \"time-step\": 3355}, {\"accuracy\": 1.0, \"loss\": 0.0014932984486222267, \"time-step\": 3356}, {\"accuracy\": 1.0, \"loss\": 0.0015048796776682138, \"time-step\": 3357}, {\"accuracy\": 1.0, \"loss\": 0.0014913248596712947, \"time-step\": 3358}, {\"accuracy\": 1.0, \"loss\": 0.0015028980560600758, \"time-step\": 3359}, {\"accuracy\": 1.0, \"loss\": 0.0014893494080752134, \"time-step\": 3360}, {\"accuracy\": 1.0, \"loss\": 0.0015009242342785, \"time-step\": 3361}, {\"accuracy\": 1.0, \"loss\": 0.0014874018961563706, \"time-step\": 3362}, {\"accuracy\": 1.0, \"loss\": 0.001498991739936173, \"time-step\": 3363}, {\"accuracy\": 1.0, \"loss\": 0.0014854773180559278, \"time-step\": 3364}, {\"accuracy\": 1.0, \"loss\": 0.0014970381744205952, \"time-step\": 3365}, {\"accuracy\": 1.0, \"loss\": 0.0014835086185485125, \"time-step\": 3366}, {\"accuracy\": 1.0, \"loss\": 0.001495092874392867, \"time-step\": 3367}, {\"accuracy\": 1.0, \"loss\": 0.0014815789181739092, \"time-step\": 3368}, {\"accuracy\": 1.0, \"loss\": 0.001493134768679738, \"time-step\": 3369}, {\"accuracy\": 1.0, \"loss\": 0.0014796152245253325, \"time-step\": 3370}, {\"accuracy\": 1.0, \"loss\": 0.0014911845792084932, \"time-step\": 3371}, {\"accuracy\": 1.0, \"loss\": 0.001477676210924983, \"time-step\": 3372}, {\"accuracy\": 1.0, \"loss\": 0.0014892322942614555, \"time-step\": 3373}, {\"accuracy\": 1.0, \"loss\": 0.0014757139142602682, \"time-step\": 3374}, {\"accuracy\": 1.0, \"loss\": 0.0014872595202177763, \"time-step\": 3375}, {\"accuracy\": 1.0, \"loss\": 0.001473776064813137, \"time-step\": 3376}, {\"accuracy\": 1.0, \"loss\": 0.0014853384345769882, \"time-step\": 3377}, {\"accuracy\": 1.0, \"loss\": 0.0014718727907165885, \"time-step\": 3378}, {\"accuracy\": 1.0, \"loss\": 0.0014834379544481635, \"time-step\": 3379}, {\"accuracy\": 1.0, \"loss\": 0.0014699682360514998, \"time-step\": 3380}, {\"accuracy\": 1.0, \"loss\": 0.0014815120957791805, \"time-step\": 3381}, {\"accuracy\": 1.0, \"loss\": 0.001468044356442988, \"time-step\": 3382}, {\"accuracy\": 1.0, \"loss\": 0.0014796024188399315, \"time-step\": 3383}, {\"accuracy\": 1.0, \"loss\": 0.0014661470195278525, \"time-step\": 3384}, {\"accuracy\": 1.0, \"loss\": 0.0014776811003684998, \"time-step\": 3385}, {\"accuracy\": 1.0, \"loss\": 0.0014642225578427315, \"time-step\": 3386}, {\"accuracy\": 1.0, \"loss\": 0.0014757819008082151, \"time-step\": 3387}, {\"accuracy\": 1.0, \"loss\": 0.0014623505994677544, \"time-step\": 3388}, {\"accuracy\": 1.0, \"loss\": 0.0014738764148205519, \"time-step\": 3389}, {\"accuracy\": 1.0, \"loss\": 0.001460439758375287, \"time-step\": 3390}, {\"accuracy\": 1.0, \"loss\": 0.0014719886239618063, \"time-step\": 3391}, {\"accuracy\": 1.0, \"loss\": 0.001458565704524517, \"time-step\": 3392}, {\"accuracy\": 1.0, \"loss\": 0.0014701081672683358, \"time-step\": 3393}, {\"accuracy\": 1.0, \"loss\": 0.0014566982863470912, \"time-step\": 3394}, {\"accuracy\": 1.0, \"loss\": 0.001468238653615117, \"time-step\": 3395}, {\"accuracy\": 1.0, \"loss\": 0.0014548420440405607, \"time-step\": 3396}, {\"accuracy\": 1.0, \"loss\": 0.0014663643669337034, \"time-step\": 3397}, {\"accuracy\": 1.0, \"loss\": 0.0014529420295730233, \"time-step\": 3398}, {\"accuracy\": 1.0, \"loss\": 0.0014644695911556482, \"time-step\": 3399}, {\"accuracy\": 1.0, \"loss\": 0.0014510753098875284, \"time-step\": 3400}, {\"accuracy\": 1.0, \"loss\": 0.001462601707316935, \"time-step\": 3401}, {\"accuracy\": 1.0, \"loss\": 0.0014491898473352194, \"time-step\": 3402}, {\"accuracy\": 1.0, \"loss\": 0.0014606999466195703, \"time-step\": 3403}, {\"accuracy\": 1.0, \"loss\": 0.0014473185874521732, \"time-step\": 3404}, {\"accuracy\": 1.0, \"loss\": 0.0014588258927688003, \"time-step\": 3405}, {\"accuracy\": 1.0, \"loss\": 0.0014454449992626905, \"time-step\": 3406}, {\"accuracy\": 1.0, \"loss\": 0.0014569568447768688, \"time-step\": 3407}, {\"accuracy\": 1.0, \"loss\": 0.0014436119236052036, \"time-step\": 3408}, {\"accuracy\": 1.0, \"loss\": 0.0014551192289218307, \"time-step\": 3409}, {\"accuracy\": 1.0, \"loss\": 0.0014417648781090975, \"time-step\": 3410}, {\"accuracy\": 1.0, \"loss\": 0.0014532679924741387, \"time-step\": 3411}, {\"accuracy\": 1.0, \"loss\": 0.0014399162027984858, \"time-step\": 3412}, {\"accuracy\": 1.0, \"loss\": 0.0014514288632199168, \"time-step\": 3413}, {\"accuracy\": 1.0, \"loss\": 0.0014380875509232283, \"time-step\": 3414}, {\"accuracy\": 1.0, \"loss\": 0.0014495959039777517, \"time-step\": 3415}, {\"accuracy\": 1.0, \"loss\": 0.00143627414945513, \"time-step\": 3416}, {\"accuracy\": 1.0, \"loss\": 0.0014477785443887115, \"time-step\": 3417}, {\"accuracy\": 1.0, \"loss\": 0.0014344500377774239, \"time-step\": 3418}, {\"accuracy\": 1.0, \"loss\": 0.0014459622325375676, \"time-step\": 3419}, {\"accuracy\": 1.0, \"loss\": 0.0014326469972729683, \"time-step\": 3420}, {\"accuracy\": 1.0, \"loss\": 0.0014441416133195162, \"time-step\": 3421}, {\"accuracy\": 1.0, \"loss\": 0.00143082020804286, \"time-step\": 3422}, {\"accuracy\": 1.0, \"loss\": 0.0014423119137063622, \"time-step\": 3423}, {\"accuracy\": 1.0, \"loss\": 0.0014289880637079477, \"time-step\": 3424}, {\"accuracy\": 1.0, \"loss\": 0.0014404519461095333, \"time-step\": 3425}, {\"accuracy\": 1.0, \"loss\": 0.0014271646505221725, \"time-step\": 3426}, {\"accuracy\": 1.0, \"loss\": 0.0014386478578671813, \"time-step\": 3427}, {\"accuracy\": 1.0, \"loss\": 0.0014253546250984073, \"time-step\": 3428}, {\"accuracy\": 1.0, \"loss\": 0.0014368250267580152, \"time-step\": 3429}, {\"accuracy\": 1.0, \"loss\": 0.0014235409907996655, \"time-step\": 3430}, {\"accuracy\": 1.0, \"loss\": 0.0014350209385156631, \"time-step\": 3431}, {\"accuracy\": 1.0, \"loss\": 0.0014217477291822433, \"time-step\": 3432}, {\"accuracy\": 1.0, \"loss\": 0.001433228375390172, \"time-step\": 3433}, {\"accuracy\": 1.0, \"loss\": 0.0014199672732502222, \"time-step\": 3434}, {\"accuracy\": 1.0, \"loss\": 0.0014314409345388412, \"time-step\": 3435}, {\"accuracy\": 1.0, \"loss\": 0.0014181819278746843, \"time-step\": 3436}, {\"accuracy\": 1.0, \"loss\": 0.0014296460431069136, \"time-step\": 3437}, {\"accuracy\": 1.0, \"loss\": 0.0014163752784952521, \"time-step\": 3438}, {\"accuracy\": 1.0, \"loss\": 0.0014278489397838712, \"time-step\": 3439}, {\"accuracy\": 1.0, \"loss\": 0.0014146158937364817, \"time-step\": 3440}, {\"accuracy\": 1.0, \"loss\": 0.001426072558388114, \"time-step\": 3441}, {\"accuracy\": 1.0, \"loss\": 0.0014128397451713681, \"time-step\": 3442}, {\"accuracy\": 1.0, \"loss\": 0.0014243177138268948, \"time-step\": 3443}, {\"accuracy\": 1.0, \"loss\": 0.0014110981719568372, \"time-step\": 3444}, {\"accuracy\": 1.0, \"loss\": 0.0014225514605641365, \"time-step\": 3445}, {\"accuracy\": 1.0, \"loss\": 0.0014093281934037805, \"time-step\": 3446}, {\"accuracy\": 1.0, \"loss\": 0.0014207581989467144, \"time-step\": 3447}, {\"accuracy\": 1.0, \"loss\": 0.0014075370272621512, \"time-step\": 3448}, {\"accuracy\": 1.0, \"loss\": 0.001418967847712338, \"time-step\": 3449}, {\"accuracy\": 1.0, \"loss\": 0.0014057679800316691, \"time-step\": 3450}, {\"accuracy\": 1.0, \"loss\": 0.0014172064838930964, \"time-step\": 3451}, {\"accuracy\": 1.0, \"loss\": 0.0014040111564099789, \"time-step\": 3452}, {\"accuracy\": 1.0, \"loss\": 0.0014154566451907158, \"time-step\": 3453}, {\"accuracy\": 1.0, \"loss\": 0.001402268884703517, \"time-step\": 3454}, {\"accuracy\": 1.0, \"loss\": 0.0014136950485408306, \"time-step\": 3455}, {\"accuracy\": 1.0, \"loss\": 0.0014005025150254369, \"time-step\": 3456}, {\"accuracy\": 1.0, \"loss\": 0.001411918899975717, \"time-step\": 3457}, {\"accuracy\": 1.0, \"loss\": 0.0013987441780045629, \"time-step\": 3458}, {\"accuracy\": 1.0, \"loss\": 0.0014101800043135881, \"time-step\": 3459}, {\"accuracy\": 1.0, \"loss\": 0.0013970289146527648, \"time-step\": 3460}, {\"accuracy\": 1.0, \"loss\": 0.001408467418514192, \"time-step\": 3461}, {\"accuracy\": 1.0, \"loss\": 0.0013953252928331494, \"time-step\": 3462}, {\"accuracy\": 1.0, \"loss\": 0.0014067522715777159, \"time-step\": 3463}, {\"accuracy\": 1.0, \"loss\": 0.0013935990864410996, \"time-step\": 3464}, {\"accuracy\": 1.0, \"loss\": 0.0014049950987100601, \"time-step\": 3465}, {\"accuracy\": 1.0, \"loss\": 0.0013918656622990966, \"time-step\": 3466}, {\"accuracy\": 1.0, \"loss\": 0.0014032532926648855, \"time-step\": 3467}, {\"accuracy\": 1.0, \"loss\": 0.0013901228085160255, \"time-step\": 3468}, {\"accuracy\": 1.0, \"loss\": 0.0014015438500791788, \"time-step\": 3469}, {\"accuracy\": 1.0, \"loss\": 0.0013884170912206173, \"time-step\": 3470}, {\"accuracy\": 1.0, \"loss\": 0.0013998195063322783, \"time-step\": 3471}, {\"accuracy\": 1.0, \"loss\": 0.0013867092784494162, \"time-step\": 3472}, {\"accuracy\": 1.0, \"loss\": 0.001398110412992537, \"time-step\": 3473}, {\"accuracy\": 1.0, \"loss\": 0.0013850087998434901, \"time-step\": 3474}, {\"accuracy\": 1.0, \"loss\": 0.0013964009704068303, \"time-step\": 3475}, {\"accuracy\": 1.0, \"loss\": 0.001383304945193231, \"time-step\": 3476}, {\"accuracy\": 1.0, \"loss\": 0.001394694671034813, \"time-step\": 3477}, {\"accuracy\": 1.0, \"loss\": 0.0013816035352647305, \"time-step\": 3478}, {\"accuracy\": 1.0, \"loss\": 0.001392989419400692, \"time-step\": 3479}, {\"accuracy\": 1.0, \"loss\": 0.001379899913445115, \"time-step\": 3480}, {\"accuracy\": 1.0, \"loss\": 0.001391278812661767, \"time-step\": 3481}, {\"accuracy\": 1.0, \"loss\": 0.0013782150344923139, \"time-step\": 3482}, {\"accuracy\": 1.0, \"loss\": 0.0013896033633500338, \"time-step\": 3483}, {\"accuracy\": 1.0, \"loss\": 0.0013765289913862944, \"time-step\": 3484}, {\"accuracy\": 1.0, \"loss\": 0.0013878940371796489, \"time-step\": 3485}, {\"accuracy\": 1.0, \"loss\": 0.001374833402223885, \"time-step\": 3486}, {\"accuracy\": 1.0, \"loss\": 0.001386200892738998, \"time-step\": 3487}, {\"accuracy\": 1.0, \"loss\": 0.0013731600483879447, \"time-step\": 3488}, {\"accuracy\": 1.0, \"loss\": 0.0013845362700521946, \"time-step\": 3489}, {\"accuracy\": 1.0, \"loss\": 0.0013715021777898073, \"time-step\": 3490}, {\"accuracy\": 1.0, \"loss\": 0.0013828511582687497, \"time-step\": 3491}, {\"accuracy\": 1.0, \"loss\": 0.0013697999529540539, \"time-step\": 3492}, {\"accuracy\": 1.0, \"loss\": 0.0013811506796628237, \"time-step\": 3493}, {\"accuracy\": 1.0, \"loss\": 0.0013681091368198395, \"time-step\": 3494}, {\"accuracy\": 1.0, \"loss\": 0.0013794514816254377, \"time-step\": 3495}, {\"accuracy\": 1.0, \"loss\": 0.0013664327561855316, \"time-step\": 3496}, {\"accuracy\": 1.0, \"loss\": 0.001377797918394208, \"time-step\": 3497}, {\"accuracy\": 1.0, \"loss\": 0.001364795956760645, \"time-step\": 3498}, {\"accuracy\": 1.0, \"loss\": 0.0013761401642113924, \"time-step\": 3499}, {\"accuracy\": 1.0, \"loss\": 0.0013631407637149096, \"time-step\": 3500}, {\"accuracy\": 1.0, \"loss\": 0.0013744793832302094, \"time-step\": 3501}, {\"accuracy\": 1.0, \"loss\": 0.001361468224786222, \"time-step\": 3502}, {\"accuracy\": 1.0, \"loss\": 0.0013727971818298101, \"time-step\": 3503}, {\"accuracy\": 1.0, \"loss\": 0.0013598211808130145, \"time-step\": 3504}, {\"accuracy\": 1.0, \"loss\": 0.0013711692299693823, \"time-step\": 3505}, {\"accuracy\": 1.0, \"loss\": 0.0013581773964688182, \"time-step\": 3506}, {\"accuracy\": 1.0, \"loss\": 0.0013694815570488572, \"time-step\": 3507}, {\"accuracy\": 1.0, \"loss\": 0.0013565149856731296, \"time-step\": 3508}, {\"accuracy\": 1.0, \"loss\": 0.0013678499963134527, \"time-step\": 3509}, {\"accuracy\": 1.0, \"loss\": 0.0013548913411796093, \"time-step\": 3510}, {\"accuracy\": 1.0, \"loss\": 0.0013662023702636361, \"time-step\": 3511}, {\"accuracy\": 1.0, \"loss\": 0.0013532472075894475, \"time-step\": 3512}, {\"accuracy\": 1.0, \"loss\": 0.0013645566068589687, \"time-step\": 3513}, {\"accuracy\": 1.0, \"loss\": 0.0013516235630959272, \"time-step\": 3514}, {\"accuracy\": 1.0, \"loss\": 0.0013629476306959987, \"time-step\": 3515}, {\"accuracy\": 1.0, \"loss\": 0.0013500119093805552, \"time-step\": 3516}, {\"accuracy\": 1.0, \"loss\": 0.0013613090850412846, \"time-step\": 3517}, {\"accuracy\": 1.0, \"loss\": 0.0013483890797942877, \"time-step\": 3518}, {\"accuracy\": 1.0, \"loss\": 0.0013596899807453156, \"time-step\": 3519}, {\"accuracy\": 1.0, \"loss\": 0.00134676368907094, \"time-step\": 3520}, {\"accuracy\": 1.0, \"loss\": 0.0013580742524936795, \"time-step\": 3521}, {\"accuracy\": 1.0, \"loss\": 0.0013451738050207496, \"time-step\": 3522}, {\"accuracy\": 1.0, \"loss\": 0.0013564556138589978, \"time-step\": 3523}, {\"accuracy\": 1.0, \"loss\": 0.0013435587752610445, \"time-step\": 3524}, {\"accuracy\": 1.0, \"loss\": 0.0013548440765589476, \"time-step\": 3525}, {\"accuracy\": 1.0, \"loss\": 0.0013419545721262693, \"time-step\": 3526}, {\"accuracy\": 1.0, \"loss\": 0.001353218569420278, \"time-step\": 3527}, {\"accuracy\": 1.0, \"loss\": 0.0013403117191046476, \"time-step\": 3528}, {\"accuracy\": 1.0, \"loss\": 0.0013515852624550462, \"time-step\": 3529}, {\"accuracy\": 1.0, \"loss\": 0.001338711939752102, \"time-step\": 3530}, {\"accuracy\": 1.0, \"loss\": 0.0013499828055500984, \"time-step\": 3531}, {\"accuracy\": 1.0, \"loss\": 0.0013371057575568557, \"time-step\": 3532}, {\"accuracy\": 1.0, \"loss\": 0.0013483846560120583, \"time-step\": 3533}, {\"accuracy\": 1.0, \"loss\": 0.001335512613877654, \"time-step\": 3534}, {\"accuracy\": 1.0, \"loss\": 0.0013467730022966862, \"time-step\": 3535}, {\"accuracy\": 1.0, \"loss\": 0.0013339078286662698, \"time-step\": 3536}, {\"accuracy\": 1.0, \"loss\": 0.0013451724080368876, \"time-step\": 3537}, {\"accuracy\": 1.0, \"loss\": 0.0013323166640475392, \"time-step\": 3538}, {\"accuracy\": 1.0, \"loss\": 0.0013435757718980312, \"time-step\": 3539}, {\"accuracy\": 1.0, \"loss\": 0.0013307308545336127, \"time-step\": 3540}, {\"accuracy\": 1.0, \"loss\": 0.0013419734314084053, \"time-step\": 3541}, {\"accuracy\": 1.0, \"loss\": 0.0013291207142174244, \"time-step\": 3542}, {\"accuracy\": 1.0, \"loss\": 0.0013403664343059063, \"time-step\": 3543}, {\"accuracy\": 1.0, \"loss\": 0.0013275350211188197, \"time-step\": 3544}, {\"accuracy\": 1.0, \"loss\": 0.0013387692160904408, \"time-step\": 3545}, {\"accuracy\": 1.0, \"loss\": 0.001325942692346871, \"time-step\": 3546}, {\"accuracy\": 1.0, \"loss\": 0.0013371846871450543, \"time-step\": 3547}, {\"accuracy\": 1.0, \"loss\": 0.0013243877328932285, \"time-step\": 3548}, {\"accuracy\": 1.0, \"loss\": 0.0013356130803003907, \"time-step\": 3549}, {\"accuracy\": 1.0, \"loss\": 0.0013228198513388634, \"time-step\": 3550}, {\"accuracy\": 1.0, \"loss\": 0.0013340465957298875, \"time-step\": 3551}, {\"accuracy\": 1.0, \"loss\": 0.001321268267929554, \"time-step\": 3552}, {\"accuracy\": 1.0, \"loss\": 0.0013324939645826817, \"time-step\": 3553}, {\"accuracy\": 1.0, \"loss\": 0.0013197122607380152, \"time-step\": 3554}, {\"accuracy\": 1.0, \"loss\": 0.0013309404021129012, \"time-step\": 3555}, {\"accuracy\": 1.0, \"loss\": 0.001318173948675394, \"time-step\": 3556}, {\"accuracy\": 1.0, \"loss\": 0.0013293843949213624, \"time-step\": 3557}, {\"accuracy\": 1.0, \"loss\": 0.0013166344724595547, \"time-step\": 3558}, {\"accuracy\": 1.0, \"loss\": 0.0013278574915602803, \"time-step\": 3559}, {\"accuracy\": 1.0, \"loss\": 0.0013151053572073579, \"time-step\": 3560}, {\"accuracy\": 1.0, \"loss\": 0.0013263099826872349, \"time-step\": 3561}, {\"accuracy\": 1.0, \"loss\": 0.0013135606423020363, \"time-step\": 3562}, {\"accuracy\": 1.0, \"loss\": 0.0013247665483504534, \"time-step\": 3563}, {\"accuracy\": 1.0, \"loss\": 0.0013120385119691491, \"time-step\": 3564}, {\"accuracy\": 1.0, \"loss\": 0.0013232443016022444, \"time-step\": 3565}, {\"accuracy\": 1.0, \"loss\": 0.001310508931055665, \"time-step\": 3566}, {\"accuracy\": 1.0, \"loss\": 0.001321702729910612, \"time-step\": 3567}, {\"accuracy\": 1.0, \"loss\": 0.0013089888961985707, \"time-step\": 3568}, {\"accuracy\": 1.0, \"loss\": 0.001320180483162403, \"time-step\": 3569}, {\"accuracy\": 1.0, \"loss\": 0.0013074371963739395, \"time-step\": 3570}, {\"accuracy\": 1.0, \"loss\": 0.0013186169089749455, \"time-step\": 3571}, {\"accuracy\": 1.0, \"loss\": 0.0013059088960289955, \"time-step\": 3572}, {\"accuracy\": 1.0, \"loss\": 0.00131707894615829, \"time-step\": 3573}, {\"accuracy\": 1.0, \"loss\": 0.0013043757062405348, \"time-step\": 3574}, {\"accuracy\": 1.0, \"loss\": 0.0013155498309060931, \"time-step\": 3575}, {\"accuracy\": 1.0, \"loss\": 0.0013028663815930486, \"time-step\": 3576}, {\"accuracy\": 1.0, \"loss\": 0.0013140320079401135, \"time-step\": 3577}, {\"accuracy\": 1.0, \"loss\": 0.0013013456482440233, \"time-step\": 3578}, {\"accuracy\": 1.0, \"loss\": 0.0013125346740707755, \"time-step\": 3579}, {\"accuracy\": 1.0, \"loss\": 0.0012998601887375116, \"time-step\": 3580}, {\"accuracy\": 1.0, \"loss\": 0.001311019528657198, \"time-step\": 3581}, {\"accuracy\": 1.0, \"loss\": 0.0012983487686142325, \"time-step\": 3582}, {\"accuracy\": 1.0, \"loss\": 0.0013094936730340123, \"time-step\": 3583}, {\"accuracy\": 1.0, \"loss\": 0.00129684095736593, \"time-step\": 3584}, {\"accuracy\": 1.0, \"loss\": 0.0013079952914267778, \"time-step\": 3585}, {\"accuracy\": 1.0, \"loss\": 0.0012953504920005798, \"time-step\": 3586}, {\"accuracy\": 1.0, \"loss\": 0.0013064935337752104, \"time-step\": 3587}, {\"accuracy\": 1.0, \"loss\": 0.0012938342988491058, \"time-step\": 3588}, {\"accuracy\": 1.0, \"loss\": 0.0013049656990915537, \"time-step\": 3589}, {\"accuracy\": 1.0, \"loss\": 0.0012923481408506632, \"time-step\": 3590}, {\"accuracy\": 1.0, \"loss\": 0.0013034716248512268, \"time-step\": 3591}, {\"accuracy\": 1.0, \"loss\": 0.0012908498756587505, \"time-step\": 3592}, {\"accuracy\": 1.0, \"loss\": 0.0013019756879657507, \"time-step\": 3593}, {\"accuracy\": 1.0, \"loss\": 0.0012893578968942165, \"time-step\": 3594}, {\"accuracy\": 1.0, \"loss\": 0.0013004784705117345, \"time-step\": 3595}, {\"accuracy\": 1.0, \"loss\": 0.0012878651032224298, \"time-step\": 3596}, {\"accuracy\": 1.0, \"loss\": 0.001298982067964971, \"time-step\": 3597}, {\"accuracy\": 1.0, \"loss\": 0.0012863855808973312, \"time-step\": 3598}, {\"accuracy\": 1.0, \"loss\": 0.0012974976561963558, \"time-step\": 3599}, {\"accuracy\": 1.0, \"loss\": 0.0012849062914028764, \"time-step\": 3600}, {\"accuracy\": 1.0, \"loss\": 0.0012960094027221203, \"time-step\": 3601}, {\"accuracy\": 1.0, \"loss\": 0.0012834154767915606, \"time-step\": 3602}, {\"accuracy\": 1.0, \"loss\": 0.0012945152120664716, \"time-step\": 3603}, {\"accuracy\": 1.0, \"loss\": 0.001281948760151863, \"time-step\": 3604}, {\"accuracy\": 1.0, \"loss\": 0.0012930587399750948, \"time-step\": 3605}, {\"accuracy\": 1.0, \"loss\": 0.001280483789741993, \"time-step\": 3606}, {\"accuracy\": 1.0, \"loss\": 0.0012915784027427435, \"time-step\": 3607}, {\"accuracy\": 1.0, \"loss\": 0.001279023359529674, \"time-step\": 3608}, {\"accuracy\": 1.0, \"loss\": 0.0012901087757200003, \"time-step\": 3609}, {\"accuracy\": 1.0, \"loss\": 0.0012775667710229754, \"time-step\": 3610}, {\"accuracy\": 1.0, \"loss\": 0.0012886542826890945, \"time-step\": 3611}, {\"accuracy\": 1.0, \"loss\": 0.001276102033443749, \"time-step\": 3612}, {\"accuracy\": 1.0, \"loss\": 0.0012871809303760529, \"time-step\": 3613}, {\"accuracy\": 1.0, \"loss\": 0.0012746411375701427, \"time-step\": 3614}, {\"accuracy\": 1.0, \"loss\": 0.0012857213150709867, \"time-step\": 3615}, {\"accuracy\": 1.0, \"loss\": 0.0012731817550957203, \"time-step\": 3616}, {\"accuracy\": 1.0, \"loss\": 0.0012842429568991065, \"time-step\": 3617}, {\"accuracy\": 1.0, \"loss\": 0.001271724933758378, \"time-step\": 3618}, {\"accuracy\": 1.0, \"loss\": 0.0012827870668843389, \"time-step\": 3619}, {\"accuracy\": 1.0, \"loss\": 0.0012702904641628265, \"time-step\": 3620}, {\"accuracy\": 1.0, \"loss\": 0.0012813466601073742, \"time-step\": 3621}, {\"accuracy\": 1.0, \"loss\": 0.0012688601855188608, \"time-step\": 3622}, {\"accuracy\": 1.0, \"loss\": 0.001279917429201305, \"time-step\": 3623}, {\"accuracy\": 1.0, \"loss\": 0.001267412444576621, \"time-step\": 3624}, {\"accuracy\": 1.0, \"loss\": 0.0012784514110535383, \"time-step\": 3625}, {\"accuracy\": 1.0, \"loss\": 0.0012659691274166107, \"time-step\": 3626}, {\"accuracy\": 1.0, \"loss\": 0.0012770143803209066, \"time-step\": 3627}, {\"accuracy\": 1.0, \"loss\": 0.0012645386159420013, \"time-step\": 3628}, {\"accuracy\": 1.0, \"loss\": 0.0012755682691931725, \"time-step\": 3629}, {\"accuracy\": 1.0, \"loss\": 0.001263116137124598, \"time-step\": 3630}, {\"accuracy\": 1.0, \"loss\": 0.001274142530746758, \"time-step\": 3631}, {\"accuracy\": 1.0, \"loss\": 0.001261683413758874, \"time-step\": 3632}, {\"accuracy\": 1.0, \"loss\": 0.001272721216082573, \"time-step\": 3633}, {\"accuracy\": 1.0, \"loss\": 0.0012602759525179863, \"time-step\": 3634}, {\"accuracy\": 1.0, \"loss\": 0.0012713068863376975, \"time-step\": 3635}, {\"accuracy\": 1.0, \"loss\": 0.0012588606914505363, \"time-step\": 3636}, {\"accuracy\": 1.0, \"loss\": 0.0012698826612904668, \"time-step\": 3637}, {\"accuracy\": 1.0, \"loss\": 0.0012574568390846252, \"time-step\": 3638}, {\"accuracy\": 1.0, \"loss\": 0.0012684566900134087, \"time-step\": 3639}, {\"accuracy\": 1.0, \"loss\": 0.0012560230679810047, \"time-step\": 3640}, {\"accuracy\": 1.0, \"loss\": 0.0012670382857322693, \"time-step\": 3641}, {\"accuracy\": 1.0, \"loss\": 0.0012546285288408399, \"time-step\": 3642}, {\"accuracy\": 1.0, \"loss\": 0.001265642698854208, \"time-step\": 3643}, {\"accuracy\": 1.0, \"loss\": 0.001253235386684537, \"time-step\": 3644}, {\"accuracy\": 1.0, \"loss\": 0.0012642326764762402, \"time-step\": 3645}, {\"accuracy\": 1.0, \"loss\": 0.001251837587915361, \"time-step\": 3646}, {\"accuracy\": 1.0, \"loss\": 0.0012628164840862155, \"time-step\": 3647}, {\"accuracy\": 1.0, \"loss\": 0.0012504213955253363, \"time-step\": 3648}, {\"accuracy\": 1.0, \"loss\": 0.0012614191509783268, \"time-step\": 3649}, {\"accuracy\": 1.0, \"loss\": 0.0012490497902035713, \"time-step\": 3650}, {\"accuracy\": 1.0, \"loss\": 0.0012600217014551163, \"time-step\": 3651}, {\"accuracy\": 1.0, \"loss\": 0.0012476402334868908, \"time-step\": 3652}, {\"accuracy\": 1.0, \"loss\": 0.0012586137745529413, \"time-step\": 3653}, {\"accuracy\": 1.0, \"loss\": 0.001246249070391059, \"time-step\": 3654}, {\"accuracy\": 1.0, \"loss\": 0.001257222960703075, \"time-step\": 3655}, {\"accuracy\": 1.0, \"loss\": 0.0012448383495211601, \"time-step\": 3656}, {\"accuracy\": 1.0, \"loss\": 0.0012557804584503174, \"time-step\": 3657}, {\"accuracy\": 1.0, \"loss\": 0.0012434307718649507, \"time-step\": 3658}, {\"accuracy\": 1.0, \"loss\": 0.0012543940683826804, \"time-step\": 3659}, {\"accuracy\": 1.0, \"loss\": 0.001242064405232668, \"time-step\": 3660}, {\"accuracy\": 1.0, \"loss\": 0.0012530178064480424, \"time-step\": 3661}, {\"accuracy\": 1.0, \"loss\": 0.0012406808091327548, \"time-step\": 3662}, {\"accuracy\": 1.0, \"loss\": 0.0012516394490376115, \"time-step\": 3663}, {\"accuracy\": 1.0, \"loss\": 0.0012393251527100801, \"time-step\": 3664}, {\"accuracy\": 1.0, \"loss\": 0.0012502579484134912, \"time-step\": 3665}, {\"accuracy\": 1.0, \"loss\": 0.0012379398103803396, \"time-step\": 3666}, {\"accuracy\": 1.0, \"loss\": 0.0012488706270232797, \"time-step\": 3667}, {\"accuracy\": 1.0, \"loss\": 0.0012365690199658275, \"time-step\": 3668}, {\"accuracy\": 1.0, \"loss\": 0.001247492851689458, \"time-step\": 3669}, {\"accuracy\": 1.0, \"loss\": 0.0012351793702691793, \"time-step\": 3670}, {\"accuracy\": 1.0, \"loss\": 0.0012461008736863732, \"time-step\": 3671}, {\"accuracy\": 1.0, \"loss\": 0.0012338099768385291, \"time-step\": 3672}, {\"accuracy\": 1.0, \"loss\": 0.0012447464978322387, \"time-step\": 3673}, {\"accuracy\": 1.0, \"loss\": 0.001232476788572967, \"time-step\": 3674}, {\"accuracy\": 1.0, \"loss\": 0.0012434030650183558, \"time-step\": 3675}, {\"accuracy\": 1.0, \"loss\": 0.0012311408063396811, \"time-step\": 3676}, {\"accuracy\": 1.0, \"loss\": 0.001242058933712542, \"time-step\": 3677}, {\"accuracy\": 1.0, \"loss\": 0.001229788176715374, \"time-step\": 3678}, {\"accuracy\": 1.0, \"loss\": 0.0012407090980559587, \"time-step\": 3679}, {\"accuracy\": 1.0, \"loss\": 0.0012284534750506282, \"time-step\": 3680}, {\"accuracy\": 1.0, \"loss\": 0.0012393379583954811, \"time-step\": 3681}, {\"accuracy\": 1.0, \"loss\": 0.0012270754668861628, \"time-step\": 3682}, {\"accuracy\": 1.0, \"loss\": 0.0012379696127027273, \"time-step\": 3683}, {\"accuracy\": 1.0, \"loss\": 0.001225727959536016, \"time-step\": 3684}, {\"accuracy\": 1.0, \"loss\": 0.001236619078554213, \"time-step\": 3685}, {\"accuracy\": 1.0, \"loss\": 0.0012243875535205007, \"time-step\": 3686}, {\"accuracy\": 1.0, \"loss\": 0.001235277857631445, \"time-step\": 3687}, {\"accuracy\": 1.0, \"loss\": 0.0012230605352669954, \"time-step\": 3688}, {\"accuracy\": 1.0, \"loss\": 0.0012339565437287092, \"time-step\": 3689}, {\"accuracy\": 1.0, \"loss\": 0.0012217609910294414, \"time-step\": 3690}, {\"accuracy\": 1.0, \"loss\": 0.0012326349969953299, \"time-step\": 3691}, {\"accuracy\": 1.0, \"loss\": 0.0012204209342598915, \"time-step\": 3692}, {\"accuracy\": 1.0, \"loss\": 0.0012312993640080094, \"time-step\": 3693}, {\"accuracy\": 1.0, \"loss\": 0.0012190969428047538, \"time-step\": 3694}, {\"accuracy\": 1.0, \"loss\": 0.0012299546506255865, \"time-step\": 3695}, {\"accuracy\": 1.0, \"loss\": 0.0012177599128335714, \"time-step\": 3696}, {\"accuracy\": 1.0, \"loss\": 0.0012286213459447026, \"time-step\": 3697}, {\"accuracy\": 1.0, \"loss\": 0.0012164529180154204, \"time-step\": 3698}, {\"accuracy\": 1.0, \"loss\": 0.0012272978201508522, \"time-step\": 3699}, {\"accuracy\": 1.0, \"loss\": 0.0012151151895523071, \"time-step\": 3700}, {\"accuracy\": 1.0, \"loss\": 0.0012259631184861064, \"time-step\": 3701}, {\"accuracy\": 1.0, \"loss\": 0.0012137879384681582, \"time-step\": 3702}, {\"accuracy\": 1.0, \"loss\": 0.001224628183990717, \"time-step\": 3703}, {\"accuracy\": 1.0, \"loss\": 0.0012124673230573535, \"time-step\": 3704}, {\"accuracy\": 1.0, \"loss\": 0.0012233128072693944, \"time-step\": 3705}, {\"accuracy\": 1.0, \"loss\": 0.0012111569521948695, \"time-step\": 3706}, {\"accuracy\": 1.0, \"loss\": 0.001221996615640819, \"time-step\": 3707}, {\"accuracy\": 1.0, \"loss\": 0.0012098475126549602, \"time-step\": 3708}, {\"accuracy\": 1.0, \"loss\": 0.0012206586543470621, \"time-step\": 3709}, {\"accuracy\": 1.0, \"loss\": 0.0012085230555385351, \"time-step\": 3710}, {\"accuracy\": 1.0, \"loss\": 0.0012193468865007162, \"time-step\": 3711}, {\"accuracy\": 1.0, \"loss\": 0.0012072339886799455, \"time-step\": 3712}, {\"accuracy\": 1.0, \"loss\": 0.0012180618941783905, \"time-step\": 3713}, {\"accuracy\": 1.0, \"loss\": 0.001205943524837494, \"time-step\": 3714}, {\"accuracy\": 1.0, \"loss\": 0.0012167550157755613, \"time-step\": 3715}, {\"accuracy\": 1.0, \"loss\": 0.0012046402553096414, \"time-step\": 3716}, {\"accuracy\": 1.0, \"loss\": 0.0012154362630099058, \"time-step\": 3717}, {\"accuracy\": 1.0, \"loss\": 0.0012033184757456183, \"time-step\": 3718}, {\"accuracy\": 1.0, \"loss\": 0.0012141144834458828, \"time-step\": 3719}, {\"accuracy\": 1.0, \"loss\": 0.0012020275462418795, \"time-step\": 3720}, {\"accuracy\": 1.0, \"loss\": 0.0012128300732001662, \"time-step\": 3721}, {\"accuracy\": 1.0, \"loss\": 0.0012007548939436674, \"time-step\": 3722}, {\"accuracy\": 1.0, \"loss\": 0.0012115549761801958, \"time-step\": 3723}, {\"accuracy\": 1.0, \"loss\": 0.001199468970298767, \"time-step\": 3724}, {\"accuracy\": 1.0, \"loss\": 0.0012102595064789057, \"time-step\": 3725}, {\"accuracy\": 1.0, \"loss\": 0.0011981797870248556, \"time-step\": 3726}, {\"accuracy\": 1.0, \"loss\": 0.0012089407537132502, \"time-step\": 3727}, {\"accuracy\": 1.0, \"loss\": 0.0011968770995736122, \"time-step\": 3728}, {\"accuracy\": 1.0, \"loss\": 0.001207650639116764, \"time-step\": 3729}, {\"accuracy\": 1.0, \"loss\": 0.0011955976951867342, \"time-step\": 3730}, {\"accuracy\": 1.0, \"loss\": 0.001206367276608944, \"time-step\": 3731}, {\"accuracy\": 1.0, \"loss\": 0.0011943192221224308, \"time-step\": 3732}, {\"accuracy\": 1.0, \"loss\": 0.0012050678487867117, \"time-step\": 3733}, {\"accuracy\": 1.0, \"loss\": 0.0011930237524211407, \"time-step\": 3734}, {\"accuracy\": 1.0, \"loss\": 0.0012037719134241343, \"time-step\": 3735}, {\"accuracy\": 1.0, \"loss\": 0.0011917618103325367, \"time-step\": 3736}, {\"accuracy\": 1.0, \"loss\": 0.0012025136966258287, \"time-step\": 3737}, {\"accuracy\": 1.0, \"loss\": 0.0011904877610504627, \"time-step\": 3738}, {\"accuracy\": 1.0, \"loss\": 0.001201235456392169, \"time-step\": 3739}, {\"accuracy\": 1.0, \"loss\": 0.0011892335023730993, \"time-step\": 3740}, {\"accuracy\": 1.0, \"loss\": 0.0011999758426100016, \"time-step\": 3741}, {\"accuracy\": 1.0, \"loss\": 0.0011879756348207593, \"time-step\": 3742}, {\"accuracy\": 1.0, \"loss\": 0.0011987084290012717, \"time-step\": 3743}, {\"accuracy\": 1.0, \"loss\": 0.0011867065913975239, \"time-step\": 3744}, {\"accuracy\": 1.0, \"loss\": 0.001197436940856278, \"time-step\": 3745}, {\"accuracy\": 1.0, \"loss\": 0.00118544592987746, \"time-step\": 3746}, {\"accuracy\": 1.0, \"loss\": 0.0011961753480136395, \"time-step\": 3747}, {\"accuracy\": 1.0, \"loss\": 0.0011841865489259362, \"time-step\": 3748}, {\"accuracy\": 1.0, \"loss\": 0.0011948986211791635, \"time-step\": 3749}, {\"accuracy\": 1.0, \"loss\": 0.0011829145951196551, \"time-step\": 3750}, {\"accuracy\": 1.0, \"loss\": 0.0011936293449252844, \"time-step\": 3751}, {\"accuracy\": 1.0, \"loss\": 0.0011816793121397495, \"time-step\": 3752}, {\"accuracy\": 1.0, \"loss\": 0.0011923944111913443, \"time-step\": 3753}, {\"accuracy\": 1.0, \"loss\": 0.001180423772893846, \"time-step\": 3754}, {\"accuracy\": 1.0, \"loss\": 0.0011911168694496155, \"time-step\": 3755}, {\"accuracy\": 1.0, \"loss\": 0.0011791775468736887, \"time-step\": 3756}, {\"accuracy\": 1.0, \"loss\": 0.001189871458336711, \"time-step\": 3757}, {\"accuracy\": 1.0, \"loss\": 0.0011779408669099212, \"time-step\": 3758}, {\"accuracy\": 1.0, \"loss\": 0.0011886267457157373, \"time-step\": 3759}, {\"accuracy\": 1.0, \"loss\": 0.001176708028651774, \"time-step\": 3760}, {\"accuracy\": 1.0, \"loss\": 0.0011873948387801647, \"time-step\": 3761}, {\"accuracy\": 1.0, \"loss\": 0.0011754712322726846, \"time-step\": 3762}, {\"accuracy\": 1.0, \"loss\": 0.0011861404636874795, \"time-step\": 3763}, {\"accuracy\": 1.0, \"loss\": 0.001174238626845181, \"time-step\": 3764}, {\"accuracy\": 1.0, \"loss\": 0.0011849054135382175, \"time-step\": 3765}, {\"accuracy\": 1.0, \"loss\": 0.001172987394966185, \"time-step\": 3766}, {\"accuracy\": 1.0, \"loss\": 0.0011836579069495201, \"time-step\": 3767}, {\"accuracy\": 1.0, \"loss\": 0.0011717858724296093, \"time-step\": 3768}, {\"accuracy\": 1.0, \"loss\": 0.0011824506800621748, \"time-step\": 3769}, {\"accuracy\": 1.0, \"loss\": 0.001170561881735921, \"time-step\": 3770}, {\"accuracy\": 1.0, \"loss\": 0.001181233674287796, \"time-step\": 3771}, {\"accuracy\": 1.0, \"loss\": 0.0011693463893607259, \"time-step\": 3772}, {\"accuracy\": 1.0, \"loss\": 0.0011799907078966498, \"time-step\": 3773}, {\"accuracy\": 1.0, \"loss\": 0.0011681168107315898, \"time-step\": 3774}, {\"accuracy\": 1.0, \"loss\": 0.001178769045509398, \"time-step\": 3775}, {\"accuracy\": 1.0, \"loss\": 0.0011668989900499582, \"time-step\": 3776}, {\"accuracy\": 1.0, \"loss\": 0.0011775349266827106, \"time-step\": 3777}, {\"accuracy\": 1.0, \"loss\": 0.001165671506896615, \"time-step\": 3778}, {\"accuracy\": 1.0, \"loss\": 0.0011763126822188497, \"time-step\": 3779}, {\"accuracy\": 1.0, \"loss\": 0.0011644621845334768, \"time-step\": 3780}, {\"accuracy\": 1.0, \"loss\": 0.0011750903213396668, \"time-step\": 3781}, {\"accuracy\": 1.0, \"loss\": 0.0011632535606622696, \"time-step\": 3782}, {\"accuracy\": 1.0, \"loss\": 0.0011738752946257591, \"time-step\": 3783}, {\"accuracy\": 1.0, \"loss\": 0.001162045169621706, \"time-step\": 3784}, {\"accuracy\": 1.0, \"loss\": 0.0011726785451173782, \"time-step\": 3785}, {\"accuracy\": 1.0, \"loss\": 0.0011608580825850368, \"time-step\": 3786}, {\"accuracy\": 1.0, \"loss\": 0.0011714613065123558, \"time-step\": 3787}, {\"accuracy\": 1.0, \"loss\": 0.0011596448021009564, \"time-step\": 3788}, {\"accuracy\": 1.0, \"loss\": 0.0011702608317136765, \"time-step\": 3789}, {\"accuracy\": 1.0, \"loss\": 0.0011584698222577572, \"time-step\": 3790}, {\"accuracy\": 1.0, \"loss\": 0.0011690856190398335, \"time-step\": 3791}, {\"accuracy\": 1.0, \"loss\": 0.0011572870425879955, \"time-step\": 3792}, {\"accuracy\": 1.0, \"loss\": 0.0011678820010274649, \"time-step\": 3793}, {\"accuracy\": 1.0, \"loss\": 0.0011560774873942137, \"time-step\": 3794}, {\"accuracy\": 1.0, \"loss\": 0.0011666633654385805, \"time-step\": 3795}, {\"accuracy\": 1.0, \"loss\": 0.001154894707724452, \"time-step\": 3796}, {\"accuracy\": 1.0, \"loss\": 0.0011654846603050828, \"time-step\": 3797}, {\"accuracy\": 1.0, \"loss\": 0.001153705408796668, \"time-step\": 3798}, {\"accuracy\": 1.0, \"loss\": 0.001164268353022635, \"time-step\": 3799}, {\"accuracy\": 1.0, \"loss\": 0.0011524813016876578, \"time-step\": 3800}, {\"accuracy\": 1.0, \"loss\": 0.0011630426160991192, \"time-step\": 3801}, {\"accuracy\": 1.0, \"loss\": 0.0011512869969010353, \"time-step\": 3802}, {\"accuracy\": 1.0, \"loss\": 0.001161860884167254, \"time-step\": 3803}, {\"accuracy\": 1.0, \"loss\": 0.0011501050321385264, \"time-step\": 3804}, {\"accuracy\": 1.0, \"loss\": 0.0011606616899371147, \"time-step\": 3805}, {\"accuracy\": 1.0, \"loss\": 0.0011489191092550755, \"time-step\": 3806}, {\"accuracy\": 1.0, \"loss\": 0.001159466104581952, \"time-step\": 3807}, {\"accuracy\": 1.0, \"loss\": 0.0011477310908958316, \"time-step\": 3808}, {\"accuracy\": 1.0, \"loss\": 0.001158277504146099, \"time-step\": 3809}, {\"accuracy\": 1.0, \"loss\": 0.0011465479619801044, \"time-step\": 3810}, {\"accuracy\": 1.0, \"loss\": 0.001157101127319038, \"time-step\": 3811}, {\"accuracy\": 1.0, \"loss\": 0.0011453843908384442, \"time-step\": 3812}, {\"accuracy\": 1.0, \"loss\": 0.001155932666733861, \"time-step\": 3813}, {\"accuracy\": 1.0, \"loss\": 0.001144222915172577, \"time-step\": 3814}, {\"accuracy\": 1.0, \"loss\": 0.0011547496542334557, \"time-step\": 3815}, {\"accuracy\": 1.0, \"loss\": 0.0011430549202486873, \"time-step\": 3816}, {\"accuracy\": 1.0, \"loss\": 0.0011535902740433812, \"time-step\": 3817}, {\"accuracy\": 1.0, \"loss\": 0.0011418894864618778, \"time-step\": 3818}, {\"accuracy\": 1.0, \"loss\": 0.0011524296132847667, \"time-step\": 3819}, {\"accuracy\": 1.0, \"loss\": 0.0011407592101022601, \"time-step\": 3820}, {\"accuracy\": 1.0, \"loss\": 0.0011512779165059328, \"time-step\": 3821}, {\"accuracy\": 1.0, \"loss\": 0.0011395964538678527, \"time-step\": 3822}, {\"accuracy\": 1.0, \"loss\": 0.0011501064291223884, \"time-step\": 3823}, {\"accuracy\": 1.0, \"loss\": 0.001138451392762363, \"time-step\": 3824}, {\"accuracy\": 1.0, \"loss\": 0.0011489666067063808, \"time-step\": 3825}, {\"accuracy\": 1.0, \"loss\": 0.0011373121524229646, \"time-step\": 3826}, {\"accuracy\": 1.0, \"loss\": 0.0011478116502985358, \"time-step\": 3827}, {\"accuracy\": 1.0, \"loss\": 0.001136145438067615, \"time-step\": 3828}, {\"accuracy\": 1.0, \"loss\": 0.001146635739132762, \"time-step\": 3829}, {\"accuracy\": 1.0, \"loss\": 0.0011349947890266776, \"time-step\": 3830}, {\"accuracy\": 1.0, \"loss\": 0.0011454909108579159, \"time-step\": 3831}, {\"accuracy\": 1.0, \"loss\": 0.0011338551994413137, \"time-step\": 3832}, {\"accuracy\": 1.0, \"loss\": 0.0011443361872807145, \"time-step\": 3833}, {\"accuracy\": 1.0, \"loss\": 0.0011326931416988373, \"time-step\": 3834}, {\"accuracy\": 1.0, \"loss\": 0.0011431784369051456, \"time-step\": 3835}, {\"accuracy\": 1.0, \"loss\": 0.0011315522715449333, \"time-step\": 3836}, {\"accuracy\": 1.0, \"loss\": 0.001142020570114255, \"time-step\": 3837}, {\"accuracy\": 1.0, \"loss\": 0.0011304094223305583, \"time-step\": 3838}, {\"accuracy\": 1.0, \"loss\": 0.0011408915743231773, \"time-step\": 3839}, {\"accuracy\": 1.0, \"loss\": 0.0011292772833257914, \"time-step\": 3840}, {\"accuracy\": 1.0, \"loss\": 0.0011397331254556775, \"time-step\": 3841}, {\"accuracy\": 1.0, \"loss\": 0.001128125237300992, \"time-step\": 3842}, {\"accuracy\": 1.0, \"loss\": 0.0011385788675397635, \"time-step\": 3843}, {\"accuracy\": 1.0, \"loss\": 0.0011269788956269622, \"time-step\": 3844}, {\"accuracy\": 1.0, \"loss\": 0.0011374294990673661, \"time-step\": 3845}, {\"accuracy\": 1.0, \"loss\": 0.0011258538579568267, \"time-step\": 3846}, {\"accuracy\": 1.0, \"loss\": 0.0011363006196916103, \"time-step\": 3847}, {\"accuracy\": 1.0, \"loss\": 0.0011247254442423582, \"time-step\": 3848}, {\"accuracy\": 1.0, \"loss\": 0.0011351672001183033, \"time-step\": 3849}, {\"accuracy\": 1.0, \"loss\": 0.0011236123973503709, \"time-step\": 3850}, {\"accuracy\": 1.0, \"loss\": 0.0011340465862303972, \"time-step\": 3851}, {\"accuracy\": 1.0, \"loss\": 0.0011224832851439714, \"time-step\": 3852}, {\"accuracy\": 1.0, \"loss\": 0.001132905832491815, \"time-step\": 3853}, {\"accuracy\": 1.0, \"loss\": 0.001121356151998043, \"time-step\": 3854}, {\"accuracy\": 1.0, \"loss\": 0.0011317864991724491, \"time-step\": 3855}, {\"accuracy\": 1.0, \"loss\": 0.0011202406603842974, \"time-step\": 3856}, {\"accuracy\": 1.0, \"loss\": 0.0011306595988571644, \"time-step\": 3857}, {\"accuracy\": 1.0, \"loss\": 0.0011191286612302065, \"time-step\": 3858}, {\"accuracy\": 1.0, \"loss\": 0.0011295363074168563, \"time-step\": 3859}, {\"accuracy\": 1.0, \"loss\": 0.0011179971043020487, \"time-step\": 3860}, {\"accuracy\": 1.0, \"loss\": 0.0011283972999081016, \"time-step\": 3861}, {\"accuracy\": 1.0, \"loss\": 0.0011168764904141426, \"time-step\": 3862}, {\"accuracy\": 1.0, \"loss\": 0.0011272678384557366, \"time-step\": 3863}, {\"accuracy\": 1.0, \"loss\": 0.001115742139518261, \"time-step\": 3864}, {\"accuracy\": 1.0, \"loss\": 0.0011261404724791646, \"time-step\": 3865}, {\"accuracy\": 1.0, \"loss\": 0.0011146381730213761, \"time-step\": 3866}, {\"accuracy\": 1.0, \"loss\": 0.0011250392999500036, \"time-step\": 3867}, {\"accuracy\": 1.0, \"loss\": 0.001113545149564743, \"time-step\": 3868}, {\"accuracy\": 1.0, \"loss\": 0.0011239199666306376, \"time-step\": 3869}, {\"accuracy\": 1.0, \"loss\": 0.0011124369921162724, \"time-step\": 3870}, {\"accuracy\": 1.0, \"loss\": 0.0011228214716538787, \"time-step\": 3871}, {\"accuracy\": 1.0, \"loss\": 0.0011113315122202039, \"time-step\": 3872}, {\"accuracy\": 1.0, \"loss\": 0.0011217083083465695, \"time-step\": 3873}, {\"accuracy\": 1.0, \"loss\": 0.0011102394200861454, \"time-step\": 3874}, {\"accuracy\": 1.0, \"loss\": 0.0011206127237528563, \"time-step\": 3875}, {\"accuracy\": 1.0, \"loss\": 0.0011091453488916159, \"time-step\": 3876}, {\"accuracy\": 1.0, \"loss\": 0.001119515160098672, \"time-step\": 3877}, {\"accuracy\": 1.0, \"loss\": 0.0011080654803663492, \"time-step\": 3878}, {\"accuracy\": 1.0, \"loss\": 0.0011184345930814743, \"time-step\": 3879}, {\"accuracy\": 1.0, \"loss\": 0.0011069822357967496, \"time-step\": 3880}, {\"accuracy\": 1.0, \"loss\": 0.0011173362145200372, \"time-step\": 3881}, {\"accuracy\": 1.0, \"loss\": 0.0011058907257393003, \"time-step\": 3882}, {\"accuracy\": 1.0, \"loss\": 0.0011162463342770934, \"time-step\": 3883}, {\"accuracy\": 1.0, \"loss\": 0.0011048077140003443, \"time-step\": 3884}, {\"accuracy\": 1.0, \"loss\": 0.0011151506332680583, \"time-step\": 3885}, {\"accuracy\": 1.0, \"loss\": 0.0011037272633984685, \"time-step\": 3886}, {\"accuracy\": 1.0, \"loss\": 0.0011140585411339998, \"time-step\": 3887}, {\"accuracy\": 1.0, \"loss\": 0.0011026400607079268, \"time-step\": 3888}, {\"accuracy\": 1.0, \"loss\": 0.0011129634222015738, \"time-step\": 3889}, {\"accuracy\": 1.0, \"loss\": 0.0011015440104529262, \"time-step\": 3890}, {\"accuracy\": 1.0, \"loss\": 0.0011118509573861957, \"time-step\": 3891}, {\"accuracy\": 1.0, \"loss\": 0.0011004350380972028, \"time-step\": 3892}, {\"accuracy\": 1.0, \"loss\": 0.0011107519967481494, \"time-step\": 3893}, {\"accuracy\": 1.0, \"loss\": 0.0010993746109306812, \"time-step\": 3894}, {\"accuracy\": 1.0, \"loss\": 0.001109687378630042, \"time-step\": 3895}, {\"accuracy\": 1.0, \"loss\": 0.0010983101092278957, \"time-step\": 3896}, {\"accuracy\": 1.0, \"loss\": 0.001108622644096613, \"time-step\": 3897}, {\"accuracy\": 1.0, \"loss\": 0.0010972355958074331, \"time-step\": 3898}, {\"accuracy\": 1.0, \"loss\": 0.0011075240327045321, \"time-step\": 3899}, {\"accuracy\": 1.0, \"loss\": 0.001096147927455604, \"time-step\": 3900}, {\"accuracy\": 1.0, \"loss\": 0.001106439856812358, \"time-step\": 3901}, {\"accuracy\": 1.0, \"loss\": 0.0010950780706480145, \"time-step\": 3902}, {\"accuracy\": 1.0, \"loss\": 0.0011053754715248942, \"time-step\": 3903}, {\"accuracy\": 1.0, \"loss\": 0.0010940332431346178, \"time-step\": 3904}, {\"accuracy\": 1.0, \"loss\": 0.0011043368140235543, \"time-step\": 3905}, {\"accuracy\": 1.0, \"loss\": 0.0010929771233350039, \"time-step\": 3906}, {\"accuracy\": 1.0, \"loss\": 0.0011032670736312866, \"time-step\": 3907}, {\"accuracy\": 1.0, \"loss\": 0.0010919250780716538, \"time-step\": 3908}, {\"accuracy\": 1.0, \"loss\": 0.001102218753658235, \"time-step\": 3909}, {\"accuracy\": 1.0, \"loss\": 0.0010908913100138307, \"time-step\": 3910}, {\"accuracy\": 1.0, \"loss\": 0.0011011555325239897, \"time-step\": 3911}, {\"accuracy\": 1.0, \"loss\": 0.0010898183099925518, \"time-step\": 3912}, {\"accuracy\": 1.0, \"loss\": 0.001100065652281046, \"time-step\": 3913}, {\"accuracy\": 1.0, \"loss\": 0.001088736462406814, \"time-step\": 3914}, {\"accuracy\": 1.0, \"loss\": 0.0010989897418767214, \"time-step\": 3915}, {\"accuracy\": 1.0, \"loss\": 0.0010876943124458194, \"time-step\": 3916}, {\"accuracy\": 1.0, \"loss\": 0.0010979517828673124, \"time-step\": 3917}, {\"accuracy\": 1.0, \"loss\": 0.0010866315569728613, \"time-step\": 3918}, {\"accuracy\": 1.0, \"loss\": 0.0010968695860356092, \"time-step\": 3919}, {\"accuracy\": 1.0, \"loss\": 0.0010855658911168575, \"time-step\": 3920}, {\"accuracy\": 1.0, \"loss\": 0.0010958114871755242, \"time-step\": 3921}, {\"accuracy\": 1.0, \"loss\": 0.0010845363140106201, \"time-step\": 3922}, {\"accuracy\": 1.0, \"loss\": 0.0010947880800813437, \"time-step\": 3923}, {\"accuracy\": 1.0, \"loss\": 0.0010834927670657635, \"time-step\": 3924}, {\"accuracy\": 1.0, \"loss\": 0.0010937218321487308, \"time-step\": 3925}, {\"accuracy\": 1.0, \"loss\": 0.0010824516648426652, \"time-step\": 3926}, {\"accuracy\": 1.0, \"loss\": 0.0010926562827080488, \"time-step\": 3927}, {\"accuracy\": 1.0, \"loss\": 0.0010813858825713396, \"time-step\": 3928}, {\"accuracy\": 1.0, \"loss\": 0.0010916065657511353, \"time-step\": 3929}, {\"accuracy\": 1.0, \"loss\": 0.0010803601471707225, \"time-step\": 3930}, {\"accuracy\": 1.0, \"loss\": 0.0010905894450843334, \"time-step\": 3931}, {\"accuracy\": 1.0, \"loss\": 0.0010793529218062758, \"time-step\": 3932}, {\"accuracy\": 1.0, \"loss\": 0.0010895684827119112, \"time-step\": 3933}, {\"accuracy\": 1.0, \"loss\": 0.0010783207835629582, \"time-step\": 3934}, {\"accuracy\": 1.0, \"loss\": 0.0010885184165090322, \"time-step\": 3935}, {\"accuracy\": 1.0, \"loss\": 0.0010772882960736752, \"time-step\": 3936}, {\"accuracy\": 1.0, \"loss\": 0.0010874872095882893, \"time-step\": 3937}, {\"accuracy\": 1.0, \"loss\": 0.0010762548772618175, \"time-step\": 3938}, {\"accuracy\": 1.0, \"loss\": 0.0010864436626434326, \"time-step\": 3939}, {\"accuracy\": 1.0, \"loss\": 0.0010752102825790644, \"time-step\": 3940}, {\"accuracy\": 1.0, \"loss\": 0.0010853928979486227, \"time-step\": 3941}, {\"accuracy\": 1.0, \"loss\": 0.0010741713922470808, \"time-step\": 3942}, {\"accuracy\": 1.0, \"loss\": 0.0010843586642295122, \"time-step\": 3943}, {\"accuracy\": 1.0, \"loss\": 0.0010731512447819114, \"time-step\": 3944}, {\"accuracy\": 1.0, \"loss\": 0.0010833261767402291, \"time-step\": 3945}, {\"accuracy\": 1.0, \"loss\": 0.0010721213184297085, \"time-step\": 3946}, {\"accuracy\": 1.0, \"loss\": 0.001082288334146142, \"time-step\": 3947}, {\"accuracy\": 1.0, \"loss\": 0.0010710882488638163, \"time-step\": 3948}, {\"accuracy\": 1.0, \"loss\": 0.0010812468826770782, \"time-step\": 3949}, {\"accuracy\": 1.0, \"loss\": 0.0010700600687414408, \"time-step\": 3950}, {\"accuracy\": 1.0, \"loss\": 0.0010802121832966805, \"time-step\": 3951}, {\"accuracy\": 1.0, \"loss\": 0.0010690443450585008, \"time-step\": 3952}, {\"accuracy\": 1.0, \"loss\": 0.0010791915701702237, \"time-step\": 3953}, {\"accuracy\": 1.0, \"loss\": 0.0010680268751457334, \"time-step\": 3954}, {\"accuracy\": 1.0, \"loss\": 0.0010781721211969852, \"time-step\": 3955}, {\"accuracy\": 1.0, \"loss\": 0.0010670152259990573, \"time-step\": 3956}, {\"accuracy\": 1.0, \"loss\": 0.001077164663001895, \"time-step\": 3957}, {\"accuracy\": 1.0, \"loss\": 0.0010660069528967142, \"time-step\": 3958}, {\"accuracy\": 1.0, \"loss\": 0.0010761473095044494, \"time-step\": 3959}, {\"accuracy\": 1.0, \"loss\": 0.0010650131152942777, \"time-step\": 3960}, {\"accuracy\": 1.0, \"loss\": 0.0010751350782811642, \"time-step\": 3961}, {\"accuracy\": 1.0, \"loss\": 0.0010640063555911183, \"time-step\": 3962}, {\"accuracy\": 1.0, \"loss\": 0.0010741451987996697, \"time-step\": 3963}, {\"accuracy\": 1.0, \"loss\": 0.0010630148462951183, \"time-step\": 3964}, {\"accuracy\": 1.0, \"loss\": 0.0010731404181569815, \"time-step\": 3965}, {\"accuracy\": 1.0, \"loss\": 0.0010620212415233254, \"time-step\": 3966}, {\"accuracy\": 1.0, \"loss\": 0.0010721408762037754, \"time-step\": 3967}, {\"accuracy\": 1.0, \"loss\": 0.0010610225144773722, \"time-step\": 3968}, {\"accuracy\": 1.0, \"loss\": 0.0010711393551900983, \"time-step\": 3969}, {\"accuracy\": 1.0, \"loss\": 0.0010600389214232564, \"time-step\": 3970}, {\"accuracy\": 1.0, \"loss\": 0.0010701376013457775, \"time-step\": 3971}, {\"accuracy\": 1.0, \"loss\": 0.0010590298334136605, \"time-step\": 3972}, {\"accuracy\": 1.0, \"loss\": 0.0010691379429772496, \"time-step\": 3973}, {\"accuracy\": 1.0, \"loss\": 0.0010580589296296239, \"time-step\": 3974}, {\"accuracy\": 1.0, \"loss\": 0.0010681475978344679, \"time-step\": 3975}, {\"accuracy\": 1.0, \"loss\": 0.0010570663725957274, \"time-step\": 3976}, {\"accuracy\": 1.0, \"loss\": 0.0010671543423086405, \"time-step\": 3977}, {\"accuracy\": 1.0, \"loss\": 0.0010560863884165883, \"time-step\": 3978}, {\"accuracy\": 1.0, \"loss\": 0.001066164462827146, \"time-step\": 3979}, {\"accuracy\": 1.0, \"loss\": 0.0010550852166488767, \"time-step\": 3980}, {\"accuracy\": 1.0, \"loss\": 0.0010651516495272517, \"time-step\": 3981}, {\"accuracy\": 1.0, \"loss\": 0.0010540732182562351, \"time-step\": 3982}, {\"accuracy\": 1.0, \"loss\": 0.0010641342960298061, \"time-step\": 3983}, {\"accuracy\": 1.0, \"loss\": 0.0010530834551900625, \"time-step\": 3984}, {\"accuracy\": 1.0, \"loss\": 0.0010631593177095056, \"time-step\": 3985}, {\"accuracy\": 1.0, \"loss\": 0.0010521153453737497, \"time-step\": 3986}, {\"accuracy\": 1.0, \"loss\": 0.001062180264852941, \"time-step\": 3987}, {\"accuracy\": 1.0, \"loss\": 0.001051144558005035, \"time-step\": 3988}, {\"accuracy\": 1.0, \"loss\": 0.0010612005135044456, \"time-step\": 3989}, {\"accuracy\": 1.0, \"loss\": 0.0010501730721443892, \"time-step\": 3990}, {\"accuracy\": 1.0, \"loss\": 0.0010602184338495135, \"time-step\": 3991}, {\"accuracy\": 1.0, \"loss\": 0.0010491969296708703, \"time-step\": 3992}, {\"accuracy\": 1.0, \"loss\": 0.0010592322796583176, \"time-step\": 3993}, {\"accuracy\": 1.0, \"loss\": 0.001048206933774054, \"time-step\": 3994}, {\"accuracy\": 1.0, \"loss\": 0.0010582375107333064, \"time-step\": 3995}, {\"accuracy\": 1.0, \"loss\": 0.0010472119320183992, \"time-step\": 3996}, {\"accuracy\": 1.0, \"loss\": 0.0010572365717962384, \"time-step\": 3997}, {\"accuracy\": 1.0, \"loss\": 0.0010462392820045352, \"time-step\": 3998}, {\"accuracy\": 1.0, \"loss\": 0.0010562698589637876, \"time-step\": 3999}, {\"accuracy\": 1.0, \"loss\": 0.001045270822942257, \"time-step\": 4000}, {\"accuracy\": 1.0, \"loss\": 0.0010552937164902687, \"time-step\": 4001}, {\"accuracy\": 1.0, \"loss\": 0.0010443116771057248, \"time-step\": 4002}, {\"accuracy\": 1.0, \"loss\": 0.001054325490258634, \"time-step\": 4003}, {\"accuracy\": 1.0, \"loss\": 0.0010433414718136191, \"time-step\": 4004}, {\"accuracy\": 1.0, \"loss\": 0.0010533597087487578, \"time-step\": 4005}, {\"accuracy\": 1.0, \"loss\": 0.0010423829080536962, \"time-step\": 4006}, {\"accuracy\": 1.0, \"loss\": 0.0010523925302550197, \"time-step\": 4007}, {\"accuracy\": 1.0, \"loss\": 0.0010414313292130828, \"time-step\": 4008}, {\"accuracy\": 1.0, \"loss\": 0.0010514239547774196, \"time-step\": 4009}, {\"accuracy\": 1.0, \"loss\": 0.0010404565837234259, \"time-step\": 4010}, {\"accuracy\": 1.0, \"loss\": 0.001050448976457119, \"time-step\": 4011}, {\"accuracy\": 1.0, \"loss\": 0.0010395089630037546, \"time-step\": 4012}, {\"accuracy\": 1.0, \"loss\": 0.0010494989110156894, \"time-step\": 4013}, {\"accuracy\": 1.0, \"loss\": 0.0010385578498244286, \"time-step\": 4014}, {\"accuracy\": 1.0, \"loss\": 0.0010485389502719045, \"time-step\": 4015}, {\"accuracy\": 1.0, \"loss\": 0.001037598354741931, \"time-step\": 4016}, {\"accuracy\": 1.0, \"loss\": 0.0010475768940523267, \"time-step\": 4017}, {\"accuracy\": 1.0, \"loss\": 0.001036667381413281, \"time-step\": 4018}, {\"accuracy\": 1.0, \"loss\": 0.0010466399835422635, \"time-step\": 4019}, {\"accuracy\": 1.0, \"loss\": 0.0010357149876654148, \"time-step\": 4020}, {\"accuracy\": 1.0, \"loss\": 0.0010456725722178817, \"time-step\": 4021}, {\"accuracy\": 1.0, \"loss\": 0.0010347607312723994, \"time-step\": 4022}, {\"accuracy\": 1.0, \"loss\": 0.0010447143577039242, \"time-step\": 4023}, {\"accuracy\": 1.0, \"loss\": 0.0010337948333472013, \"time-step\": 4024}, {\"accuracy\": 1.0, \"loss\": 0.001043747179210186, \"time-step\": 4025}, {\"accuracy\": 1.0, \"loss\": 0.0010328554781153798, \"time-step\": 4026}, {\"accuracy\": 1.0, \"loss\": 0.0010428171372041106, \"time-step\": 4027}, {\"accuracy\": 1.0, \"loss\": 0.0010319240391254425, \"time-step\": 4028}, {\"accuracy\": 1.0, \"loss\": 0.0010418714955449104, \"time-step\": 4029}, {\"accuracy\": 1.0, \"loss\": 0.0010309969075024128, \"time-step\": 4030}, {\"accuracy\": 1.0, \"loss\": 0.0010409430833533406, \"time-step\": 4031}, {\"accuracy\": 1.0, \"loss\": 0.0010300558060407639, \"time-step\": 4032}, {\"accuracy\": 1.0, \"loss\": 0.0010399941820651293, \"time-step\": 4033}, {\"accuracy\": 1.0, \"loss\": 0.001029119361191988, \"time-step\": 4034}, {\"accuracy\": 1.0, \"loss\": 0.0010390497045591474, \"time-step\": 4035}, {\"accuracy\": 1.0, \"loss\": 0.0010281858267262578, \"time-step\": 4036}, {\"accuracy\": 1.0, \"loss\": 0.001038100104779005, \"time-step\": 4037}, {\"accuracy\": 1.0, \"loss\": 0.0010272408835589886, \"time-step\": 4038}, {\"accuracy\": 1.0, \"loss\": 0.0010371508542448282, \"time-step\": 4039}, {\"accuracy\": 1.0, \"loss\": 0.0010263146832585335, \"time-step\": 4040}, {\"accuracy\": 1.0, \"loss\": 0.0010362282628193498, \"time-step\": 4041}, {\"accuracy\": 1.0, \"loss\": 0.0010253861546516418, \"time-step\": 4042}, {\"accuracy\": 1.0, \"loss\": 0.0010352868121117353, \"time-step\": 4043}, {\"accuracy\": 1.0, \"loss\": 0.0010244583245366812, \"time-step\": 4044}, {\"accuracy\": 1.0, \"loss\": 0.0010343568865209818, \"time-step\": 4045}, {\"accuracy\": 1.0, \"loss\": 0.0010235300287604332, \"time-step\": 4046}, {\"accuracy\": 1.0, \"loss\": 0.0010334268445149064, \"time-step\": 4047}, {\"accuracy\": 1.0, \"loss\": 0.0010226208250969648, \"time-step\": 4048}, {\"accuracy\": 1.0, \"loss\": 0.0010325209004804492, \"time-step\": 4049}, {\"accuracy\": 1.0, \"loss\": 0.0010216941591352224, \"time-step\": 4050}, {\"accuracy\": 1.0, \"loss\": 0.001031570602208376, \"time-step\": 4051}, {\"accuracy\": 1.0, \"loss\": 0.0010207750601693988, \"time-step\": 4052}, {\"accuracy\": 1.0, \"loss\": 0.0010306501062586904, \"time-step\": 4053}, {\"accuracy\": 1.0, \"loss\": 0.0010198504896834493, \"time-step\": 4054}, {\"accuracy\": 1.0, \"loss\": 0.0010297175031155348, \"time-step\": 4055}, {\"accuracy\": 1.0, \"loss\": 0.001018922426737845, \"time-step\": 4056}, {\"accuracy\": 1.0, \"loss\": 0.0010287956101819873, \"time-step\": 4057}, {\"accuracy\": 1.0, \"loss\": 0.0010180252138525248, \"time-step\": 4058}, {\"accuracy\": 1.0, \"loss\": 0.001027901889756322, \"time-step\": 4059}, {\"accuracy\": 1.0, \"loss\": 0.0010171179892495275, \"time-step\": 4060}, {\"accuracy\": 1.0, \"loss\": 0.0010269597405567765, \"time-step\": 4061}, {\"accuracy\": 1.0, \"loss\": 0.0010161795653402805, \"time-step\": 4062}, {\"accuracy\": 1.0, \"loss\": 0.0010260299313813448, \"time-step\": 4063}, {\"accuracy\": 1.0, \"loss\": 0.0010152659378945827, \"time-step\": 4064}, {\"accuracy\": 1.0, \"loss\": 0.0010251067578792572, \"time-step\": 4065}, {\"accuracy\": 1.0, \"loss\": 0.001014358946122229, \"time-step\": 4066}, {\"accuracy\": 1.0, \"loss\": 0.0010241966228932142, \"time-step\": 4067}, {\"accuracy\": 1.0, \"loss\": 0.0010134532349184155, \"time-step\": 4068}, {\"accuracy\": 1.0, \"loss\": 0.0010232905624434352, \"time-step\": 4069}, {\"accuracy\": 1.0, \"loss\": 0.0010125479893758893, \"time-step\": 4070}, {\"accuracy\": 1.0, \"loss\": 0.001022369833663106, \"time-step\": 4071}, {\"accuracy\": 1.0, \"loss\": 0.001011635409668088, \"time-step\": 4072}, {\"accuracy\": 1.0, \"loss\": 0.001021475763991475, \"time-step\": 4073}, {\"accuracy\": 1.0, \"loss\": 0.0010107445996254683, \"time-step\": 4074}, {\"accuracy\": 1.0, \"loss\": 0.0010205587605014443, \"time-step\": 4075}, {\"accuracy\": 1.0, \"loss\": 0.0010098523925989866, \"time-step\": 4076}, {\"accuracy\": 1.0, \"loss\": 0.001019659568555653, \"time-step\": 4077}, {\"accuracy\": 1.0, \"loss\": 0.0010089395800605416, \"time-step\": 4078}, {\"accuracy\": 1.0, \"loss\": 0.0010187564184889197, \"time-step\": 4079}, {\"accuracy\": 1.0, \"loss\": 0.0010080531938001513, \"time-step\": 4080}, {\"accuracy\": 1.0, \"loss\": 0.001017855480313301, \"time-step\": 4081}, {\"accuracy\": 1.0, \"loss\": 0.0010071602882817388, \"time-step\": 4082}, {\"accuracy\": 1.0, \"loss\": 0.0010169737506657839, \"time-step\": 4083}, {\"accuracy\": 1.0, \"loss\": 0.0010062877554446459, \"time-step\": 4084}, {\"accuracy\": 1.0, \"loss\": 0.001016096444800496, \"time-step\": 4085}, {\"accuracy\": 1.0, \"loss\": 0.001005417201668024, \"time-step\": 4086}, {\"accuracy\": 1.0, \"loss\": 0.001015196554362774, \"time-step\": 4087}, {\"accuracy\": 1.0, \"loss\": 0.001004502410069108, \"time-step\": 4088}, {\"accuracy\": 1.0, \"loss\": 0.0010142833925783634, \"time-step\": 4089}, {\"accuracy\": 1.0, \"loss\": 0.0010036107851192355, \"time-step\": 4090}, {\"accuracy\": 1.0, \"loss\": 0.0010134021285921335, \"time-step\": 4091}, {\"accuracy\": 1.0, \"loss\": 0.0010027396492660046, \"time-step\": 4092}, {\"accuracy\": 1.0, \"loss\": 0.0010125181870535016, \"time-step\": 4093}, {\"accuracy\": 1.0, \"loss\": 0.0010018546599894762, \"time-step\": 4094}, {\"accuracy\": 1.0, \"loss\": 0.0010116127086803317, \"time-step\": 4095}, {\"accuracy\": 1.0, \"loss\": 0.0010009577963501215, \"time-step\": 4096}, {\"accuracy\": 1.0, \"loss\": 0.001010722597129643, \"time-step\": 4097}, {\"accuracy\": 1.0, \"loss\": 0.0010000705951824784, \"time-step\": 4098}, {\"accuracy\": 1.0, \"loss\": 0.0010098177008330822, \"time-step\": 4099}, {\"accuracy\": 1.0, \"loss\": 0.0009991828119382262, \"time-step\": 4100}, {\"accuracy\": 1.0, \"loss\": 0.0010089363204315305, \"time-step\": 4101}, {\"accuracy\": 1.0, \"loss\": 0.0009983174968510866, \"time-step\": 4102}, {\"accuracy\": 1.0, \"loss\": 0.0010080579668283463, \"time-step\": 4103}, {\"accuracy\": 1.0, \"loss\": 0.0009974315762519836, \"time-step\": 4104}, {\"accuracy\": 1.0, \"loss\": 0.0010071861324831843, \"time-step\": 4105}, {\"accuracy\": 1.0, \"loss\": 0.0009965626522898674, \"time-step\": 4106}, {\"accuracy\": 1.0, \"loss\": 0.0010062837973237038, \"time-step\": 4107}, {\"accuracy\": 1.0, \"loss\": 0.0009956805733963847, \"time-step\": 4108}, {\"accuracy\": 1.0, \"loss\": 0.0010054062586277723, \"time-step\": 4109}, {\"accuracy\": 1.0, \"loss\": 0.0009947980288416147, \"time-step\": 4110}, {\"accuracy\": 1.0, \"loss\": 0.0010045134695246816, \"time-step\": 4111}, {\"accuracy\": 1.0, \"loss\": 0.0009939305018633604, \"time-step\": 4112}, {\"accuracy\": 1.0, \"loss\": 0.0010036519961431623, \"time-step\": 4113}, {\"accuracy\": 1.0, \"loss\": 0.0009930754313245416, \"time-step\": 4114}, {\"accuracy\": 1.0, \"loss\": 0.00100279925391078, \"time-step\": 4115}, {\"accuracy\": 1.0, \"loss\": 0.000992211396805942, \"time-step\": 4116}, {\"accuracy\": 1.0, \"loss\": 0.0010019252076745033, \"time-step\": 4117}, {\"accuracy\": 1.0, \"loss\": 0.0009913541143760085, \"time-step\": 4118}, {\"accuracy\": 1.0, \"loss\": 0.0010010587284341455, \"time-step\": 4119}, {\"accuracy\": 1.0, \"loss\": 0.0009904926409944892, \"time-step\": 4120}, {\"accuracy\": 1.0, \"loss\": 0.0010002031922340393, \"time-step\": 4121}, {\"accuracy\": 1.0, \"loss\": 0.0009896514238789678, \"time-step\": 4122}, {\"accuracy\": 1.0, \"loss\": 0.000999334966763854, \"time-step\": 4123}, {\"accuracy\": 1.0, \"loss\": 0.0009887834312394261, \"time-step\": 4124}, {\"accuracy\": 1.0, \"loss\": 0.0009984729113057256, \"time-step\": 4125}, {\"accuracy\": 1.0, \"loss\": 0.000987933250144124, \"time-step\": 4126}, {\"accuracy\": 1.0, \"loss\": 0.0009976205183193088, \"time-step\": 4127}, {\"accuracy\": 1.0, \"loss\": 0.0009870787616819143, \"time-step\": 4128}, {\"accuracy\": 1.0, \"loss\": 0.000996753922663629, \"time-step\": 4129}, {\"accuracy\": 1.0, \"loss\": 0.0009862181032076478, \"time-step\": 4130}, {\"accuracy\": 1.0, \"loss\": 0.0009958820883184671, \"time-step\": 4131}, {\"accuracy\": 1.0, \"loss\": 0.0009853558149188757, \"time-step\": 4132}, {\"accuracy\": 1.0, \"loss\": 0.0009950214298442006, \"time-step\": 4133}, {\"accuracy\": 1.0, \"loss\": 0.000984497251920402, \"time-step\": 4134}, {\"accuracy\": 1.0, \"loss\": 0.0009941579774022102, \"time-step\": 4135}, {\"accuracy\": 1.0, \"loss\": 0.000983659760095179, \"time-step\": 4136}, {\"accuracy\": 1.0, \"loss\": 0.0009933304972946644, \"time-step\": 4137}, {\"accuracy\": 1.0, \"loss\": 0.0009828354232013226, \"time-step\": 4138}, {\"accuracy\": 1.0, \"loss\": 0.0009924826445057988, \"time-step\": 4139}, {\"accuracy\": 1.0, \"loss\": 0.0009819810511544347, \"time-step\": 4140}, {\"accuracy\": 1.0, \"loss\": 0.0009916271083056927, \"time-step\": 4141}, {\"accuracy\": 1.0, \"loss\": 0.0009811394847929478, \"time-step\": 4142}, {\"accuracy\": 1.0, \"loss\": 0.00099076796323061, \"time-step\": 4143}, {\"accuracy\": 1.0, \"loss\": 0.0009802732383832335, \"time-step\": 4144}, {\"accuracy\": 1.0, \"loss\": 0.0009899046272039413, \"time-step\": 4145}, {\"accuracy\": 1.0, \"loss\": 0.0009794299257919192, \"time-step\": 4146}, {\"accuracy\": 1.0, \"loss\": 0.0009890655055642128, \"time-step\": 4147}, {\"accuracy\": 1.0, \"loss\": 0.0009785934817045927, \"time-step\": 4148}, {\"accuracy\": 1.0, \"loss\": 0.0009882162557914853, \"time-step\": 4149}, {\"accuracy\": 1.0, \"loss\": 0.0009777456289157271, \"time-step\": 4150}, {\"accuracy\": 1.0, \"loss\": 0.0009873738745227456, \"time-step\": 4151}, {\"accuracy\": 1.0, \"loss\": 0.0009769366588443518, \"time-step\": 4152}, {\"accuracy\": 1.0, \"loss\": 0.000986558268778026, \"time-step\": 4153}, {\"accuracy\": 1.0, \"loss\": 0.0009761034161783755, \"time-step\": 4154}, {\"accuracy\": 1.0, \"loss\": 0.0009856934193521738, \"time-step\": 4155}, {\"accuracy\": 1.0, \"loss\": 0.0009752378682605922, \"time-step\": 4156}, {\"accuracy\": 1.0, \"loss\": 0.0009848377667367458, \"time-step\": 4157}, {\"accuracy\": 1.0, \"loss\": 0.000974408583715558, \"time-step\": 4158}, {\"accuracy\": 1.0, \"loss\": 0.0009840053971856833, \"time-step\": 4159}, {\"accuracy\": 1.0, \"loss\": 0.0009735747589729726, \"time-step\": 4160}, {\"accuracy\": 1.0, \"loss\": 0.0009831631323322654, \"time-step\": 4161}, {\"accuracy\": 1.0, \"loss\": 0.0009727348806336522, \"time-step\": 4162}, {\"accuracy\": 1.0, \"loss\": 0.000982321216724813, \"time-step\": 4163}, {\"accuracy\": 1.0, \"loss\": 0.0009719079826027155, \"time-step\": 4164}, {\"accuracy\": 1.0, \"loss\": 0.000981486402451992, \"time-step\": 4165}, {\"accuracy\": 1.0, \"loss\": 0.0009710959857329726, \"time-step\": 4166}, {\"accuracy\": 1.0, \"loss\": 0.00098067382350564, \"time-step\": 4167}, {\"accuracy\": 1.0, \"loss\": 0.000970270368270576, \"time-step\": 4168}, {\"accuracy\": 1.0, \"loss\": 0.000979844480752945, \"time-step\": 4169}, {\"accuracy\": 1.0, \"loss\": 0.000969455111771822, \"time-step\": 4170}, {\"accuracy\": 1.0, \"loss\": 0.0009790195617824793, \"time-step\": 4171}, {\"accuracy\": 1.0, \"loss\": 0.0009686480043455958, \"time-step\": 4172}, {\"accuracy\": 1.0, \"loss\": 0.0009782069828361273, \"time-step\": 4173}, {\"accuracy\": 1.0, \"loss\": 0.000967813772149384, \"time-step\": 4174}, {\"accuracy\": 1.0, \"loss\": 0.0009773575002327561, \"time-step\": 4175}, {\"accuracy\": 1.0, \"loss\": 0.000966966210398823, \"time-step\": 4176}, {\"accuracy\": 1.0, \"loss\": 0.0009765083668753505, \"time-step\": 4177}, {\"accuracy\": 1.0, \"loss\": 0.0009661578224040568, \"time-step\": 4178}, {\"accuracy\": 1.0, \"loss\": 0.0009757119114510715, \"time-step\": 4179}, {\"accuracy\": 1.0, \"loss\": 0.0009653521701693535, \"time-step\": 4180}, {\"accuracy\": 1.0, \"loss\": 0.0009748822776600718, \"time-step\": 4181}, {\"accuracy\": 1.0, \"loss\": 0.0009645199170336127, \"time-step\": 4182}, {\"accuracy\": 1.0, \"loss\": 0.0009740650421008468, \"time-step\": 4183}, {\"accuracy\": 1.0, \"loss\": 0.0009637178736738861, \"time-step\": 4184}, {\"accuracy\": 1.0, \"loss\": 0.0009732379112392664, \"time-step\": 4185}, {\"accuracy\": 1.0, \"loss\": 0.0009628997649997473, \"time-step\": 4186}, {\"accuracy\": 1.0, \"loss\": 0.0009724383126012981, \"time-step\": 4187}, {\"accuracy\": 1.0, \"loss\": 0.000962116988375783, \"time-step\": 4188}, {\"accuracy\": 1.0, \"loss\": 0.000971640576608479, \"time-step\": 4189}, {\"accuracy\": 1.0, \"loss\": 0.0009613113943487406, \"time-step\": 4190}, {\"accuracy\": 1.0, \"loss\": 0.0009708256693556905, \"time-step\": 4191}, {\"accuracy\": 1.0, \"loss\": 0.0009605116210877895, \"time-step\": 4192}, {\"accuracy\": 1.0, \"loss\": 0.0009700249647721648, \"time-step\": 4193}, {\"accuracy\": 1.0, \"loss\": 0.0009597132448107004, \"time-step\": 4194}, {\"accuracy\": 1.0, \"loss\": 0.0009692215244285762, \"time-step\": 4195}, {\"accuracy\": 1.0, \"loss\": 0.0009589123073965311, \"time-step\": 4196}, {\"accuracy\": 1.0, \"loss\": 0.0009684164542704821, \"time-step\": 4197}, {\"accuracy\": 1.0, \"loss\": 0.0009581116610206664, \"time-step\": 4198}, {\"accuracy\": 1.0, \"loss\": 0.0009676019544713199, \"time-step\": 4199}, {\"accuracy\": 1.0, \"loss\": 0.0009573063580319285, \"time-step\": 4200}, {\"accuracy\": 1.0, \"loss\": 0.0009667865233495831, \"time-step\": 4201}, {\"accuracy\": 1.0, \"loss\": 0.000956486095674336, \"time-step\": 4202}, {\"accuracy\": 1.0, \"loss\": 0.0009659615461714566, \"time-step\": 4203}, {\"accuracy\": 1.0, \"loss\": 0.0009556766017340124, \"time-step\": 4204}, {\"accuracy\": 1.0, \"loss\": 0.0009651707368902862, \"time-step\": 4205}, {\"accuracy\": 1.0, \"loss\": 0.0009548973757773638, \"time-step\": 4206}, {\"accuracy\": 1.0, \"loss\": 0.0009643640369176865, \"time-step\": 4207}, {\"accuracy\": 1.0, \"loss\": 0.000954088696744293, \"time-step\": 4208}, {\"accuracy\": 1.0, \"loss\": 0.0009635519236326218, \"time-step\": 4209}, {\"accuracy\": 1.0, \"loss\": 0.0009532942203804851, \"time-step\": 4210}, {\"accuracy\": 1.0, \"loss\": 0.0009627615800127387, \"time-step\": 4211}, {\"accuracy\": 1.0, \"loss\": 0.000952512607909739, \"time-step\": 4212}, {\"accuracy\": 1.0, \"loss\": 0.0009619636693969369, \"time-step\": 4213}, {\"accuracy\": 1.0, \"loss\": 0.0009517205762676895, \"time-step\": 4214}, {\"accuracy\": 1.0, \"loss\": 0.0009611680870875716, \"time-step\": 4215}, {\"accuracy\": 1.0, \"loss\": 0.0009509245865046978, \"time-step\": 4216}, {\"accuracy\": 1.0, \"loss\": 0.0009603723883628845, \"time-step\": 4217}, {\"accuracy\": 1.0, \"loss\": 0.0009501415188424289, \"time-step\": 4218}, {\"accuracy\": 1.0, \"loss\": 0.0009595869923941791, \"time-step\": 4219}, {\"accuracy\": 1.0, \"loss\": 0.000949349720031023, \"time-step\": 4220}, {\"accuracy\": 1.0, \"loss\": 0.0009587820386514068, \"time-step\": 4221}, {\"accuracy\": 1.0, \"loss\": 0.0009485518676228821, \"time-step\": 4222}, {\"accuracy\": 1.0, \"loss\": 0.0009579689940437675, \"time-step\": 4223}, {\"accuracy\": 1.0, \"loss\": 0.0009477512794546783, \"time-step\": 4224}, {\"accuracy\": 1.0, \"loss\": 0.000957177544478327, \"time-step\": 4225}, {\"accuracy\": 1.0, \"loss\": 0.0009469783981330693, \"time-step\": 4226}, {\"accuracy\": 1.0, \"loss\": 0.0009564204956404865, \"time-step\": 4227}, {\"accuracy\": 1.0, \"loss\": 0.0009462317684665322, \"time-step\": 4228}, {\"accuracy\": 1.0, \"loss\": 0.0009556549484841526, \"time-step\": 4229}, {\"accuracy\": 1.0, \"loss\": 0.0009454588871449232, \"time-step\": 4230}, {\"accuracy\": 1.0, \"loss\": 0.0009548672242090106, \"time-step\": 4231}, {\"accuracy\": 1.0, \"loss\": 0.000944662606343627, \"time-step\": 4232}, {\"accuracy\": 1.0, \"loss\": 0.0009540630271658301, \"time-step\": 4233}, {\"accuracy\": 1.0, \"loss\": 0.0009438811102882028, \"time-step\": 4234}, {\"accuracy\": 1.0, \"loss\": 0.0009532778640277684, \"time-step\": 4235}, {\"accuracy\": 1.0, \"loss\": 0.0009430953650735319, \"time-step\": 4236}, {\"accuracy\": 1.0, \"loss\": 0.0009524952038191259, \"time-step\": 4237}, {\"accuracy\": 1.0, \"loss\": 0.0009423217852599919, \"time-step\": 4238}, {\"accuracy\": 1.0, \"loss\": 0.0009517119615338743, \"time-step\": 4239}, {\"accuracy\": 1.0, \"loss\": 0.0009415608365088701, \"time-step\": 4240}, {\"accuracy\": 1.0, \"loss\": 0.0009509448427706957, \"time-step\": 4241}, {\"accuracy\": 1.0, \"loss\": 0.0009407891193404794, \"time-step\": 4242}, {\"accuracy\": 1.0, \"loss\": 0.0009501733584329486, \"time-step\": 4243}, {\"accuracy\": 1.0, \"loss\": 0.0009400239796377718, \"time-step\": 4244}, {\"accuracy\": 1.0, \"loss\": 0.0009493960533291101, \"time-step\": 4245}, {\"accuracy\": 1.0, \"loss\": 0.0009392635547555983, \"time-step\": 4246}, {\"accuracy\": 1.0, \"loss\": 0.0009486516937613487, \"time-step\": 4247}, {\"accuracy\": 1.0, \"loss\": 0.0009385077282786369, \"time-step\": 4248}, {\"accuracy\": 1.0, \"loss\": 0.0009478702559135854, \"time-step\": 4249}, {\"accuracy\": 1.0, \"loss\": 0.0009377454989589751, \"time-step\": 4250}, {\"accuracy\": 1.0, \"loss\": 0.000947121880017221, \"time-step\": 4251}, {\"accuracy\": 1.0, \"loss\": 0.0009370037587359548, \"time-step\": 4252}, {\"accuracy\": 1.0, \"loss\": 0.0009463470196351409, \"time-step\": 4253}, {\"accuracy\": 1.0, \"loss\": 0.0009362204000353813, \"time-step\": 4254}, {\"accuracy\": 1.0, \"loss\": 0.0009455704130232334, \"time-step\": 4255}, {\"accuracy\": 1.0, \"loss\": 0.0009354619542136788, \"time-step\": 4256}, {\"accuracy\": 1.0, \"loss\": 0.0009448057971894741, \"time-step\": 4257}, {\"accuracy\": 1.0, \"loss\": 0.0009346966398879886, \"time-step\": 4258}, {\"accuracy\": 1.0, \"loss\": 0.0009440433350391686, \"time-step\": 4259}, {\"accuracy\": 1.0, \"loss\": 0.0009339424432255328, \"time-step\": 4260}, {\"accuracy\": 1.0, \"loss\": 0.0009432808146812022, \"time-step\": 4261}, {\"accuracy\": 1.0, \"loss\": 0.0009331958135589957, \"time-step\": 4262}, {\"accuracy\": 1.0, \"loss\": 0.0009425249882042408, \"time-step\": 4263}, {\"accuracy\": 1.0, \"loss\": 0.0009324466809630394, \"time-step\": 4264}, {\"accuracy\": 1.0, \"loss\": 0.0009417873225174844, \"time-step\": 4265}, {\"accuracy\": 1.0, \"loss\": 0.0009317179210484028, \"time-step\": 4266}, {\"accuracy\": 1.0, \"loss\": 0.0009410367347300053, \"time-step\": 4267}, {\"accuracy\": 1.0, \"loss\": 0.0009309586021117866, \"time-step\": 4268}, {\"accuracy\": 1.0, \"loss\": 0.0009402649593539536, \"time-step\": 4269}, {\"accuracy\": 1.0, \"loss\": 0.0009301950922235847, \"time-step\": 4270}, {\"accuracy\": 1.0, \"loss\": 0.0009395022061653435, \"time-step\": 4271}, {\"accuracy\": 1.0, \"loss\": 0.0009294436895288527, \"time-step\": 4272}, {\"accuracy\": 1.0, \"loss\": 0.0009387583704665303, \"time-step\": 4273}, {\"accuracy\": 1.0, \"loss\": 0.0009287044522352517, \"time-step\": 4274}, {\"accuracy\": 1.0, \"loss\": 0.0009380004485137761, \"time-step\": 4275}, {\"accuracy\": 1.0, \"loss\": 0.0009279532823711634, \"time-step\": 4276}, {\"accuracy\": 1.0, \"loss\": 0.0009372620261274278, \"time-step\": 4277}, {\"accuracy\": 1.0, \"loss\": 0.0009272209135815501, \"time-step\": 4278}, {\"accuracy\": 1.0, \"loss\": 0.0009365107398480177, \"time-step\": 4279}, {\"accuracy\": 1.0, \"loss\": 0.0009264668915420771, \"time-step\": 4280}, {\"accuracy\": 1.0, \"loss\": 0.0009357536910101771, \"time-step\": 4281}, {\"accuracy\": 1.0, \"loss\": 0.000925721600651741, \"time-step\": 4282}, {\"accuracy\": 1.0, \"loss\": 0.0009349931497126818, \"time-step\": 4283}, {\"accuracy\": 1.0, \"loss\": 0.0009249647264368832, \"time-step\": 4284}, {\"accuracy\": 1.0, \"loss\": 0.0009342411649413407, \"time-step\": 4285}, {\"accuracy\": 1.0, \"loss\": 0.0009242198430001736, \"time-step\": 4286}, {\"accuracy\": 1.0, \"loss\": 0.0009334880742244422, \"time-step\": 4287}, {\"accuracy\": 1.0, \"loss\": 0.0009234729805029929, \"time-step\": 4288}, {\"accuracy\": 1.0, \"loss\": 0.0009327437728643417, \"time-step\": 4289}, {\"accuracy\": 1.0, \"loss\": 0.0009227562695741653, \"time-step\": 4290}, {\"accuracy\": 1.0, \"loss\": 0.0009320084936916828, \"time-step\": 4291}, {\"accuracy\": 1.0, \"loss\": 0.0009220055071637034, \"time-step\": 4292}, {\"accuracy\": 1.0, \"loss\": 0.0009312704205513, \"time-step\": 4293}, {\"accuracy\": 1.0, \"loss\": 0.0009212850127369165, \"time-step\": 4294}, {\"accuracy\": 1.0, \"loss\": 0.0009305333369411528, \"time-step\": 4295}, {\"accuracy\": 1.0, \"loss\": 0.0009205455426126719, \"time-step\": 4296}, {\"accuracy\": 1.0, \"loss\": 0.0009297898504883051, \"time-step\": 4297}, {\"accuracy\": 1.0, \"loss\": 0.0009198174229823053, \"time-step\": 4298}, {\"accuracy\": 1.0, \"loss\": 0.0009290652815252542, \"time-step\": 4299}, {\"accuracy\": 1.0, \"loss\": 0.0009190825512632728, \"time-step\": 4300}, {\"accuracy\": 1.0, \"loss\": 0.0009283167310059071, \"time-step\": 4301}, {\"accuracy\": 1.0, \"loss\": 0.0009183399961329997, \"time-step\": 4302}, {\"accuracy\": 1.0, \"loss\": 0.0009275597985833883, \"time-step\": 4303}, {\"accuracy\": 1.0, \"loss\": 0.0009175911545753479, \"time-step\": 4304}, {\"accuracy\": 1.0, \"loss\": 0.0009268190478906035, \"time-step\": 4305}, {\"accuracy\": 1.0, \"loss\": 0.0009168776450678706, \"time-step\": 4306}, {\"accuracy\": 1.0, \"loss\": 0.000926105072721839, \"time-step\": 4307}, {\"accuracy\": 1.0, \"loss\": 0.0009161586640402675, \"time-step\": 4308}, {\"accuracy\": 1.0, \"loss\": 0.000925378524698317, \"time-step\": 4309}, {\"accuracy\": 1.0, \"loss\": 0.0009154443978331983, \"time-step\": 4310}, {\"accuracy\": 1.0, \"loss\": 0.0009246690315194428, \"time-step\": 4311}, {\"accuracy\": 1.0, \"loss\": 0.0009147304808720946, \"time-step\": 4312}, {\"accuracy\": 1.0, \"loss\": 0.0009239264763891697, \"time-step\": 4313}, {\"accuracy\": 1.0, \"loss\": 0.0009139913599938154, \"time-step\": 4314}, {\"accuracy\": 1.0, \"loss\": 0.0009231861331500113, \"time-step\": 4315}, {\"accuracy\": 1.0, \"loss\": 0.0009132615523412824, \"time-step\": 4316}, {\"accuracy\": 1.0, \"loss\": 0.0009224583627656102, \"time-step\": 4317}, {\"accuracy\": 1.0, \"loss\": 0.0009125572396442294, \"time-step\": 4318}, {\"accuracy\": 1.0, \"loss\": 0.0009217523620463908, \"time-step\": 4319}, {\"accuracy\": 1.0, \"loss\": 0.0009118432062678039, \"time-step\": 4320}, {\"accuracy\": 1.0, \"loss\": 0.0009210267453454435, \"time-step\": 4321}, {\"accuracy\": 1.0, \"loss\": 0.0009111107792705297, \"time-step\": 4322}, {\"accuracy\": 1.0, \"loss\": 0.0009202876244671643, \"time-step\": 4323}, {\"accuracy\": 1.0, \"loss\": 0.0009103792835958302, \"time-step\": 4324}, {\"accuracy\": 1.0, \"loss\": 0.0009195560123771429, \"time-step\": 4325}, {\"accuracy\": 1.0, \"loss\": 0.0009096745634451509, \"time-step\": 4326}, {\"accuracy\": 1.0, \"loss\": 0.0009188415133394301, \"time-step\": 4327}, {\"accuracy\": 1.0, \"loss\": 0.0009089552331715822, \"time-step\": 4328}, {\"accuracy\": 1.0, \"loss\": 0.000918135279789567, \"time-step\": 4329}, {\"accuracy\": 1.0, \"loss\": 0.0009082644828595221, \"time-step\": 4330}, {\"accuracy\": 1.0, \"loss\": 0.00091742625227198, \"time-step\": 4331}, {\"accuracy\": 1.0, \"loss\": 0.0009075470152311027, \"time-step\": 4332}, {\"accuracy\": 1.0, \"loss\": 0.0009167017415165901, \"time-step\": 4333}, {\"accuracy\": 1.0, \"loss\": 0.0009068435174413025, \"time-step\": 4334}, {\"accuracy\": 1.0, \"loss\": 0.0009160107001662254, \"time-step\": 4335}, {\"accuracy\": 1.0, \"loss\": 0.0009061516611836851, \"time-step\": 4336}, {\"accuracy\": 1.0, \"loss\": 0.0009152917773462832, \"time-step\": 4337}, {\"accuracy\": 1.0, \"loss\": 0.0009054335532709956, \"time-step\": 4338}, {\"accuracy\": 1.0, \"loss\": 0.000914566102437675, \"time-step\": 4339}, {\"accuracy\": 1.0, \"loss\": 0.000904716202057898, \"time-step\": 4340}, {\"accuracy\": 1.0, \"loss\": 0.000913848343770951, \"time-step\": 4341}, {\"accuracy\": 1.0, \"loss\": 0.0009039970464073122, \"time-step\": 4342}, {\"accuracy\": 1.0, \"loss\": 0.0009131233673542738, \"time-step\": 4343}, {\"accuracy\": 1.0, \"loss\": 0.0009032884263433516, \"time-step\": 4344}, {\"accuracy\": 1.0, \"loss\": 0.0009124246425926685, \"time-step\": 4345}, {\"accuracy\": 1.0, \"loss\": 0.0009025913896039128, \"time-step\": 4346}, {\"accuracy\": 1.0, \"loss\": 0.000911719398573041, \"time-step\": 4347}, {\"accuracy\": 1.0, \"loss\": 0.0009019023273140192, \"time-step\": 4348}, {\"accuracy\": 1.0, \"loss\": 0.0009110050741583109, \"time-step\": 4349}, {\"accuracy\": 1.0, \"loss\": 0.0009011728689074516, \"time-step\": 4350}, {\"accuracy\": 1.0, \"loss\": 0.0009102858603000641, \"time-step\": 4351}, {\"accuracy\": 1.0, \"loss\": 0.0009004819439724088, \"time-step\": 4352}, {\"accuracy\": 1.0, \"loss\": 0.0009095821296796203, \"time-step\": 4353}, {\"accuracy\": 1.0, \"loss\": 0.0008997613331303, \"time-step\": 4354}, {\"accuracy\": 1.0, \"loss\": 0.0009088508086279035, \"time-step\": 4355}, {\"accuracy\": 1.0, \"loss\": 0.0008990606293082237, \"time-step\": 4356}, {\"accuracy\": 1.0, \"loss\": 0.0009081698954105377, \"time-step\": 4357}, {\"accuracy\": 1.0, \"loss\": 0.0008983804727904499, \"time-step\": 4358}, {\"accuracy\": 1.0, \"loss\": 0.0009074897388927639, \"time-step\": 4359}, {\"accuracy\": 1.0, \"loss\": 0.000897702295333147, \"time-step\": 4360}, {\"accuracy\": 1.0, \"loss\": 0.000906786706764251, \"time-step\": 4361}, {\"accuracy\": 1.0, \"loss\": 0.0008970040362328291, \"time-step\": 4362}, {\"accuracy\": 1.0, \"loss\": 0.000906085129827261, \"time-step\": 4363}, {\"accuracy\": 1.0, \"loss\": 0.0008963050204329193, \"time-step\": 4364}, {\"accuracy\": 1.0, \"loss\": 0.0009053690009750426, \"time-step\": 4365}, {\"accuracy\": 1.0, \"loss\": 0.0008955879602581263, \"time-step\": 4366}, {\"accuracy\": 1.0, \"loss\": 0.0009046641644090414, \"time-step\": 4367}, {\"accuracy\": 1.0, \"loss\": 0.000894898665137589, \"time-step\": 4368}, {\"accuracy\": 1.0, \"loss\": 0.000903973588719964, \"time-step\": 4369}, {\"accuracy\": 1.0, \"loss\": 0.0008942100103013217, \"time-step\": 4370}, {\"accuracy\": 1.0, \"loss\": 0.000903282780200243, \"time-step\": 4371}, {\"accuracy\": 1.0, \"loss\": 0.0008935281075537205, \"time-step\": 4372}, {\"accuracy\": 1.0, \"loss\": 0.0009025875478982925, \"time-step\": 4373}, {\"accuracy\": 1.0, \"loss\": 0.000892851734533906, \"time-step\": 4374}, {\"accuracy\": 1.0, \"loss\": 0.0009019084391184151, \"time-step\": 4375}, {\"accuracy\": 1.0, \"loss\": 0.0008921701228246093, \"time-step\": 4376}, {\"accuracy\": 1.0, \"loss\": 0.0009012168738991022, \"time-step\": 4377}, {\"accuracy\": 1.0, \"loss\": 0.0008914853679016232, \"time-step\": 4378}, {\"accuracy\": 1.0, \"loss\": 0.0009005463216453791, \"time-step\": 4379}, {\"accuracy\": 1.0, \"loss\": 0.0008908312884159386, \"time-step\": 4380}, {\"accuracy\": 1.0, \"loss\": 0.0008998660487122834, \"time-step\": 4381}, {\"accuracy\": 1.0, \"loss\": 0.0008901404798962176, \"time-step\": 4382}, {\"accuracy\": 1.0, \"loss\": 0.0008991853101179004, \"time-step\": 4383}, {\"accuracy\": 1.0, \"loss\": 0.0008894777856767178, \"time-step\": 4384}, {\"accuracy\": 1.0, \"loss\": 0.0008985226741060615, \"time-step\": 4385}, {\"accuracy\": 1.0, \"loss\": 0.0008888199226930737, \"time-step\": 4386}, {\"accuracy\": 1.0, \"loss\": 0.0008978443220257759, \"time-step\": 4387}, {\"accuracy\": 1.0, \"loss\": 0.0008881303947418928, \"time-step\": 4388}, {\"accuracy\": 1.0, \"loss\": 0.0008971535135060549, \"time-step\": 4389}, {\"accuracy\": 1.0, \"loss\": 0.0008874682825990021, \"time-step\": 4390}, {\"accuracy\": 1.0, \"loss\": 0.0008964883745647967, \"time-step\": 4391}, {\"accuracy\": 1.0, \"loss\": 0.0008867866126820445, \"time-step\": 4392}, {\"accuracy\": 1.0, \"loss\": 0.0008957790560089052, \"time-step\": 4393}, {\"accuracy\": 1.0, \"loss\": 0.0008860797388479114, \"time-step\": 4394}, {\"accuracy\": 1.0, \"loss\": 0.0008950804476626217, \"time-step\": 4395}, {\"accuracy\": 1.0, \"loss\": 0.0008854022598825395, \"time-step\": 4396}, {\"accuracy\": 1.0, \"loss\": 0.0008944175788201392, \"time-step\": 4397}, {\"accuracy\": 1.0, \"loss\": 0.0008847502176649868, \"time-step\": 4398}, {\"accuracy\": 1.0, \"loss\": 0.0008937540696933866, \"time-step\": 4399}, {\"accuracy\": 1.0, \"loss\": 0.0008840932277962565, \"time-step\": 4400}, {\"accuracy\": 1.0, \"loss\": 0.0008930881740525365, \"time-step\": 4401}, {\"accuracy\": 1.0, \"loss\": 0.0008834125474095345, \"time-step\": 4402}, {\"accuracy\": 1.0, \"loss\": 0.0008923909044824541, \"time-step\": 4403}, {\"accuracy\": 1.0, \"loss\": 0.00088272470748052, \"time-step\": 4404}, {\"accuracy\": 1.0, \"loss\": 0.0008916975930333138, \"time-step\": 4405}, {\"accuracy\": 1.0, \"loss\": 0.000882050720974803, \"time-step\": 4406}, {\"accuracy\": 1.0, \"loss\": 0.0008910453179851174, \"time-step\": 4407}, {\"accuracy\": 1.0, \"loss\": 0.0008814187604002655, \"time-step\": 4408}, {\"accuracy\": 1.0, \"loss\": 0.000890398514457047, \"time-step\": 4409}, {\"accuracy\": 1.0, \"loss\": 0.0008807433769106865, \"time-step\": 4410}, {\"accuracy\": 1.0, \"loss\": 0.000889707705937326, \"time-step\": 4411}, {\"accuracy\": 1.0, \"loss\": 0.000880085863173008, \"time-step\": 4412}, {\"accuracy\": 1.0, \"loss\": 0.000889045768417418, \"time-step\": 4413}, {\"accuracy\": 1.0, \"loss\": 0.0008794106543064117, \"time-step\": 4414}, {\"accuracy\": 1.0, \"loss\": 0.0008883799309842288, \"time-step\": 4415}, {\"accuracy\": 1.0, \"loss\": 0.0008787724655121565, \"time-step\": 4416}, {\"accuracy\": 1.0, \"loss\": 0.0008877222426235676, \"time-step\": 4417}, {\"accuracy\": 1.0, \"loss\": 0.0008781144861131907, \"time-step\": 4418}, {\"accuracy\": 1.0, \"loss\": 0.0008870680467225611, \"time-step\": 4419}, {\"accuracy\": 1.0, \"loss\": 0.0008774591260589659, \"time-step\": 4420}, {\"accuracy\": 1.0, \"loss\": 0.0008864160045050085, \"time-step\": 4421}, {\"accuracy\": 1.0, \"loss\": 0.0008768195984885097, \"time-step\": 4422}, {\"accuracy\": 1.0, \"loss\": 0.0008857722859829664, \"time-step\": 4423}, {\"accuracy\": 1.0, \"loss\": 0.0008761726785451174, \"time-step\": 4424}, {\"accuracy\": 1.0, \"loss\": 0.0008851016173139215, \"time-step\": 4425}, {\"accuracy\": 1.0, \"loss\": 0.0008754939772188663, \"time-step\": 4426}, {\"accuracy\": 1.0, \"loss\": 0.0008844303665682673, \"time-step\": 4427}, {\"accuracy\": 1.0, \"loss\": 0.0008748503169044852, \"time-step\": 4428}, {\"accuracy\": 1.0, \"loss\": 0.0008837825153023005, \"time-step\": 4429}, {\"accuracy\": 1.0, \"loss\": 0.0008742095669731498, \"time-step\": 4430}, {\"accuracy\": 1.0, \"loss\": 0.0008831327431835234, \"time-step\": 4431}, {\"accuracy\": 1.0, \"loss\": 0.0008735531591810286, \"time-step\": 4432}, {\"accuracy\": 1.0, \"loss\": 0.0008824790129438043, \"time-step\": 4433}, {\"accuracy\": 1.0, \"loss\": 0.0008729081018827856, \"time-step\": 4434}, {\"accuracy\": 1.0, \"loss\": 0.0008818241767585278, \"time-step\": 4435}, {\"accuracy\": 1.0, \"loss\": 0.000872271484695375, \"time-step\": 4436}, {\"accuracy\": 1.0, \"loss\": 0.0008811839506961405, \"time-step\": 4437}, {\"accuracy\": 1.0, \"loss\": 0.0008716199663467705, \"time-step\": 4438}, {\"accuracy\": 1.0, \"loss\": 0.0008805149118416011, \"time-step\": 4439}, {\"accuracy\": 1.0, \"loss\": 0.0008709533140063286, \"time-step\": 4440}, {\"accuracy\": 1.0, \"loss\": 0.0008798505295999348, \"time-step\": 4441}, {\"accuracy\": 1.0, \"loss\": 0.0008703077910467982, \"time-step\": 4442}, {\"accuracy\": 1.0, \"loss\": 0.00087919388897717, \"time-step\": 4443}, {\"accuracy\": 1.0, \"loss\": 0.0008696273434907198, \"time-step\": 4444}, {\"accuracy\": 1.0, \"loss\": 0.0008785159443505108, \"time-step\": 4445}, {\"accuracy\": 1.0, \"loss\": 0.000868993520271033, \"time-step\": 4446}, {\"accuracy\": 1.0, \"loss\": 0.0008778921328485012, \"time-step\": 4447}, {\"accuracy\": 1.0, \"loss\": 0.0008683633059263229, \"time-step\": 4448}, {\"accuracy\": 1.0, \"loss\": 0.0008772372384555638, \"time-step\": 4449}, {\"accuracy\": 1.0, \"loss\": 0.0008677156874909997, \"time-step\": 4450}, {\"accuracy\": 1.0, \"loss\": 0.0008765948587097228, \"time-step\": 4451}, {\"accuracy\": 1.0, \"loss\": 0.0008670897805131972, \"time-step\": 4452}, {\"accuracy\": 1.0, \"loss\": 0.0008759687189012766, \"time-step\": 4453}, {\"accuracy\": 1.0, \"loss\": 0.0008664510678499937, \"time-step\": 4454}, {\"accuracy\": 1.0, \"loss\": 0.0008753171423450112, \"time-step\": 4455}, {\"accuracy\": 1.0, \"loss\": 0.0008658156148158014, \"time-step\": 4456}, {\"accuracy\": 1.0, \"loss\": 0.0008746766252443194, \"time-step\": 4457}, {\"accuracy\": 1.0, \"loss\": 0.0008651806274428964, \"time-step\": 4458}, {\"accuracy\": 1.0, \"loss\": 0.0008740589837543666, \"time-step\": 4459}, {\"accuracy\": 1.0, \"loss\": 0.0008645702619105577, \"time-step\": 4460}, {\"accuracy\": 1.0, \"loss\": 0.0008734248112887144, \"time-step\": 4461}, {\"accuracy\": 1.0, \"loss\": 0.000863931025378406, \"time-step\": 4462}, {\"accuracy\": 1.0, \"loss\": 0.000872795470058918, \"time-step\": 4463}, {\"accuracy\": 1.0, \"loss\": 0.0008633060497231781, \"time-step\": 4464}, {\"accuracy\": 1.0, \"loss\": 0.0008721580961719155, \"time-step\": 4465}, {\"accuracy\": 1.0, \"loss\": 0.0008626726921647787, \"time-step\": 4466}, {\"accuracy\": 1.0, \"loss\": 0.000871507974807173, \"time-step\": 4467}, {\"accuracy\": 1.0, \"loss\": 0.0008620311855338514, \"time-step\": 4468}, {\"accuracy\": 1.0, \"loss\": 0.0008708664681762457, \"time-step\": 4469}, {\"accuracy\": 1.0, \"loss\": 0.0008613920072093606, \"time-step\": 4470}, {\"accuracy\": 1.0, \"loss\": 0.0008702331688255072, \"time-step\": 4471}, {\"accuracy\": 1.0, \"loss\": 0.0008607734343968332, \"time-step\": 4472}, {\"accuracy\": 1.0, \"loss\": 0.0008696023724041879, \"time-step\": 4473}, {\"accuracy\": 1.0, \"loss\": 0.000860140600707382, \"time-step\": 4474}, {\"accuracy\": 1.0, \"loss\": 0.0008689601090736687, \"time-step\": 4475}, {\"accuracy\": 1.0, \"loss\": 0.0008595070685259998, \"time-step\": 4476}, {\"accuracy\": 1.0, \"loss\": 0.0008683206979185343, \"time-step\": 4477}, {\"accuracy\": 1.0, \"loss\": 0.0008588631171733141, \"time-step\": 4478}, {\"accuracy\": 1.0, \"loss\": 0.0008676659199409187, \"time-step\": 4479}, {\"accuracy\": 1.0, \"loss\": 0.0008582206210121512, \"time-step\": 4480}, {\"accuracy\": 1.0, \"loss\": 0.000867040769662708, \"time-step\": 4481}, {\"accuracy\": 1.0, \"loss\": 0.0008576078107580543, \"time-step\": 4482}, {\"accuracy\": 1.0, \"loss\": 0.0008664300548844039, \"time-step\": 4483}, {\"accuracy\": 1.0, \"loss\": 0.0008570037898607552, \"time-step\": 4484}, {\"accuracy\": 1.0, \"loss\": 0.0008658011211082339, \"time-step\": 4485}, {\"accuracy\": 1.0, \"loss\": 0.0008563750307075679, \"time-step\": 4486}, {\"accuracy\": 1.0, \"loss\": 0.0008651823154650629, \"time-step\": 4487}, {\"accuracy\": 1.0, \"loss\": 0.0008557676337659359, \"time-step\": 4488}, {\"accuracy\": 1.0, \"loss\": 0.0008645495981909335, \"time-step\": 4489}, {\"accuracy\": 1.0, \"loss\": 0.0008551342762075365, \"time-step\": 4490}, {\"accuracy\": 1.0, \"loss\": 0.000863923691213131, \"time-step\": 4491}, {\"accuracy\": 1.0, \"loss\": 0.0008545169839635491, \"time-step\": 4492}, {\"accuracy\": 1.0, \"loss\": 0.0008632965618744493, \"time-step\": 4493}, {\"accuracy\": 1.0, \"loss\": 0.0008539026021026075, \"time-step\": 4494}, {\"accuracy\": 1.0, \"loss\": 0.0008626780472695827, \"time-step\": 4495}, {\"accuracy\": 1.0, \"loss\": 0.0008532756473869085, \"time-step\": 4496}, {\"accuracy\": 1.0, \"loss\": 0.0008620606968179345, \"time-step\": 4497}, {\"accuracy\": 1.0, \"loss\": 0.0008526740712113678, \"time-step\": 4498}, {\"accuracy\": 1.0, \"loss\": 0.0008614464895799756, \"time-step\": 4499}, {\"accuracy\": 1.0, \"loss\": 0.0008520514820702374, \"time-step\": 4500}, {\"accuracy\": 1.0, \"loss\": 0.0008607974159531295, \"time-step\": 4501}, {\"accuracy\": 1.0, \"loss\": 0.0008514116052538157, \"time-step\": 4502}, {\"accuracy\": 1.0, \"loss\": 0.0008601770387031138, \"time-step\": 4503}, {\"accuracy\": 1.0, \"loss\": 0.0008508003083989024, \"time-step\": 4504}, {\"accuracy\": 1.0, \"loss\": 0.0008595670224167407, \"time-step\": 4505}, {\"accuracy\": 1.0, \"loss\": 0.0008502034470438957, \"time-step\": 4506}, {\"accuracy\": 1.0, \"loss\": 0.0008589477511122823, \"time-step\": 4507}, {\"accuracy\": 1.0, \"loss\": 0.00084958371007815, \"time-step\": 4508}, {\"accuracy\": 1.0, \"loss\": 0.0008583224262110889, \"time-step\": 4509}, {\"accuracy\": 1.0, \"loss\": 0.0008489691535942256, \"time-step\": 4510}, {\"accuracy\": 1.0, \"loss\": 0.0008577101398259401, \"time-step\": 4511}, {\"accuracy\": 1.0, \"loss\": 0.0008483532583341002, \"time-step\": 4512}, {\"accuracy\": 1.0, \"loss\": 0.0008570852223783731, \"time-step\": 4513}, {\"accuracy\": 1.0, \"loss\": 0.0008477485389448702, \"time-step\": 4514}, {\"accuracy\": 1.0, \"loss\": 0.0008564950549043715, \"time-step\": 4515}, {\"accuracy\": 1.0, \"loss\": 0.0008471650071442127, \"time-step\": 4516}, {\"accuracy\": 1.0, \"loss\": 0.0008558992994949222, \"time-step\": 4517}, {\"accuracy\": 1.0, \"loss\": 0.000846564827952534, \"time-step\": 4518}, {\"accuracy\": 1.0, \"loss\": 0.0008552903309464455, \"time-step\": 4519}, {\"accuracy\": 1.0, \"loss\": 0.0008459521923214197, \"time-step\": 4520}, {\"accuracy\": 1.0, \"loss\": 0.0008546652970835567, \"time-step\": 4521}, {\"accuracy\": 1.0, \"loss\": 0.0008453510818071663, \"time-step\": 4522}, {\"accuracy\": 1.0, \"loss\": 0.0008540679118596017, \"time-step\": 4523}, {\"accuracy\": 1.0, \"loss\": 0.0008447431027889252, \"time-step\": 4524}, {\"accuracy\": 1.0, \"loss\": 0.0008534665103070438, \"time-step\": 4525}, {\"accuracy\": 1.0, \"loss\": 0.0008441524696536362, \"time-step\": 4526}, {\"accuracy\": 1.0, \"loss\": 0.0008528555044904351, \"time-step\": 4527}, {\"accuracy\": 1.0, \"loss\": 0.0008435453055426478, \"time-step\": 4528}, {\"accuracy\": 1.0, \"loss\": 0.0008522426942363381, \"time-step\": 4529}, {\"accuracy\": 1.0, \"loss\": 0.0008429407025687397, \"time-step\": 4530}, {\"accuracy\": 1.0, \"loss\": 0.0008516497910022736, \"time-step\": 4531}, {\"accuracy\": 1.0, \"loss\": 0.000842363340780139, \"time-step\": 4532}, {\"accuracy\": 1.0, \"loss\": 0.0008510589832440019, \"time-step\": 4533}, {\"accuracy\": 1.0, \"loss\": 0.0008417592034675181, \"time-step\": 4534}, {\"accuracy\": 1.0, \"loss\": 0.0008504593861289322, \"time-step\": 4535}, {\"accuracy\": 1.0, \"loss\": 0.0008411813178099692, \"time-step\": 4536}, {\"accuracy\": 1.0, \"loss\": 0.0008498742827214301, \"time-step\": 4537}, {\"accuracy\": 1.0, \"loss\": 0.0008405852131545544, \"time-step\": 4538}, {\"accuracy\": 1.0, \"loss\": 0.000849263567943126, \"time-step\": 4539}, {\"accuracy\": 1.0, \"loss\": 0.0008399906218983233, \"time-step\": 4540}, {\"accuracy\": 1.0, \"loss\": 0.0008486623992212117, \"time-step\": 4541}, {\"accuracy\": 1.0, \"loss\": 0.0008393876487389207, \"time-step\": 4542}, {\"accuracy\": 1.0, \"loss\": 0.0008480542455799878, \"time-step\": 4543}, {\"accuracy\": 1.0, \"loss\": 0.0008387928828597069, \"time-step\": 4544}, {\"accuracy\": 1.0, \"loss\": 0.000847457442432642, \"time-step\": 4545}, {\"accuracy\": 1.0, \"loss\": 0.0008382006781175733, \"time-step\": 4546}, {\"accuracy\": 1.0, \"loss\": 0.0008468727464787662, \"time-step\": 4547}, {\"accuracy\": 1.0, \"loss\": 0.0008376094046980143, \"time-step\": 4548}, {\"accuracy\": 1.0, \"loss\": 0.0008462603436782956, \"time-step\": 4549}, {\"accuracy\": 1.0, \"loss\": 0.0008370220893993974, \"time-step\": 4550}, {\"accuracy\": 1.0, \"loss\": 0.0008456825744360685, \"time-step\": 4551}, {\"accuracy\": 1.0, \"loss\": 0.0008364475797861814, \"time-step\": 4552}, {\"accuracy\": 1.0, \"loss\": 0.0008450956083834171, \"time-step\": 4553}, {\"accuracy\": 1.0, \"loss\": 0.0008358572376891971, \"time-step\": 4554}, {\"accuracy\": 1.0, \"loss\": 0.0008445002604275942, \"time-step\": 4555}, {\"accuracy\": 1.0, \"loss\": 0.0008352588047273457, \"time-step\": 4556}, {\"accuracy\": 1.0, \"loss\": 0.0008438960649073124, \"time-step\": 4557}, {\"accuracy\": 1.0, \"loss\": 0.0008346660761162639, \"time-step\": 4558}, {\"accuracy\": 1.0, \"loss\": 0.0008433029870502651, \"time-step\": 4559}, {\"accuracy\": 1.0, \"loss\": 0.0008340826025232673, \"time-step\": 4560}, {\"accuracy\": 1.0, \"loss\": 0.0008427105494774878, \"time-step\": 4561}, {\"accuracy\": 1.0, \"loss\": 0.0008334862068295479, \"time-step\": 4562}, {\"accuracy\": 1.0, \"loss\": 0.0008421142119914293, \"time-step\": 4563}, {\"accuracy\": 1.0, \"loss\": 0.0008328973781317472, \"time-step\": 4564}, {\"accuracy\": 1.0, \"loss\": 0.0008415285847149789, \"time-step\": 4565}, {\"accuracy\": 1.0, \"loss\": 0.0008323254296556115, \"time-step\": 4566}, {\"accuracy\": 1.0, \"loss\": 0.0008409471483901143, \"time-step\": 4567}, {\"accuracy\": 1.0, \"loss\": 0.0008317477768287063, \"time-step\": 4568}, {\"accuracy\": 1.0, \"loss\": 0.0008403559913858771, \"time-step\": 4569}, {\"accuracy\": 1.0, \"loss\": 0.0008311660494655371, \"time-step\": 4570}, {\"accuracy\": 1.0, \"loss\": 0.0008397795609198511, \"time-step\": 4571}, {\"accuracy\": 1.0, \"loss\": 0.0008305845549330115, \"time-step\": 4572}, {\"accuracy\": 1.0, \"loss\": 0.0008391889277845621, \"time-step\": 4573}, {\"accuracy\": 1.0, \"loss\": 0.0008300010813400149, \"time-step\": 4574}, {\"accuracy\": 1.0, \"loss\": 0.0008386042900383472, \"time-step\": 4575}, {\"accuracy\": 1.0, \"loss\": 0.00082944001769647, \"time-step\": 4576}, {\"accuracy\": 1.0, \"loss\": 0.0008380459039472044, \"time-step\": 4577}, {\"accuracy\": 1.0, \"loss\": 0.0008288688841275871, \"time-step\": 4578}, {\"accuracy\": 1.0, \"loss\": 0.0008374606259167194, \"time-step\": 4579}, {\"accuracy\": 1.0, \"loss\": 0.0008282907074317336, \"time-step\": 4580}, {\"accuracy\": 1.0, \"loss\": 0.0008368752896785736, \"time-step\": 4581}, {\"accuracy\": 1.0, \"loss\": 0.000827704556286335, \"time-step\": 4582}, {\"accuracy\": 1.0, \"loss\": 0.0008362854132428765, \"time-step\": 4583}, {\"accuracy\": 1.0, \"loss\": 0.000827122014015913, \"time-step\": 4584}, {\"accuracy\": 1.0, \"loss\": 0.000835695187561214, \"time-step\": 4585}, {\"accuracy\": 1.0, \"loss\": 0.0008265498909167945, \"time-step\": 4586}, {\"accuracy\": 1.0, \"loss\": 0.000835130806080997, \"time-step\": 4587}, {\"accuracy\": 1.0, \"loss\": 0.0008259870810434222, \"time-step\": 4588}, {\"accuracy\": 1.0, \"loss\": 0.0008345509995706379, \"time-step\": 4589}, {\"accuracy\": 1.0, \"loss\": 0.0008253940031863749, \"time-step\": 4590}, {\"accuracy\": 1.0, \"loss\": 0.0008339622290804982, \"time-step\": 4591}, {\"accuracy\": 1.0, \"loss\": 0.0008248113445006311, \"time-step\": 4592}, {\"accuracy\": 1.0, \"loss\": 0.000833389931358397, \"time-step\": 4593}, {\"accuracy\": 1.0, \"loss\": 0.0008242673939093947, \"time-step\": 4594}, {\"accuracy\": 1.0, \"loss\": 0.0008328226977027953, \"time-step\": 4595}, {\"accuracy\": 1.0, \"loss\": 0.0008236901485361159, \"time-step\": 4596}, {\"accuracy\": 1.0, \"loss\": 0.0008322346839122474, \"time-step\": 4597}, {\"accuracy\": 1.0, \"loss\": 0.0008231059182435274, \"time-step\": 4598}, {\"accuracy\": 1.0, \"loss\": 0.0008316482417285442, \"time-step\": 4599}, {\"accuracy\": 1.0, \"loss\": 0.0008225249475799501, \"time-step\": 4600}, {\"accuracy\": 1.0, \"loss\": 0.0008310600533150136, \"time-step\": 4601}, {\"accuracy\": 1.0, \"loss\": 0.000821957248263061, \"time-step\": 4602}, {\"accuracy\": 1.0, \"loss\": 0.0008305070223286748, \"time-step\": 4603}, {\"accuracy\": 1.0, \"loss\": 0.0008214185363613069, \"time-step\": 4604}, {\"accuracy\": 1.0, \"loss\": 0.0008299561450257897, \"time-step\": 4605}, {\"accuracy\": 1.0, \"loss\": 0.0008208481594920158, \"time-step\": 4606}, {\"accuracy\": 1.0, \"loss\": 0.0008293733699247241, \"time-step\": 4607}, {\"accuracy\": 1.0, \"loss\": 0.0008202592143788934, \"time-step\": 4608}, {\"accuracy\": 1.0, \"loss\": 0.0008287790114991367, \"time-step\": 4609}, {\"accuracy\": 1.0, \"loss\": 0.0008196987328119576, \"time-step\": 4610}, {\"accuracy\": 1.0, \"loss\": 0.000828234595246613, \"time-step\": 4611}, {\"accuracy\": 1.0, \"loss\": 0.000819141510874033, \"time-step\": 4612}, {\"accuracy\": 1.0, \"loss\": 0.000827659503556788, \"time-step\": 4613}, {\"accuracy\": 1.0, \"loss\": 0.0008185853948816657, \"time-step\": 4614}, {\"accuracy\": 1.0, \"loss\": 0.000827107927761972, \"time-step\": 4615}, {\"accuracy\": 1.0, \"loss\": 0.0008180219447240233, \"time-step\": 4616}, {\"accuracy\": 1.0, \"loss\": 0.0008265237556770444, \"time-step\": 4617}, {\"accuracy\": 1.0, \"loss\": 0.0008174499962478876, \"time-step\": 4618}, {\"accuracy\": 1.0, \"loss\": 0.00082595698768273, \"time-step\": 4619}, {\"accuracy\": 1.0, \"loss\": 0.0008169024367816746, \"time-step\": 4620}, {\"accuracy\": 1.0, \"loss\": 0.0008254183921962976, \"time-step\": 4621}, {\"accuracy\": 1.0, \"loss\": 0.0008163698948919773, \"time-step\": 4622}, {\"accuracy\": 1.0, \"loss\": 0.0008248690865002573, \"time-step\": 4623}, {\"accuracy\": 1.0, \"loss\": 0.0008158180862665176, \"time-step\": 4624}, {\"accuracy\": 1.0, \"loss\": 0.0008243228658102453, \"time-step\": 4625}, {\"accuracy\": 1.0, \"loss\": 0.0008152733789756894, \"time-step\": 4626}, {\"accuracy\": 1.0, \"loss\": 0.0008237621514126658, \"time-step\": 4627}, {\"accuracy\": 1.0, \"loss\": 0.0008147097541950643, \"time-step\": 4628}, {\"accuracy\": 1.0, \"loss\": 0.0008232034160755575, \"time-step\": 4629}, {\"accuracy\": 1.0, \"loss\": 0.0008141545695252717, \"time-step\": 4630}, {\"accuracy\": 1.0, \"loss\": 0.0008226248901337385, \"time-step\": 4631}, {\"accuracy\": 1.0, \"loss\": 0.0008135936222970486, \"time-step\": 4632}, {\"accuracy\": 1.0, \"loss\": 0.0008220779127441347, \"time-step\": 4633}, {\"accuracy\": 1.0, \"loss\": 0.000813043094240129, \"time-step\": 4634}, {\"accuracy\": 1.0, \"loss\": 0.0008215120760723948, \"time-step\": 4635}, {\"accuracy\": 1.0, \"loss\": 0.0008124860469251871, \"time-step\": 4636}, {\"accuracy\": 1.0, \"loss\": 0.000820955669041723, \"time-step\": 4637}, {\"accuracy\": 1.0, \"loss\": 0.0008119448903016746, \"time-step\": 4638}, {\"accuracy\": 1.0, \"loss\": 0.0008204197511076927, \"time-step\": 4639}, {\"accuracy\": 1.0, \"loss\": 0.0008114215452224016, \"time-step\": 4640}, {\"accuracy\": 1.0, \"loss\": 0.0008198840077966452, \"time-step\": 4641}, {\"accuracy\": 1.0, \"loss\": 0.0008108728216029704, \"time-step\": 4642}, {\"accuracy\": 1.0, \"loss\": 0.0008193308021873236, \"time-step\": 4643}, {\"accuracy\": 1.0, \"loss\": 0.0008103234576992691, \"time-step\": 4644}, {\"accuracy\": 1.0, \"loss\": 0.0008187835337594151, \"time-step\": 4645}, {\"accuracy\": 1.0, \"loss\": 0.0008097919053398073, \"time-step\": 4646}, {\"accuracy\": 1.0, \"loss\": 0.0008182514575310051, \"time-step\": 4647}, {\"accuracy\": 1.0, \"loss\": 0.0008092586067505181, \"time-step\": 4648}, {\"accuracy\": 1.0, \"loss\": 0.0008177008712664247, \"time-step\": 4649}, {\"accuracy\": 1.0, \"loss\": 0.000808709766715765, \"time-step\": 4650}, {\"accuracy\": 1.0, \"loss\": 0.0008171480149030685, \"time-step\": 4651}, {\"accuracy\": 1.0, \"loss\": 0.0008081553387455642, \"time-step\": 4652}, {\"accuracy\": 1.0, \"loss\": 0.0008165886392816901, \"time-step\": 4653}, {\"accuracy\": 1.0, \"loss\": 0.00080761534627527, \"time-step\": 4654}, {\"accuracy\": 1.0, \"loss\": 0.0008160392171703279, \"time-step\": 4655}, {\"accuracy\": 1.0, \"loss\": 0.0008070527110248804, \"time-step\": 4656}, {\"accuracy\": 1.0, \"loss\": 0.0008154729148373008, \"time-step\": 4657}, {\"accuracy\": 1.0, \"loss\": 0.0008064954308792949, \"time-step\": 4658}, {\"accuracy\": 1.0, \"loss\": 0.0008149194763973355, \"time-step\": 4659}, {\"accuracy\": 1.0, \"loss\": 0.0008059628307819366, \"time-step\": 4660}, {\"accuracy\": 1.0, \"loss\": 0.0008143901941366494, \"time-step\": 4661}, {\"accuracy\": 1.0, \"loss\": 0.0008054433856159449, \"time-step\": 4662}, {\"accuracy\": 1.0, \"loss\": 0.0008138720877468586, \"time-step\": 4663}, {\"accuracy\": 1.0, \"loss\": 0.0008049173629842699, \"time-step\": 4664}, {\"accuracy\": 1.0, \"loss\": 0.0008133308147080243, \"time-step\": 4665}, {\"accuracy\": 1.0, \"loss\": 0.0008043849375098944, \"time-step\": 4666}, {\"accuracy\": 1.0, \"loss\": 0.0008128010085783899, \"time-step\": 4667}, {\"accuracy\": 1.0, \"loss\": 0.0008038586238399148, \"time-step\": 4668}, {\"accuracy\": 1.0, \"loss\": 0.00081225874600932, \"time-step\": 4669}, {\"accuracy\": 1.0, \"loss\": 0.0008033282938413322, \"time-step\": 4670}, {\"accuracy\": 1.0, \"loss\": 0.0008117378456518054, \"time-step\": 4671}, {\"accuracy\": 1.0, \"loss\": 0.0008027921430766582, \"time-step\": 4672}, {\"accuracy\": 1.0, \"loss\": 0.0008111890638247132, \"time-step\": 4673}, {\"accuracy\": 1.0, \"loss\": 0.0008022417314350605, \"time-step\": 4674}, {\"accuracy\": 1.0, \"loss\": 0.0008106192690320313, \"time-step\": 4675}, {\"accuracy\": 1.0, \"loss\": 0.0008016913780011237, \"time-step\": 4676}, {\"accuracy\": 1.0, \"loss\": 0.0008100966806523502, \"time-step\": 4677}, {\"accuracy\": 1.0, \"loss\": 0.0008011921308934689, \"time-step\": 4678}, {\"accuracy\": 1.0, \"loss\": 0.0008095767116174102, \"time-step\": 4679}, {\"accuracy\": 1.0, \"loss\": 0.0008006551652215421, \"time-step\": 4680}, {\"accuracy\": 1.0, \"loss\": 0.0008090410847216845, \"time-step\": 4681}, {\"accuracy\": 1.0, \"loss\": 0.0008001375244930387, \"time-step\": 4682}, {\"accuracy\": 1.0, \"loss\": 0.0008085161680355668, \"time-step\": 4683}, {\"accuracy\": 1.0, \"loss\": 0.0007996167987585068, \"time-step\": 4684}, {\"accuracy\": 1.0, \"loss\": 0.0008079935214482248, \"time-step\": 4685}, {\"accuracy\": 1.0, \"loss\": 0.0007990947924554348, \"time-step\": 4686}, {\"accuracy\": 1.0, \"loss\": 0.0008074746001511812, \"time-step\": 4687}, {\"accuracy\": 1.0, \"loss\": 0.0007985737174749374, \"time-step\": 4688}, {\"accuracy\": 1.0, \"loss\": 0.0008069322793744504, \"time-step\": 4689}, {\"accuracy\": 1.0, \"loss\": 0.0007980427471920848, \"time-step\": 4690}, {\"accuracy\": 1.0, \"loss\": 0.0008064116118475795, \"time-step\": 4691}, {\"accuracy\": 1.0, \"loss\": 0.0007975196349434555, \"time-step\": 4692}, {\"accuracy\": 1.0, \"loss\": 0.0008058623643592, \"time-step\": 4693}, {\"accuracy\": 1.0, \"loss\": 0.0007969806902110577, \"time-step\": 4694}, {\"accuracy\": 1.0, \"loss\": 0.0008053327910602093, \"time-step\": 4695}, {\"accuracy\": 1.0, \"loss\": 0.0007964551914483309, \"time-step\": 4696}, {\"accuracy\": 1.0, \"loss\": 0.0008047979790717363, \"time-step\": 4697}, {\"accuracy\": 1.0, \"loss\": 0.0007959232898429036, \"time-step\": 4698}, {\"accuracy\": 1.0, \"loss\": 0.0008042629924602807, \"time-step\": 4699}, {\"accuracy\": 1.0, \"loss\": 0.0007953981985338032, \"time-step\": 4700}, {\"accuracy\": 1.0, \"loss\": 0.0008037355146370828, \"time-step\": 4701}, {\"accuracy\": 1.0, \"loss\": 0.0007948695565573871, \"time-step\": 4702}, {\"accuracy\": 1.0, \"loss\": 0.0008031927281990647, \"time-step\": 4703}, {\"accuracy\": 1.0, \"loss\": 0.0007943274686113, \"time-step\": 4704}, {\"accuracy\": 1.0, \"loss\": 0.0008026599534787238, \"time-step\": 4705}, {\"accuracy\": 1.0, \"loss\": 0.0007937968475744128, \"time-step\": 4706}, {\"accuracy\": 1.0, \"loss\": 0.0008021132671274245, \"time-step\": 4707}, {\"accuracy\": 1.0, \"loss\": 0.0007932598236948252, \"time-step\": 4708}, {\"accuracy\": 1.0, \"loss\": 0.000801589572802186, \"time-step\": 4709}, {\"accuracy\": 1.0, \"loss\": 0.0007927524857223034, \"time-step\": 4710}, {\"accuracy\": 1.0, \"loss\": 0.0008010717574506998, \"time-step\": 4711}, {\"accuracy\": 1.0, \"loss\": 0.0007922282093204558, \"time-step\": 4712}, {\"accuracy\": 1.0, \"loss\": 0.0008005436393432319, \"time-step\": 4713}, {\"accuracy\": 1.0, \"loss\": 0.0007917191251181066, \"time-step\": 4714}, {\"accuracy\": 1.0, \"loss\": 0.0008000324014574289, \"time-step\": 4715}, {\"accuracy\": 1.0, \"loss\": 0.000791208993177861, \"time-step\": 4716}, {\"accuracy\": 1.0, \"loss\": 0.0007995246560312808, \"time-step\": 4717}, {\"accuracy\": 1.0, \"loss\": 0.0007907052058726549, \"time-step\": 4718}, {\"accuracy\": 1.0, \"loss\": 0.0007990105077624321, \"time-step\": 4719}, {\"accuracy\": 1.0, \"loss\": 0.0007901937933638692, \"time-step\": 4720}, {\"accuracy\": 1.0, \"loss\": 0.0007984781404957175, \"time-step\": 4721}, {\"accuracy\": 1.0, \"loss\": 0.0007896493189036846, \"time-step\": 4722}, {\"accuracy\": 1.0, \"loss\": 0.0007979337824508548, \"time-step\": 4723}, {\"accuracy\": 1.0, \"loss\": 0.0007891322602517903, \"time-step\": 4724}, {\"accuracy\": 1.0, \"loss\": 0.0007974356412887573, \"time-step\": 4725}, {\"accuracy\": 1.0, \"loss\": 0.0007886465173214674, \"time-step\": 4726}, {\"accuracy\": 1.0, \"loss\": 0.0007969369180500507, \"time-step\": 4727}, {\"accuracy\": 1.0, \"loss\": 0.0007881352212280035, \"time-step\": 4728}, {\"accuracy\": 1.0, \"loss\": 0.0007964188698679209, \"time-step\": 4729}, {\"accuracy\": 1.0, \"loss\": 0.0007876300951465964, \"time-step\": 4730}, {\"accuracy\": 1.0, \"loss\": 0.0007959171780385077, \"time-step\": 4731}, {\"accuracy\": 1.0, \"loss\": 0.0007871207199059427, \"time-step\": 4732}, {\"accuracy\": 1.0, \"loss\": 0.0007954033790156245, \"time-step\": 4733}, {\"accuracy\": 1.0, \"loss\": 0.0007866197265684605, \"time-step\": 4734}, {\"accuracy\": 1.0, \"loss\": 0.0007948902202770114, \"time-step\": 4735}, {\"accuracy\": 1.0, \"loss\": 0.0007861016201786697, \"time-step\": 4736}, {\"accuracy\": 1.0, \"loss\": 0.0007943626260384917, \"time-step\": 4737}, {\"accuracy\": 1.0, \"loss\": 0.0007855850271880627, \"time-step\": 4738}, {\"accuracy\": 1.0, \"loss\": 0.0007938542403280735, \"time-step\": 4739}, {\"accuracy\": 1.0, \"loss\": 0.0007850878755562007, \"time-step\": 4740}, {\"accuracy\": 1.0, \"loss\": 0.0007933452725410461, \"time-step\": 4741}, {\"accuracy\": 1.0, \"loss\": 0.0007845815271139145, \"time-step\": 4742}, {\"accuracy\": 1.0, \"loss\": 0.0007928290287964046, \"time-step\": 4743}, {\"accuracy\": 1.0, \"loss\": 0.0007840597536414862, \"time-step\": 4744}, {\"accuracy\": 1.0, \"loss\": 0.0007923216326162219, \"time-step\": 4745}, {\"accuracy\": 1.0, \"loss\": 0.0007835774449631572, \"time-step\": 4746}, {\"accuracy\": 1.0, \"loss\": 0.0007918291375972331, \"time-step\": 4747}, {\"accuracy\": 1.0, \"loss\": 0.000783064984716475, \"time-step\": 4748}, {\"accuracy\": 1.0, \"loss\": 0.000791306491009891, \"time-step\": 4749}, {\"accuracy\": 1.0, \"loss\": 0.0007825759239494801, \"time-step\": 4750}, {\"accuracy\": 1.0, \"loss\": 0.0007908237748779356, \"time-step\": 4751}, {\"accuracy\": 1.0, \"loss\": 0.0007820812752470374, \"time-step\": 4752}, {\"accuracy\": 1.0, \"loss\": 0.0007903081132099032, \"time-step\": 4753}, {\"accuracy\": 1.0, \"loss\": 0.0007815748103894293, \"time-step\": 4754}, {\"accuracy\": 1.0, \"loss\": 0.0007898075273260474, \"time-step\": 4755}, {\"accuracy\": 1.0, \"loss\": 0.0007810722454451025, \"time-step\": 4756}, {\"accuracy\": 1.0, \"loss\": 0.0007892947178333998, \"time-step\": 4757}, {\"accuracy\": 1.0, \"loss\": 0.0007805604836903512, \"time-step\": 4758}, {\"accuracy\": 1.0, \"loss\": 0.0007887778920121491, \"time-step\": 4759}, {\"accuracy\": 1.0, \"loss\": 0.0007800634484738111, \"time-step\": 4760}, {\"accuracy\": 1.0, \"loss\": 0.0007882918580435216, \"time-step\": 4761}, {\"accuracy\": 1.0, \"loss\": 0.0007795743294991553, \"time-step\": 4762}, {\"accuracy\": 1.0, \"loss\": 0.0007877874304540455, \"time-step\": 4763}, {\"accuracy\": 1.0, \"loss\": 0.0007790817180648446, \"time-step\": 4764}, {\"accuracy\": 1.0, \"loss\": 0.0007872923160903156, \"time-step\": 4765}, {\"accuracy\": 1.0, \"loss\": 0.0007785821217112243, \"time-step\": 4766}, {\"accuracy\": 1.0, \"loss\": 0.0007867883541621268, \"time-step\": 4767}, {\"accuracy\": 1.0, \"loss\": 0.0007780803134664893, \"time-step\": 4768}, {\"accuracy\": 1.0, \"loss\": 0.0007862747879698873, \"time-step\": 4769}, {\"accuracy\": 1.0, \"loss\": 0.0007775841513648629, \"time-step\": 4770}, {\"accuracy\": 1.0, \"loss\": 0.0007857849122956395, \"time-step\": 4771}, {\"accuracy\": 1.0, \"loss\": 0.0007770883967168629, \"time-step\": 4772}, {\"accuracy\": 1.0, \"loss\": 0.0007852871785871685, \"time-step\": 4773}, {\"accuracy\": 1.0, \"loss\": 0.0007766091148369014, \"time-step\": 4774}, {\"accuracy\": 1.0, \"loss\": 0.0007848022505640984, \"time-step\": 4775}, {\"accuracy\": 1.0, \"loss\": 0.0007761165034025908, \"time-step\": 4776}, {\"accuracy\": 1.0, \"loss\": 0.000784300034865737, \"time-step\": 4777}, {\"accuracy\": 1.0, \"loss\": 0.00077561050420627, \"time-step\": 4778}, {\"accuracy\": 1.0, \"loss\": 0.0007837919401936233, \"time-step\": 4779}, {\"accuracy\": 1.0, \"loss\": 0.0007751210359856486, \"time-step\": 4780}, {\"accuracy\": 1.0, \"loss\": 0.0007833020063117146, \"time-step\": 4781}, {\"accuracy\": 1.0, \"loss\": 0.0007746372139081359, \"time-step\": 4782}, {\"accuracy\": 1.0, \"loss\": 0.0007828164962120354, \"time-step\": 4783}, {\"accuracy\": 1.0, \"loss\": 0.0007741522858850658, \"time-step\": 4784}, {\"accuracy\": 1.0, \"loss\": 0.0007823198684491217, \"time-step\": 4785}, {\"accuracy\": 1.0, \"loss\": 0.0007736592669971287, \"time-step\": 4786}, {\"accuracy\": 1.0, \"loss\": 0.0007818301673978567, \"time-step\": 4787}, {\"accuracy\": 1.0, \"loss\": 0.0007731615914963186, \"time-step\": 4788}, {\"accuracy\": 1.0, \"loss\": 0.0007813215488567948, \"time-step\": 4789}, {\"accuracy\": 1.0, \"loss\": 0.00077268440509215, \"time-step\": 4790}, {\"accuracy\": 1.0, \"loss\": 0.0007808574009686708, \"time-step\": 4791}, {\"accuracy\": 1.0, \"loss\": 0.0007722063455730677, \"time-step\": 4792}, {\"accuracy\": 1.0, \"loss\": 0.0007803666521795094, \"time-step\": 4793}, {\"accuracy\": 1.0, \"loss\": 0.0007717180415056646, \"time-step\": 4794}, {\"accuracy\": 1.0, \"loss\": 0.0007798693259246647, \"time-step\": 4795}, {\"accuracy\": 1.0, \"loss\": 0.000771211227402091, \"time-step\": 4796}, {\"accuracy\": 1.0, \"loss\": 0.0007793609402142465, \"time-step\": 4797}, {\"accuracy\": 1.0, \"loss\": 0.0007707175682298839, \"time-step\": 4798}, {\"accuracy\": 1.0, \"loss\": 0.0007788445800542831, \"time-step\": 4799}, {\"accuracy\": 1.0, \"loss\": 0.0007702187867835164, \"time-step\": 4800}, {\"accuracy\": 1.0, \"loss\": 0.0007783723413012922, \"time-step\": 4801}, {\"accuracy\": 1.0, \"loss\": 0.0007697506225667894, \"time-step\": 4802}, {\"accuracy\": 1.0, \"loss\": 0.0007778815925121307, \"time-step\": 4803}, {\"accuracy\": 1.0, \"loss\": 0.0007692514918744564, \"time-step\": 4804}, {\"accuracy\": 1.0, \"loss\": 0.000777382985688746, \"time-step\": 4805}, {\"accuracy\": 1.0, \"loss\": 0.0007687804754823446, \"time-step\": 4806}, {\"accuracy\": 1.0, \"loss\": 0.000776914821472019, \"time-step\": 4807}, {\"accuracy\": 1.0, \"loss\": 0.0007683010771870613, \"time-step\": 4808}, {\"accuracy\": 1.0, \"loss\": 0.0007764353649690747, \"time-step\": 4809}, {\"accuracy\": 1.0, \"loss\": 0.0007678295369260013, \"time-step\": 4810}, {\"accuracy\": 1.0, \"loss\": 0.0007759418222121894, \"time-step\": 4811}, {\"accuracy\": 1.0, \"loss\": 0.0007673450745642185, \"time-step\": 4812}, {\"accuracy\": 1.0, \"loss\": 0.0007754600374028087, \"time-step\": 4813}, {\"accuracy\": 1.0, \"loss\": 0.0007668546168133616, \"time-step\": 4814}, {\"accuracy\": 1.0, \"loss\": 0.0007749656797386706, \"time-step\": 4815}, {\"accuracy\": 1.0, \"loss\": 0.0007663664873689413, \"time-step\": 4816}, {\"accuracy\": 1.0, \"loss\": 0.0007744813337922096, \"time-step\": 4817}, {\"accuracy\": 1.0, \"loss\": 0.0007658945396542549, \"time-step\": 4818}, {\"accuracy\": 1.0, \"loss\": 0.0007740113651379943, \"time-step\": 4819}, {\"accuracy\": 1.0, \"loss\": 0.000765447854064405, \"time-step\": 4820}, {\"accuracy\": 1.0, \"loss\": 0.0007735516992397606, \"time-step\": 4821}, {\"accuracy\": 1.0, \"loss\": 0.0007649724138900638, \"time-step\": 4822}, {\"accuracy\": 1.0, \"loss\": 0.0007730750367045403, \"time-step\": 4823}, {\"accuracy\": 1.0, \"loss\": 0.0007645026198588312, \"time-step\": 4824}, {\"accuracy\": 1.0, \"loss\": 0.0007726006442680955, \"time-step\": 4825}, {\"accuracy\": 1.0, \"loss\": 0.0007640231633558869, \"time-step\": 4826}, {\"accuracy\": 1.0, \"loss\": 0.0007721157744526863, \"time-step\": 4827}, {\"accuracy\": 1.0, \"loss\": 0.0007635597721673548, \"time-step\": 4828}, {\"accuracy\": 1.0, \"loss\": 0.0007716512191109359, \"time-step\": 4829}, {\"accuracy\": 1.0, \"loss\": 0.0007630996988154948, \"time-step\": 4830}, {\"accuracy\": 1.0, \"loss\": 0.0007711839862167835, \"time-step\": 4831}, {\"accuracy\": 1.0, \"loss\": 0.000762623967602849, \"time-step\": 4832}, {\"accuracy\": 1.0, \"loss\": 0.0007707125041633844, \"time-step\": 4833}, {\"accuracy\": 1.0, \"loss\": 0.0007621538243256509, \"time-step\": 4834}, {\"accuracy\": 1.0, \"loss\": 0.0007702328730374575, \"time-step\": 4835}, {\"accuracy\": 1.0, \"loss\": 0.0007616904331371188, \"time-step\": 4836}, {\"accuracy\": 1.0, \"loss\": 0.0007697669207118452, \"time-step\": 4837}, {\"accuracy\": 1.0, \"loss\": 0.0007612091139890254, \"time-step\": 4838}, {\"accuracy\": 1.0, \"loss\": 0.0007692675571888685, \"time-step\": 4839}, {\"accuracy\": 1.0, \"loss\": 0.0007607194129377604, \"time-step\": 4840}, {\"accuracy\": 1.0, \"loss\": 0.0007687801844440401, \"time-step\": 4841}, {\"accuracy\": 1.0, \"loss\": 0.0007602465921081603, \"time-step\": 4842}, {\"accuracy\": 1.0, \"loss\": 0.0007683246512897313, \"time-step\": 4843}, {\"accuracy\": 1.0, \"loss\": 0.0007598118390887976, \"time-step\": 4844}, {\"accuracy\": 1.0, \"loss\": 0.0007678712718188763, \"time-step\": 4845}, {\"accuracy\": 1.0, \"loss\": 0.0007593365153297782, \"time-step\": 4846}, {\"accuracy\": 1.0, \"loss\": 0.0007673825602978468, \"time-step\": 4847}, {\"accuracy\": 1.0, \"loss\": 0.0007588626467622817, \"time-step\": 4848}, {\"accuracy\": 1.0, \"loss\": 0.0007669327314943075, \"time-step\": 4849}, {\"accuracy\": 1.0, \"loss\": 0.0007584048435091972, \"time-step\": 4850}, {\"accuracy\": 1.0, \"loss\": 0.0007664423901587725, \"time-step\": 4851}, {\"accuracy\": 1.0, \"loss\": 0.0007579165976494551, \"time-step\": 4852}, {\"accuracy\": 1.0, \"loss\": 0.0007659581606276333, \"time-step\": 4853}, {\"accuracy\": 1.0, \"loss\": 0.0007574498886242509, \"time-step\": 4854}, {\"accuracy\": 1.0, \"loss\": 0.0007654859800823033, \"time-step\": 4855}, {\"accuracy\": 1.0, \"loss\": 0.0007569909212179482, \"time-step\": 4856}, {\"accuracy\": 1.0, \"loss\": 0.0007650345796719193, \"time-step\": 4857}, {\"accuracy\": 1.0, \"loss\": 0.0007565345149487257, \"time-step\": 4858}, {\"accuracy\": 1.0, \"loss\": 0.0007645651348866522, \"time-step\": 4859}, {\"accuracy\": 1.0, \"loss\": 0.0007560645462945104, \"time-step\": 4860}, {\"accuracy\": 1.0, \"loss\": 0.0007641009869985282, \"time-step\": 4861}, {\"accuracy\": 1.0, \"loss\": 0.0007556099444627762, \"time-step\": 4862}, {\"accuracy\": 1.0, \"loss\": 0.0007636356167495251, \"time-step\": 4863}, {\"accuracy\": 1.0, \"loss\": 0.0007551483577117324, \"time-step\": 4864}, {\"accuracy\": 1.0, \"loss\": 0.0007631750777363777, \"time-step\": 4865}, {\"accuracy\": 1.0, \"loss\": 0.0007547066197730601, \"time-step\": 4866}, {\"accuracy\": 1.0, \"loss\": 0.0007627221057191491, \"time-step\": 4867}, {\"accuracy\": 1.0, \"loss\": 0.0007542424718849361, \"time-step\": 4868}, {\"accuracy\": 1.0, \"loss\": 0.0007622671546414495, \"time-step\": 4869}, {\"accuracy\": 1.0, \"loss\": 0.0007537866476923227, \"time-step\": 4870}, {\"accuracy\": 1.0, \"loss\": 0.0007617967785336077, \"time-step\": 4871}, {\"accuracy\": 1.0, \"loss\": 0.0007533289026468992, \"time-step\": 4872}, {\"accuracy\": 1.0, \"loss\": 0.000761350616812706, \"time-step\": 4873}, {\"accuracy\": 1.0, \"loss\": 0.0007528803544119, \"time-step\": 4874}, {\"accuracy\": 1.0, \"loss\": 0.0007608843734487891, \"time-step\": 4875}, {\"accuracy\": 1.0, \"loss\": 0.0007524261600337923, \"time-step\": 4876}, {\"accuracy\": 1.0, \"loss\": 0.0007604373968206346, \"time-step\": 4877}, {\"accuracy\": 1.0, \"loss\": 0.0007519920472986996, \"time-step\": 4878}, {\"accuracy\": 1.0, \"loss\": 0.0007599805830977857, \"time-step\": 4879}, {\"accuracy\": 1.0, \"loss\": 0.0007515218458138406, \"time-step\": 4880}, {\"accuracy\": 1.0, \"loss\": 0.0007595013012178242, \"time-step\": 4881}, {\"accuracy\": 1.0, \"loss\": 0.000751051411498338, \"time-step\": 4882}, {\"accuracy\": 1.0, \"loss\": 0.00075903395190835, \"time-step\": 4883}, {\"accuracy\": 1.0, \"loss\": 0.0007505850517190993, \"time-step\": 4884}, {\"accuracy\": 1.0, \"loss\": 0.0007585734711028636, \"time-step\": 4885}, {\"accuracy\": 1.0, \"loss\": 0.0007501379586756229, \"time-step\": 4886}, {\"accuracy\": 1.0, \"loss\": 0.000758123816922307, \"time-step\": 4887}, {\"accuracy\": 1.0, \"loss\": 0.0007496791076846421, \"time-step\": 4888}, {\"accuracy\": 1.0, \"loss\": 0.0007576552452519536, \"time-step\": 4889}, {\"accuracy\": 1.0, \"loss\": 0.0007492292206734419, \"time-step\": 4890}, {\"accuracy\": 1.0, \"loss\": 0.0007571985479444265, \"time-step\": 4891}, {\"accuracy\": 1.0, \"loss\": 0.0007487796829082072, \"time-step\": 4892}, {\"accuracy\": 1.0, \"loss\": 0.0007567543652839959, \"time-step\": 4893}, {\"accuracy\": 1.0, \"loss\": 0.0007483430672436953, \"time-step\": 4894}, {\"accuracy\": 1.0, \"loss\": 0.0007563324761576951, \"time-step\": 4895}, {\"accuracy\": 1.0, \"loss\": 0.0007479228661395609, \"time-step\": 4896}, {\"accuracy\": 1.0, \"loss\": 0.0007558915531262755, \"time-step\": 4897}, {\"accuracy\": 1.0, \"loss\": 0.0007474572630599141, \"time-step\": 4898}, {\"accuracy\": 1.0, \"loss\": 0.0007554118055850267, \"time-step\": 4899}, {\"accuracy\": 1.0, \"loss\": 0.0007470048731192946, \"time-step\": 4900}, {\"accuracy\": 1.0, \"loss\": 0.0007549664587713778, \"time-step\": 4901}, {\"accuracy\": 1.0, \"loss\": 0.000746561330743134, \"time-step\": 4902}, {\"accuracy\": 1.0, \"loss\": 0.0007545056287199259, \"time-step\": 4903}, {\"accuracy\": 1.0, \"loss\": 0.0007461037021130323, \"time-step\": 4904}, {\"accuracy\": 1.0, \"loss\": 0.0007540576625615358, \"time-step\": 4905}, {\"accuracy\": 1.0, \"loss\": 0.0007456616731360555, \"time-step\": 4906}, {\"accuracy\": 1.0, \"loss\": 0.0007536046323366463, \"time-step\": 4907}, {\"accuracy\": 1.0, \"loss\": 0.0007452134741470218, \"time-step\": 4908}, {\"accuracy\": 1.0, \"loss\": 0.0007531589944846928, \"time-step\": 4909}, {\"accuracy\": 1.0, \"loss\": 0.0007447696989402175, \"time-step\": 4910}, {\"accuracy\": 1.0, \"loss\": 0.0007527071284130216, \"time-step\": 4911}, {\"accuracy\": 1.0, \"loss\": 0.000744324701372534, \"time-step\": 4912}, {\"accuracy\": 1.0, \"loss\": 0.0007522576488554478, \"time-step\": 4913}, {\"accuracy\": 1.0, \"loss\": 0.0007438714383170009, \"time-step\": 4914}, {\"accuracy\": 1.0, \"loss\": 0.0007518030470237136, \"time-step\": 4915}, {\"accuracy\": 1.0, \"loss\": 0.0007434346480295062, \"time-step\": 4916}, {\"accuracy\": 1.0, \"loss\": 0.000751360144931823, \"time-step\": 4917}, {\"accuracy\": 1.0, \"loss\": 0.0007429852848872542, \"time-step\": 4918}, {\"accuracy\": 1.0, \"loss\": 0.0007509088027291, \"time-step\": 4919}, {\"accuracy\": 1.0, \"loss\": 0.0007425468065775931, \"time-step\": 4920}, {\"accuracy\": 1.0, \"loss\": 0.0007504658424295485, \"time-step\": 4921}, {\"accuracy\": 1.0, \"loss\": 0.0007420990732498467, \"time-step\": 4922}, {\"accuracy\": 1.0, \"loss\": 0.000750011473428458, \"time-step\": 4923}, {\"accuracy\": 1.0, \"loss\": 0.000741650233976543, \"time-step\": 4924}, {\"accuracy\": 1.0, \"loss\": 0.00074956682510674, \"time-step\": 4925}, {\"accuracy\": 1.0, \"loss\": 0.000741202908102423, \"time-step\": 4926}, {\"accuracy\": 1.0, \"loss\": 0.0007491104770451784, \"time-step\": 4927}, {\"accuracy\": 1.0, \"loss\": 0.0007407702505588531, \"time-step\": 4928}, {\"accuracy\": 1.0, \"loss\": 0.0007486816612072289, \"time-step\": 4929}, {\"accuracy\": 1.0, \"loss\": 0.0007403435301966965, \"time-step\": 4930}, {\"accuracy\": 1.0, \"loss\": 0.0007482513901777565, \"time-step\": 4931}, {\"accuracy\": 1.0, \"loss\": 0.0007399073801934719, \"time-step\": 4932}, {\"accuracy\": 1.0, \"loss\": 0.0007478039478883147, \"time-step\": 4933}, {\"accuracy\": 1.0, \"loss\": 0.0007394657586701214, \"time-step\": 4934}, {\"accuracy\": 1.0, \"loss\": 0.0007473570294678211, \"time-step\": 4935}, {\"accuracy\": 1.0, \"loss\": 0.0007390093524008989, \"time-step\": 4936}, {\"accuracy\": 1.0, \"loss\": 0.0007468937546946108, \"time-step\": 4937}, {\"accuracy\": 1.0, \"loss\": 0.0007385764038190246, \"time-step\": 4938}, {\"accuracy\": 1.0, \"loss\": 0.0007464721566066146, \"time-step\": 4939}, {\"accuracy\": 1.0, \"loss\": 0.0007381467148661613, \"time-step\": 4940}, {\"accuracy\": 1.0, \"loss\": 0.0007460307679139078, \"time-step\": 4941}, {\"accuracy\": 1.0, \"loss\": 0.000737725174985826, \"time-step\": 4942}, {\"accuracy\": 1.0, \"loss\": 0.0007456065504811704, \"time-step\": 4943}, {\"accuracy\": 1.0, \"loss\": 0.0007372871623374522, \"time-step\": 4944}, {\"accuracy\": 1.0, \"loss\": 0.0007451591081917286, \"time-step\": 4945}, {\"accuracy\": 1.0, \"loss\": 0.0007368552614934742, \"time-step\": 4946}, {\"accuracy\": 1.0, \"loss\": 0.0007447367534041405, \"time-step\": 4947}, {\"accuracy\": 1.0, \"loss\": 0.0007364326738752425, \"time-step\": 4948}, {\"accuracy\": 1.0, \"loss\": 0.0007442984497174621, \"time-step\": 4949}, {\"accuracy\": 1.0, \"loss\": 0.0007359882583841681, \"time-step\": 4950}, {\"accuracy\": 1.0, \"loss\": 0.0007438493194058537, \"time-step\": 4951}, {\"accuracy\": 1.0, \"loss\": 0.0007355520501732826, \"time-step\": 4952}, {\"accuracy\": 1.0, \"loss\": 0.0007434304570779204, \"time-step\": 4953}, {\"accuracy\": 1.0, \"loss\": 0.0007351257372647524, \"time-step\": 4954}, {\"accuracy\": 1.0, \"loss\": 0.0007429818506352603, \"time-step\": 4955}, {\"accuracy\": 1.0, \"loss\": 0.0007346898200921714, \"time-step\": 4956}, {\"accuracy\": 1.0, \"loss\": 0.0007425556541420519, \"time-step\": 4957}, {\"accuracy\": 1.0, \"loss\": 0.0007342715398408473, \"time-step\": 4958}, {\"accuracy\": 1.0, \"loss\": 0.000742129166610539, \"time-step\": 4959}, {\"accuracy\": 1.0, \"loss\": 0.0007338436553254724, \"time-step\": 4960}, {\"accuracy\": 1.0, \"loss\": 0.0007416838780045509, \"time-step\": 4961}, {\"accuracy\": 1.0, \"loss\": 0.0007334044203162193, \"time-step\": 4962}, {\"accuracy\": 1.0, \"loss\": 0.000741249998100102, \"time-step\": 4963}, {\"accuracy\": 1.0, \"loss\": 0.0007329797372221947, \"time-step\": 4964}, {\"accuracy\": 1.0, \"loss\": 0.0007408193196170032, \"time-step\": 4965}, {\"accuracy\": 1.0, \"loss\": 0.0007325475453399122, \"time-step\": 4966}, {\"accuracy\": 1.0, \"loss\": 0.0007403820636682212, \"time-step\": 4967}, {\"accuracy\": 1.0, \"loss\": 0.0007321140728890896, \"time-step\": 4968}, {\"accuracy\": 1.0, \"loss\": 0.0007399487076327205, \"time-step\": 4969}, {\"accuracy\": 1.0, \"loss\": 0.0007316757109947503, \"time-step\": 4970}, {\"accuracy\": 1.0, \"loss\": 0.0007394984713755548, \"time-step\": 4971}, {\"accuracy\": 1.0, \"loss\": 0.0007312421803362668, \"time-step\": 4972}, {\"accuracy\": 1.0, \"loss\": 0.0007390666869468987, \"time-step\": 4973}, {\"accuracy\": 1.0, \"loss\": 0.0007308197673410177, \"time-step\": 4974}, {\"accuracy\": 1.0, \"loss\": 0.0007386321085505188, \"time-step\": 4975}, {\"accuracy\": 1.0, \"loss\": 0.0007303828606382012, \"time-step\": 4976}, {\"accuracy\": 1.0, \"loss\": 0.0007382150506600738, \"time-step\": 4977}, {\"accuracy\": 1.0, \"loss\": 0.0007299843127839267, \"time-step\": 4978}, {\"accuracy\": 1.0, \"loss\": 0.0007378238369710743, \"time-step\": 4979}, {\"accuracy\": 1.0, \"loss\": 0.0007295871619135141, \"time-step\": 4980}, {\"accuracy\": 1.0, \"loss\": 0.0007373992702923715, \"time-step\": 4981}, {\"accuracy\": 1.0, \"loss\": 0.0007291564252227545, \"time-step\": 4982}, {\"accuracy\": 1.0, \"loss\": 0.0007369618397206068, \"time-step\": 4983}, {\"accuracy\": 1.0, \"loss\": 0.0007287305779755116, \"time-step\": 4984}, {\"accuracy\": 1.0, \"loss\": 0.0007365294732153416, \"time-step\": 4985}, {\"accuracy\": 1.0, \"loss\": 0.0007283018785528839, \"time-step\": 4986}, {\"accuracy\": 1.0, \"loss\": 0.0007361037423834205, \"time-step\": 4987}, {\"accuracy\": 1.0, \"loss\": 0.0007278826087713242, \"time-step\": 4988}, {\"accuracy\": 1.0, \"loss\": 0.0007356888381764293, \"time-step\": 4989}, {\"accuracy\": 1.0, \"loss\": 0.0007274654344655573, \"time-step\": 4990}, {\"accuracy\": 1.0, \"loss\": 0.0007352633401751518, \"time-step\": 4991}, {\"accuracy\": 1.0, \"loss\": 0.0007270402275025845, \"time-step\": 4992}, {\"accuracy\": 1.0, \"loss\": 0.0007348343497142196, \"time-step\": 4993}, {\"accuracy\": 1.0, \"loss\": 0.0007266225875355303, \"time-step\": 4994}, {\"accuracy\": 1.0, \"loss\": 0.0007344129262492061, \"time-step\": 4995}, {\"accuracy\": 1.0, \"loss\": 0.0007262114086188376, \"time-step\": 4996}, {\"accuracy\": 1.0, \"loss\": 0.0007339917938224971, \"time-step\": 4997}, {\"accuracy\": 1.0, \"loss\": 0.0007257911493070424, \"time-step\": 4998}, {\"accuracy\": 1.0, \"loss\": 0.0007335797417908907, \"time-step\": 4999}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['acc']\n",
    "\n",
    "df = pd.DataFrame({\"accuracy\":accuracy, \"loss\":loss, \"time-step\": np.arange(0, len(accuracy))})\n",
    "\n",
    "base = alt.Chart(df).mark_line(color=\"blue\").encode(x=\"time-step\", y=\"accuracy\")\n",
    "loss = alt.Chart(df).mark_line(color=\"red\").encode(x=\"time-step\", y=\"loss\")\n",
    "(base  + loss).properties(title='Chart 2').resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy goes to 100% in around 1,000 epochs (note that different runs may slightly change the results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a strict sense, LSTM is a type of layer instead of a type of network. What I've calling LSTM networks is basically any RNN composed of LSTM layers. Most RNNs you'll find in the wild (i.e., the internet) use either LSTMs or [Gated Recurrent Units (GRU)](https://en.wikipedia.org/wiki/Gated_recurrent_unit). We don't cover GRU here since they are very similar to LSTMs and this chapter is dense enough as it is. If you want to learn more about GRU see [Cho et al (2014)](https://arxiv.org/abs/1406.1078) and [Chapter 9.1 from Zhang (2020)](https://d2l.ai/chapter_recurrent-modern/gru.html).\n",
    "\n",
    "For this section, I'll base the code in the example provided by Chollet (2017) in chapter 6. As a side note, if you are interested in learning Keras in-depth, [Chollet's book](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438) is probably the best source since he is the creator of Keras library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preprocessing data from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are like me,  you like to check the [IMDB](https://www.imdb.com/) reviews before watching a movie. For this example, we will make use of the [IMDB dataset](https://www.imdb.com/interfaces/), and Lucky us, Keras comes pre-packaged with it. The IMDB dataset comprises 50,000 movie reviews, 50% positive and 50% negative. Keras give access to a numerically encoded version of the dataset where each word is mapped to sequences of integers. We can download the dataset by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for this section\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This time I also imported Tensorflow, and from there Keras layers and models. Keras happens to be integrated with Tensorflow, as a high-level interface, so nothing important changes when doing this. Yet, there are some implementation issues with the optimizer that require importing from Tensorflow to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "num_words=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `num_words=5000` restrict the dataset to the top 5,000 most frequent words. We do this to avoid highly infrequent words. Often, infrequent words are either typos or words for which we don't have enough statistical information to learn useful representations. \n",
    "\n",
    "Data is downloaded as a (25000,) tuples of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (25000,), train-labels shape: (25000,)\n",
      "test-data shape: (25000,), test-labels shape: (25000,) \n",
      "\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {train_data.shape}, train-labels shape: {train_labels.shape}')\n",
    "print(f'test-data shape: {test_data.shape}, test-labels shape: {test_labels.shape} \\n')\n",
    "print(train_data[0],train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about the review contents, the code snippet below decodes the first review into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to **\"pad\" each sequence with zeros** such that all sequences are of the same length. We do this because Keras layers expect same-length vectors as input sequences. Given that we are considering only the 5,000 more frequent words, we have max length of any sequence is 5,000. Hence, we have to pad every sequence to have length 5,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_data, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we go from a list of list (samples= 25000,), to a matrix of shape (samples=25000, maxleng=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (25000, 5000), train-labels shape: (25000, 5000) \n",
      "\n",
      "[  0   0   0 ...  19 178  32] [  0   0   0 ...  14   6 717]\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {X_train.shape}, train-labels shape: {X_test.shape} \\n')\n",
    "print(X_train[0],X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will take only the first 5,000 training and testing examples. We do this because training RNNs is computationally expensive, and we don't have access to enough hardware resources to train a large model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=5000\n",
    "training_sentences = X_train[0:training_size]\n",
    "testing_sentences =  X_test[0:training_size]\n",
    "training_labels = train_labels[0:training_size]\n",
    "testing_labels = test_labels[0:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (5000, 5000), train-labels shape: (5000,)\n",
      "test-data shape: (5000, 5000), test-labels shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {training_sentences.shape}, train-labels shape: {training_labels.shape}')\n",
    "print(f'test-data shape: {testing_sentences.shape}, test-labels shape: {testing_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the percentage of positive reviews samples on training and testing as a sanity check. We want this to be close to 50% so the sample is balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of positive reviews in training: 0.5092\n",
      "percentage of positive reviews in testing: 0.4858\n"
     ]
    }
   ],
   "source": [
    "print(f'percentage of positive reviews in training: {training_labels.sum()/training_size}')\n",
    "print(f'percentage of positive reviews in testing: {testing_labels.sum()/training_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use word embeddings instead of one-hot encodings this time. Again, Keras provides convenience functions (or layer) to learn word embeddings along with RNNs training. An embedding in Keras is a layer that takes two inputs as a minimum: the **max length of a sequence** (i.e., the max number of tokens), and the **desired dimensionality of the embedding** (i.e., in how many vectors you want to represent the tokens). For instance, for an embedding with 5,000 tokens and 32 embedding vectors we just define `model.add(Embedding(5,000, 32))`. We will do this when defining the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining RNN with LSTM layers is remarkably simple with Keras (considering how complex LSTMs are as mathematical objects). I'll define a relatively \"shallow\" network with just 1 hidden LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network as a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add embedding layer with:\n",
    "    # - Max number of tokens: 10,000\n",
    "    # - Number of embeddings vectors: 32 \n",
    "model.add(Embedding(maxlen, 32))\n",
    "\n",
    "# Add LSTM layer with 32 units (sequence length)\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add output layer with sigmoid activation unit\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Application: IMDB review prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train and test our RNN. I'll run just five epochs, again, because we don't have enough computational resources and for a demo is more than enough. If you run this, it may take around 5-15 minutes in a CPU.  For instance, my Intel i7-8550U took ~10 min to run five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(training_sentences, training_labels,\n",
    "                    epochs=5,\n",
    "                    batch_size=128, # update gradients every 128 sequences\n",
    "                    validation_split=0.2, # validation subsample\n",
    "                    verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: a \"validation split\" is different from the testing set: It's a sub-sample from the training set. For instance, with a training sample of 5,000, the `validation_split = 0.2` will split the data in a 4,000 effective training set and a 1,000 validation set. The network is trained only in the training set, whereas the validation set is used as a real-time(ish) way to help with hyper-parameter tunning, by synchronously evaluating the network in such a sub-sample. To learn more about this see the [Wikipedia article on the topic](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training accuracy:0.8777499794960022\n",
      "final validation accuracy:0.8149999976158142\n"
     ]
    }
   ],
   "source": [
    "print(f\"final training accuracy:{history.history['acc'][-1]}\")\n",
    "print(f\"final validation accuracy:{history.history['val_acc'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a **training accuracy of ~88%** and **validation accuracy of ~81%** (note that different runs may slightly change the results). The left-pane in **Chart 3** shows the training and validation curves for accuracy, whereas the right-pane shows the same for the loss. It is clear that the network overfitting the data by the 3rd epoch. This is expected as our architecture is shallow, the training set relatively small, and no regularization method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-07863c1c6e494449b9c2b8b78a81926c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-07863c1c6e494449b9c2b8b78a81926c\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#0202d6\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"#7272a1\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"val_accuracy\"}}}]}, {\"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#d60202\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"#cc6e6e\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"val_loss\"}}}]}], \"data\": {\"name\": \"data-9587a6adfd99e4fe616912ad0f7d25cb\"}, \"title\": \"Chart 3\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-9587a6adfd99e4fe616912ad0f7d25cb\": [{\"accuracy\": 0.5809999704360962, \"val_accuracy\": 0.7310000061988831, \"loss\": 0.687211995601654, \"val_loss\": 0.6642515668869019, \"time-step\": 1}, {\"accuracy\": 0.7577499747276306, \"val_accuracy\": 0.777999997138977, \"loss\": 0.6248226439952851, \"val_loss\": 0.590116578578949, \"time-step\": 2}, {\"accuracy\": 0.8230000138282776, \"val_accuracy\": 0.6859999895095825, \"loss\": 0.5040617761611939, \"val_loss\": 0.5789643344879151, \"time-step\": 3}, {\"accuracy\": 0.846750020980835, \"val_accuracy\": 0.8209999799728394, \"loss\": 0.4134952628612518, \"val_loss\": 0.4255082859992981, \"time-step\": 4}, {\"accuracy\": 0.8777499794960022, \"val_accuracy\": 0.8149999976158142, \"loss\": 0.3233668565750122, \"val_loss\": 0.4243640975952148, \"time-step\": 5}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "df = pd.DataFrame({\"accuracy\":accuracy,\n",
    "                   \"val_accuracy\": val_acc,\n",
    "                   \"loss\":loss,\n",
    "                   \"val_loss\": val_loss,\n",
    "                   \"time-step\": np.arange(1, len(accuracy)+1)})\n",
    "\n",
    "accu = alt.Chart(df).mark_line(color=\"#0202d6\").encode(x=\"time-step\", y=\"accuracy\")\n",
    "val_accu = alt.Chart(df).mark_line(color=\"#7272a1\").encode(x=\"time-step\", y=\"val_accuracy\")\n",
    "\n",
    "loss = alt.Chart(df).mark_line(color=\"#d60202\").encode(x=\"time-step\", y=\"loss\")\n",
    "val_loss = alt.Chart(df).mark_line(color=\"#cc6e6e\").encode(x=\"time-step\", y=\"val_loss\")\n",
    "\n",
    "((accu  + val_accu)|(loss + val_loss)).properties(title='Chart 3') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model obtains a **test set accuracy of ~80%** echoing the results from the validation set. All things considered, this is a very respectable result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss score: 0.4296374634742737\n",
      "Test accuracy score:0.8098000288009644\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testing_sentences, testing_labels, verbose=0)\n",
    "print(f'Test loss score: {score[0]}')\n",
    "print(f'Test accuracy score:{ score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs is hard and costly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in previous sections, there are three well-known issues that make training RNNs really hard: (1) vanishing gradients, (2) exploding gradients, (3) and its sequential nature, which make them computationally expensive as parallelization is difficult. I won't discuss again these issues. Many techniques have been developed to address all these issues, from architectures like LSTM,  GRU, and ResNets, to techniques like gradient clipping and regularization (Pascanu et al (2012); for an up to date (i.e., 2020) review of this issues see [Chapter 9 of Zhang et al book](https://d2l.ai/chapter_recurrent-modern/index.html).). \n",
    "\n",
    "The quest for solutions to RNNs deficiencies has prompt the development of new architectures like Encoder-Decoder networks with \"attention\" mechanisms (Bahdanau et al, 2014; Vaswani et al, 2017). This new type of architecture seems to be outperforming RNNs in tasks like machine translation and text generation, in addition to overcoming some RNN deficiencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do RNNs \"really\" understand...anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critics like Gary Marcus have pointed out the apparent inability of neural-networks based models to \"really\" understand their outputs (Marcus, 2018). This is prominent for RNNs since they have been used profusely used in the context of language generation and understanding. For instance, even state-of-the-art models like [OpenAI GPT-2](https://openai.com/blog/better-language-models/) sometimes produce incoherent sentences. Marcus gives the [following example](https://thegradient.pub/gpt2-and-the-nature-of-intelligence/):\n",
    "\n",
    "> (**Marcus**) Suppose for example that I ask the system what happens when I put two trophies a table and another: *I put two trophies on a table, and then add another, the total number is...* \n",
    "\n",
    "> (**GPT-2 answer**) *...is five trophies and I'm like, 'Well, I can live with that, right?*\n",
    "\n",
    "From Marcus' perspective, this lack of coherence is an exemplar of GPT-2 incapacity to understand language. \n",
    "\n",
    "Yet, I'll argue two things. First, this is an unfairly underspecified question: **What do we mean by understanding?** From a cognitive science perspective, this is a fundamental yet strikingly hard question to answer. If you ask five cognitive science what does it \"really\" mean to understand something you are likely to get five different answers. What do we need is a falsifiable way to decide when a system \"really\" understands language. Is lack of coherence enough? I produce incoherent phrases all the time, and I know lots of people that do the same. In any case, it is important to question whether human-level understanding of language (however you want to define it) is necessary to show that a computational model of any cognitive process is a good model or not. We have several great models of many natural phenomena, yet not a single one gets all the aspects of the phenomena perfectly. For instance, Marcus has said that the fact that GPT-2 sometimes produces incoherent sentences is somehow a proof that human \"thoughts\" (i.e., internal representations) can't possibly be represented as vectors (like neural nets do), which I believe is non-sequitur. \n",
    "\n",
    "Second, **Why should we expect that a network trained for a narrow task like language production should understand what language \"really\" is?** The exercise of comparing computational models of \"cognitive processes\" with \"full-blown\" human cognition, makes as much sense as comparing a model of bipedal locomotion with the entire motor control system of an animal. A model of bipedal locomotion is just that: **a model of a sub-system or sub-process within a larger system, not a reproduction of the entire system**. The fact that a model of bipedal locomotion does not capture well the mechanics of \"jumping\", does not undermine it's veracity or utility, in the same manner, that the inability of a model of language production to \"understand\" all aspects of language does not undermine its plausibility as a model of...languague production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks have been prolific models in cognitive science (Munakata et al, 1997; St. John, 1992; Plaut et al., 1996; Christiansen & Chater, 1999; Botvinick & Plaut, 2004; Muñoz-Organero et al., 2019), bringing together intuitions about how cognitive systems work in time-dependent domains, and how neural networks may accommodate such processes.\n",
    "\n",
    "Nevertheless, problems like vanishing gradients, exploding gradients, and computational inefficiency (i.e., lack of parallelization) have difficulted RNN use in many domains. Although new architectures (without recursive structures) have been developed to improve RNN results and overcome its limitations, they remain relevant from a cognitive science perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. ArXiv Preprint ArXiv:1409.0473.\n",
    "- Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.\n",
    "- Botvinick, M., & Plaut, D. C. (2004). Doing without schema hierarchies: A recurrent connectionist approach to normal and impaired routine sequential action. Psychological Review, 111(2), 395.\n",
    "- Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Current Opinion in Neurobiology, 46, 1–6. https://doi.org/10.1016/j.conb.2017.06.003\n",
    "- Chen, G. (2016). A gentle tutorial of recurrent neural network with error backpropagation. arXiv preprint arXiv:1610.02583.\n",
    "- Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1\n",
    "- François, C. (2017). 6. Deep Learning for text and sequences. Deep learning with Python. Manning.\n",
    "- Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n",
    "- Christiansen, M. H., & Chater, N. (1999). Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2), 157–205.\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). 10. Sequence Modeling: Recurrent and Recursive Nets. In Deep Learning. MIT Press. https://www.deeplearningbook.org/contents/mlp.html\n",
    "- Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Psychology Press.\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.\n",
    "- Güçlü, U., & van Gerven, M. A. (2017). Modeling the dynamics of human brain activity with recurrent neural networks. Frontiers in Computational Neuroscience, 11, 7.\n",
    "- Graves, A. (2012). Supervised sequence labelling. In Supervised sequence labelling with recurrent neural networks (pp. 5-13). Springer, Berlin, Heidelberg.\n",
    "- Jarne, C., & Laje, R. (2019). A detailed study of recurrent neural networks used to model tasks in the cerebral cortex. ArXiv Preprint ArXiv:1906.01094.\n",
    "- John, M. F. (1992). The story gestalt: A model of knowledge-intensive processes in text comprehension. Cognitive Science, 16(2), 271–306.\n",
    "- Marcus, G. (2018). Deep learning: A critical appraisal. ArXiv Preprint ArXiv:1801.00631.\n",
    "- Munakata, Y., McClelland, J. L., Johnson, M. H., & Siegler, R. S. (1997). Rethinking infant knowledge: Toward an adaptive process account of successes and failures in object permanence tasks. Psychological Review, 104(4), 686.\n",
    "- Muñoz-Organero, M., Powell, L., Heller, B., Harpin, V., & Parker, J. (2019). Using Recurrent Neural Networks to Compare Movement Patterns in ADHD and Normally Developing Children Based on Acceleration Signals from the Wrist and Ankle. Sensors (Basel, Switzerland), 19(13). https://doi.org/10.3390/s19132935\n",
    "- K. J. Lang, A. H. Waibel, and G. E. Hinton. A Time-delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3(1):23-43, 1990\n",
    "- Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International Conference on Machine Learning, 1310–1318.\n",
    "- Philipp, G., Song, D., & Carbonell, J. G. (2017). The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions. ArXiv Preprint ArXiv:1712.05577.\n",
    "- Plaut, D. C., McClelland, J. L., Seidenberg, M. S., & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103(1), 56.\n",
    "- Raj, B. (2020, Spring). Neural Networks: Hopfield Nets and Auto Associators [Lecture]. http://deeplearning.cs.cmu.edu/document/slides/lec17.hopfield.pdf\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \\Lukasz, & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 5998–6008.\n",
    "- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). 8. Recurrent Neural Networks. In Dive into Deep Learning. https://d2l.ai/chapter_convolutional-neural-networks/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful online resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a list of my favorite online resources to learn more about Recurrent Neural Networks:\n",
    "\n",
    "- Stanford Lectures: Natural Language Processing with Deep Learning, Winter 2020. [Coruse webpage](http://web.stanford.edu/class/cs224n/index.html#schedule).\n",
    "- Bhiksha Raj's Deep Learning Lectures 13, 14, and 15 at CMU. [Course webpage](http://deeplearning.cs.cmu.edu/)\n",
    "- Geoffrey Hinton's Neural Network Lectures 7 and 8. [YouTube Videos](https://www.youtube.com/watch?v=2fRnHVVLf1Y&list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
