{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the principles behind the creation of the recurrent neural network\n",
    "2. Obtain intuition about difficulties training RNNs, namely: vanishing/exploding gradients and long-term dependencies\n",
    "3. Obtain intuition about mechanics of backpropagation through time BPTT\n",
    "4. Develop a Long Short-Term memory implementation in Keras \n",
    "5. Learn about the uses and limitations of RNNs from a cognitive science perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poet Delmore Schwartz once wrote: **\"...time is the fire in which we burn\"**. We can't escape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that **data is \"perceived\" all at once**, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that **past-states have no influence in future-states**. This is, the input pattern at time-step $t-1$ does not influence the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.\n",
    "\n",
    "Multilayer Perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration (for instance [Cui et al, 2016](https://arxiv.org/pdf/1603.06995.pdf)). Nevertheless, introducing time considerations in such architectures is cumbersome, and better architectures have been envisioned. In particular, **Recurrent Neural Networks (RNNs)** are the modern standard to deal with **time-dependent** and/or **sequence-dependent** problems. This type of network is \"recurrent\" in the sense that they can **revisit or reuse past states as inputs to predict the next or future states**. To put it plainly, they have **memory**. Indeed, memory is what allows us to incorporate our past thoughts and behaviors into our future thoughts and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the earliest examples of networks incorporating \"recurrences\" was the so-called **Hopfield Network**, introduced in 1982 by [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield), at the time, a physicist at Caltech. Hopfield networks were important as they helped to reignite the interest in neural networks in the early '80s. In his 1982 paper, Hopfield wanted to address the fundamental question of **emergence** in cognitive systems: Can relatively stable cognitive phenomena, like memories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physical systems like vortex patterns in fluid flow. Brains seemed like another promising candidate.\n",
    "\n",
    "Hopfield networks are known as a type of **energy-based** (instead of error-based) network because their properties derive from a global energy-function (Raj, 2020). In resemblance to the McCulloch-Pitts neuron, Hopfield neurons are binary threshold units but with recurrent instead of feed-forward connections, where each unit is **bi-directionally connected** to each other, as shown in **Figure 1**. This means that each unit *receives* inputs and *sends* inputs to every other connected unit. A consequence of this architecture is that **weights values are symmetric**, such that weights *coming into* a unit are the same as the ones *coming out* of a unit. The value of each unit is determined by a linear function wrapped into a threshold function $T$, as $y_i = T(\\sum w_{ji}y_j + b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 1: Hopfield Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/hopfield-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopfield network's idea is that each configuration of binary-values $C$ in the network is associated with a **global energy value $-E$**. Here is a simplified picture of the training process: imagine you have a network with five neurons with a configuration of $C_1=(0, 1, 0, 1, 0)$. Now, imagine $C_1$ yields a global energy-value $E_1= 2$ (following the energy function formula). Your goal is to *minimize* $E$ by changing one element of the network $c_i$ at a time. By using the weight updating rule $\\Delta w$, you can subsequently get a new configuration like $C_2=(1, 1, 0, 1, 0)$, as new weights will cause a change in the activation values $(0,1)$. If $C_2$ yields a *lower value of $E$*, let's say, $1.5$, you are moving in the right direction. If you keep iterating with new configurations the network will eventually \"settle\" into a **global energy minimum** (conditioned to the initial state of the network).\n",
    "\n",
    "A fascinating aspect of Hopfield networks, besides the introduction of recurrence, is that is closely based in neuroscience research about learning and memory, particularly Hebbian learning (Hebb, 1949). In fact, Hopfield (1982) proposed this model as a way to capture **memory formation and retrieval**. The idea is that the energy-minima of the network could represent the **formation of a memory**, which further gives rise to a property known as **[content-addressable memory (CAM)](https://en.wikipedia.org/wiki/Content-addressable_memory)**. Here is the idea with a computer analogy: when you access information stored in the random access memory of your computer (RAM), you give the \"address\" where the \"memory\" is located to retrieve it. CAM works the other way around: you give information about the **content** you are searching for, and the computer should retrieve the \"memory\". This is great because this works even when you have **partial or corrupted** information about the content, which is a much more **realistic depiction of how human memory works**. It is similar to doing a google search. Just think in how many times you have searched for lyrics with partial information, like \"song with the beeeee bop ba bodda bope!\".\n",
    "\n",
    "It is important to highlight that the sequential adjustment of Hopfield networks is **not driven by error correction**: there isn't a \"target\" as in supervised-based neural networks. Hopfield networks are systems that \"evolve\" until they find a stable low-energy state. If you \"perturb\" such a system, the system will \"re-evolve\" towards its previous stable-state, similar to how those inflatable \"Bop Bags\" toys get back to their initial position no matter how hard you punch them. It is almost like the system \"remembers\" its previous stable-state (isn't?). This ability to \"return\" to a previous stable-state after the perturbation is why they serve as models of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Hopfield networks where innovative and fascinating models, the first successful example of a recurrent network trained with backpropagation was introduced by [Jeffrey Elman](https://en.wikipedia.org/wiki/Jeffrey_Elman), the so-called **Elman Network** (Elman, 1990). Elman was a cognitive scientist at UC San Diego at the time, part of the group of researchers that published the famous PDP book.\n",
    "\n",
    "In 1990, Elman published \"Finding Structure in Time\", a highly influential work for in cognitive science. Elman was concerned with the problem of representing \"time\" or \"sequences\" in neural networks. In his view, you could take either an \"explicit\" approach or an \"implicit\" approach. The **explicit** approach represents time **spacially**. Consider a vector $x = [x_1,x_2 \\cdots, x_n]$, where element $x_1$ represents the first value of a sequence, $x_2$ the second element, and $x_n$ the last element. Hence, the spacial location in $\\bf{x}$ is indicating the temporal location of each element. You can think about elements of $\\bf{x}$ as sequences of words or actions, one after the other, for instance: $x^1=[Sound, of, the, funky, drummer]$ is a sequence of length five. Elman saw **several drawbacks** to this approach. First, although $\\bf{x}$ is a sequence, the network still needs to represent the sequence all at once as an input, this is, a network would need five input neurons to process $x^1$. Second, it imposes a rigid limit on the duration of pattern, in other words, the network needs a fixed number of elements for every input vector $\\bf{x}$: a network with five input units, can't accommodate a sequence of length six. True, you could start with a six input network, but then shorter sequences would be misrepresented since mismatched units would receive zero input. This is a problem for most domains where sequences have a variable duration. Finally, it can't easily distinguish **relative** temporal position from **absolute** temporal position. Consider the sequence $s = [1, 1]$ and a vector input length of four bits. Such a sequence can be presented in at least three variations:\n",
    "\n",
    "$$\n",
    "x_1 = [0, 1, 1, 0]\\\\\n",
    "x_2 = [0, 0, 1, 1]\\\\\n",
    "x_3 = [1, 1, 0, 0]\n",
    "$$\n",
    "\n",
    "Here, $\\bf{x_1}$, $\\bf{x_2}$, and $\\bf{x_3}$ are instances of $\\bf{s}$ but spacially displaced in the input vector. Geometrically, those three vectors are very different from each other (you can compute similarity measures to put a number on that), although representing the same instance. Even though you can train a neural net to learn those three patterns are associated with the same target, their inherent dissimilarity probably will hinder the network's ability to generalize the learned association.\n",
    "\n",
    "The **implicit** approach represents time by **its effect in intermediate computations**. To do this, Elman added a **context unit** to save past computations and incorporate those in future computations. In short, **memory**. Elman based his approach in the work of [Michael I. Jordan](https://people.eecs.berkeley.edu/~jordan/) on serial processing (1986). Jordan's network implements recurrent connections from the network output $\\hat{y}$ to its hidden units $h$, via a \"memory unit\" $\\mu$ (equivalent to Elman's \"context unit\") as depicted in **Figure 2**. In short, the memory unit keeps a running average of **all past outputs**: this is how the past history is implicitly accounted for on each new computation. There is no learning in the memory unit, which means the weights are fixed to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 2: Jordan Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/jordan-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jordan's network diagrams exemplifies the two ways in which recurrent nets are usually represented. On the left, the **compact format** depicts the network structure as a circuit. On the right, the **unfolded representation** incorporates the notion of time-steps calculations. The unfolded representation also illustrates how a recurrent network can be constructed in a pure feed-forward fashion, with as many layers as time-steps in your sequence. One key consideration is that the weights will be identical on each time-step (or layer). Keep this unfolded representation in mind as will become important later.\n",
    "\n",
    "Elman's innovation was twofold: **recurrent connections between hidden units and memory** (context) units, and **trainable parameters from the memory units to the hidden units**. Memory units now have to \"remember\" the past state of hidden units, which means that instead of keeping a running average, they \"clone\" the value at the previous time-step $t-1$. Memory units also have to learn useful representations (weights) for encoding temporal properties of the sequential input.  **Figure 3** summarizes Elman's network in compact and unfolded fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 3: Elman Network </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/elman-net.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there is something curious about Elman's architecture. What it is the point of \"cloning\" $h$ into $c$ at each time-step? You could bypass $c$ altogether by sending the value of $h_t$ straight into $h_{t+1}$, wich yield mathematically identical results. The most likely explanation for this was that Elman's starting point was Jordan's network, which had a separated memory unit. Regardless, keep in mind we don't need $c$ units to design a functionally identical network. \n",
    "\n",
    "Elman performed multiple experiments with this architecture demonstrating it was capable to solve multiple problems with a sequential structure: a temporal version of the XOR problem; learning the structure (i.e., vowels and consonants sequential order) in sequences of letters; discovering the notion of \"word\", and even learning complex lexical classes like word order in short sentences. Let's briefly explore the temporal XOR solution as an exemplar. **Table 1** shows the XOR problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**: Truth Table For XOR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to transform the XOR problem into a sequence. Consider the following vector: \n",
    "\n",
    "$$\n",
    "s= [1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,...]\n",
    "$$\n",
    "\n",
    "In $\\bf{s}$, the first and second elements, $s_1$ and $s_2$, represent $x_1$ and $x_2$ inputs of **Table 1**, whereas the third element, $s_3$, represents the corresponding output $y$. This pattern repeats until the end of the sequence $s$ as shown in **Figure 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 4: Temporal XOR </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/temporal-xor.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elman trained his network with a 3,000 elements sequence for 600 iterations over the entire dataset, on the task of predicting the next item $s_{t+1}$ of the sequence $s$, meaning that he fed inputs to the network **one by one**. He showed that **error pattern** followed a predictable trend: the mean squared error was **lower every 3 outputs**, and higher in between, meaning the network learned to predict the third element in the sequence, as shown in **Chart 1** (the numbers are made up, but the pattern is the same found by Elman (1990))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ea1fc8d820034139a5fea4297f115f30\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-ea1fc8d820034139a5fea4297f115f30\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2fb8460af59365532c40b2d7e4fe648a\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"cycle\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"MSE\"}}, \"title\": \"Chart 1\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-2fb8460af59365532c40b2d7e4fe648a\": [{\"MSE\": 0.35, \"cycle\": 1}, {\"MSE\": 0.15, \"cycle\": 2}, {\"MSE\": 0.3, \"cycle\": 3}, {\"MSE\": 0.27, \"cycle\": 4}, {\"MSE\": 0.14, \"cycle\": 5}, {\"MSE\": 0.4, \"cycle\": 6}, {\"MSE\": 0.35, \"cycle\": 7}, {\"MSE\": 0.12, \"cycle\": 8}, {\"MSE\": 0.36, \"cycle\": 9}, {\"MSE\": 0.31, \"cycle\": 10}, {\"MSE\": 0.15, \"cycle\": 11}, {\"MSE\": 0.32, \"cycle\": 12}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame({\"MSE\": [0.35, 0.15, 0.30, 0.27, 0.14, 0.40, 0.35, 0.12, 0.36, 0.31, 0.15, 0.32],\n",
    "                  \"cycle\": np.arange(1, 13)})\n",
    "alt.Chart(s).mark_line().encode(x=\"cycle\", y=\"MSE\").properties(title='Chart 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An immediate advantage of this approach is the network can take **inputs of any length**, without having to alter the network architecture at all.\n",
    "\n",
    "In the same paper, Elman showed that the **internal (hidden) representations** learned by the network grouped into meaningful categories, this is, **semantically similar words group together** when analyzed with [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering). This was remarkable as demonstrated the utility of RNNs as a model of cognition in sequence-based problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: vanishing and exploding gradients in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, training recurrent neural networks is hard. Considerably harder than multilayer-perceptrons. When faced with the task of training **very deep networks**, like RNNs, the gradients have the impolite tendency of either (1) **vanishing**, or (2) **exploding** (Bengio et al, 1994; Pascanu et al, 2012). Recall that RNNs can be unfolded so that recurrent connections follow pure feed-forward computations. This unrolled RNN will have as many layers as elements in the sequence. Thus, a sequence of 50 words will be unrolled as an RNN of 50 layers (taking \"word\" as a unit). \n",
    "\n",
    "Concretely, the **vanishing gradient problem** will make close to impossible to learn **long-term dependencies** in sequences. Let's say you have a collection of poems, where the last sentence refers to the first one. Such a dependency will be hard to learn for a deep RNN where gradients vanish as we move backward in the network. The **exploding gradient problem** will completely derail the learning process. In very deep networks this is often a problem because more layers amplify the effect of large gradients, compounding into very large updates to the network weights, to the point values completely blow up.        \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient vanishing**: when gradients *begin small*, as you move backward through the network computing gradients, they will get even smaller as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the output layer will obtain larger updates than weights closer to the input layer. This means that the weights closer to the input layer will hardly change at all, whereas the weights closer to the output layer will change a lot. This is a serious problem when **earlier layers matter for prediction**: they will keep propagating more or less the same signal forward because no learning (i.e., weight updates) will happen, which may significantly hinder the network performance.  \n",
    "\n",
    "Here is the intuition for the **mechanics of gradient explosion**: when gradients *begin large*, as you move backward through the network computing gradients, they will get even larger as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the input layer will obtain larger updates than weights closer to the output layer. Learning can go wrong really fast. Recall that the signal propagated by each layer is the outcome of taking the product between the previous hidden-state and the current hidden-state. If the weights in earlier layers get really large, they will forward-propagate larger and larger signals on each iteration, and the predicted output values will spiral-up out of control, making the error $y-\\hat{y}$ so large that the network will be unable to learn at all. In fact, your computer will \"overflow\" quickly as it would unable to represent numbers that big. Very dramatic.  \n",
    "\n",
    "The mathematics of gradient vanishing and explosion gets complicated quickly. If you want to delve into the mathematics see [Bengio et all (1994)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf), [Pascanu et all (2012)](https://arxiv.org/abs/1211.5063), and [Philipp et all (2017)](https://arxiv.org/abs/1712.05577). \n",
    "\n",
    "For our purposes, I'll give you a simplified numerical example for intuition. Consider the task of predicting a vector $y = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, from inputs $x = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, with a multilayer-perceptron with 5 hidden layers and tanh activation functions. We have two cases:\n",
    "\n",
    "- the weight matrix $W_l$ is initialized to large values $w_{ij} = 2$\n",
    "- the weight matrix $W_s$ is initialized to small values $w_{ij} = 0.02$\n",
    "\n",
    "Now, let's compute a single forward-propagation pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for large initial weights: \n",
      " [[3.99730269]\n",
      " [3.99730269]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_l = np.array([[2, 2],\n",
    "                [2, 2]])\n",
    "\n",
    "h1 = np.tanh(W_l @ x)\n",
    "h2 = np.tanh(W_l @ h1)\n",
    "h3 = np.tanh(W_l @ h2)\n",
    "h4 = np.tanh(W_l @ h3)\n",
    "h5 = np.tanh(W_l @ h4)\n",
    "y_hat = (W_l @ h5)\n",
    "print(f'output for large initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output for small initial weights: \n",
      " [[4.09381337e-09]\n",
      " [4.09381337e-09]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "W_s = np.array([[0.02, 0.02],\n",
    "                [0.02, 0.02]])\n",
    "\n",
    "h1 = np.tanh(W_s @ x)\n",
    "h2 = np.tanh(W_s @ h1)\n",
    "h3 = np.tanh(W_s @ h2)\n",
    "h4 = np.tanh(W_s @ h3)\n",
    "h5 = np.tanh(W_s @ h4)\n",
    "y_hat = (W_s @ h5)\n",
    "print(f'output for small initial weights: \\n {y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for $W_l$ the output $\\hat{y}\\approx4$, whereas for $W_s$ the output $\\hat{y} \\approx 0$. Why does this matter? We haven't done the gradient computation but you can probably anticipate what it's going to happen: for the $W_l$ case, the gradient update is going to be very large, and for the $W_s$ very small. If you keep cycling through forward and backward passes these problems will become worse, leading to gradient explosion and vanishing respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several challenges difficulted progress in RNN in the early '90s (Hochreiter & Schmidhuber, 1997; Pascanu et al, 2012). In addition to vanishing and exploding gradients, we have the fact that the **forward computation is slow**, as RNNs can't compute in parallel: to preserve the time-dependencies through the layers, each layer has to be computed sequentially, which naturally takes more time. Elman networks proved to be effective at solving relatively simple problems, but as the sequences scaled in size and complexity, this type of network struggle. \n",
    "\n",
    "Several approaches were proposed in the '90s to address the aforementioned issues like time-delay neural networks (Lang et al, 1990), simulated annealing (Bengio et al., 1994), and others. The architecture that really moved the field forward was the so-called **Long Short-Term Memory (LSTM) Network**, introduced by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [Jurgen Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber) in 1997. As the name suggests, the defining characteristic of LSTMs is the addition of units combining both short-memory and long-memory capabilities. \n",
    "\n",
    "In LSTMs, instead of having a simple memory unit \"cloning\" values from the hidden unit as in Elman networks, we have a (1) **cell unit** (a.k.a., memory unit) which effectively acts as long-term memory storage, and (2) a **hidden-state** which acts as a memory controller. These two elements are integrated as a circuit of logic gates controlling the flow of information at each time-step. Understanding the notation is crucial here, which is depicted in **Figure 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 5: LSTM architecture </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-unit.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTMs $x_t$, $h_t$, and $c_t$ represent vectors of values. Lightish-pink circles represent element-wise operations, and darkish-pink boxes are fully-connected layers with trainable weights. The top part of the diagram acts as a **memory storage**, whereas the bottom part has a double role: (1) passing the hidden-state information from the previous time-step $t-1$ to the next time step $t$, and (2) to regulate the **influx** of information from $x_t$ and $h_{t-1}$ **into** the memory storage, and the **outflux** of information **from** the memory storage into the next hidden state $h-t$. The second role is the core idea behind LSTM. You can think about it as making **three decisions** at each time-step:\n",
    "\n",
    "1. **Is the *old information* $c_{t-1}$ worth to keep in my memory storage $c_t$?** If so, let the information pass, otherwise, \"forget\" such information. This is controlled by the *forget gate*.    \n",
    "2. **Is this *new information* (inputs) worth to be saved into my memory storage $c_t$?** If so, let information flow into $c_t$. This is controlled by the *input gate* and the *candidate memory cell*. \n",
    "3. **What elements of the information saved in my memory storage $c_t$ are relevant for the computation of the next hidden-state $h_t$?** Select them from $c_t$, combine them new hidden-state output, and let them pass into the next hidden-state $h_t$. This is controlled by the *output gate* and the *tanh* function. \n",
    "\n",
    "Decisions 1 and 2 will determine the information that keeps flowing through the memory storage at the top. Decision 3 will determine the information that flows to the next hidden-state at the bottom. The conjunction of these decisions sometimes is called \"memory block\". Now, keep in mind that this *sequence of decision* is just a convenient *interpretation* of LSTM mechanics. In practice, the weights are the ones determining what each function ends up doing, which may or may not fit well with human intuitions or design objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 6: LSTM as a sequence of decisions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/lstm-choices.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put LSTMs in context, imagine the following simplified scenerio: we are trying to **predict the next word in a sequence**. Let's say, squences are about sports. From past sequences, we saved in the memory block the type of sport: \"soccer\". For the current sequence, we receive a phrase like \"A basketball player...\". In such a case, we first want to \"forget\" the previous type of sport \"soccer\" (*decision 1*) by multplying $c_{t-1} \\odot f_t$. Next, we want to \"update\" memory with the new type of sport, \"basketball\" (*decision 2*), by adding $c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})$. Finally, we want to output (*decision 3*) a verb relevant for \"A basketball player...\", like \"shoot\" or \"dunk\" by $\\hat{y_t} = softmax(W_{hz}h_t + b_z)$.\n",
    "\n",
    "LSTMs long-term memory capabilities make them good at capturing long-term dependencies. The memory cell effectively counteracts the vanishing gradient problem at preserving information as long the forget gate does not \"erase\" past information (Graves, 2012). All the above make LSTMs very useful for a wide variety of applications (see [here for a list](https://en.wikipedia.org/wiki/Long_short-term_memory#Applications)). For instance, when you use [Google's Voice Transcription](https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html) services an RNN is doing the hard work of recognizing your voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs and cognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Convolutional Neural Networks, researchers utilizing RNN for approaching sequential problems like natural language processing (NLP) or time-series prediction, do not *necessarily* care about (although some might) how good of a model of cognition and brain-activity are RNNs. What they really care is about solving problems like translation, speech recognition, and stock market prediction, and many advances in the field come from pursuing such goals. Still, RNN has many **desirable traits as a model of neuro-cognitive activity**, and have been prolifically used to **model several aspects of human cognition and behavior**: child behavior in an object permanence tasks (Munakata et al, 1997); knowledge-intensive text-comprehension (St. John, 1992); processing in quasi-regular domains, like English word reading (Plaut et al., 1996); human performance in processing recursive language structures (Christiansen & Chater, 1999); human sequential action (Botvinick & Plaut, 2004); movement patterns in typical and atypical developing children (Muñoz-Organero et al., 2019). And many others. Neuroscientists have used RNNs to model a wide variety of aspects as well (for reviews see Barak, 2017, Güçlü & van Gerven, 2017, Jarne & Laje, 2019). Overall, RNN has demonstrated to be a productive tool for modeling cognitive and brain function, in distributed representations paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two mathematically complex issues with RNNs: (1) computing hidden-states, and (2) backpropagation. The rest are common operations found in multilayer-perceptrons. LSTMs and its many variants are the facto standards when modeling any kind of sequential problem. Elman networks can be seen as a simplified version of an LSTM, so I'll focus my attention on LSTMs for the most part. My exposition is based on a combination of sources that you may want to review for extended explanations (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997; Graves, 2012; Chen, 2016; Zhang et al., 2020).\n",
    "\n",
    "The LSTM architecture can be desribed by: \n",
    "\n",
    "**Forward pass**:\n",
    "- non-linear forget function\n",
    "- non-linear input function \n",
    "- non-linear candidate-memory function\n",
    "- non-linear output function\n",
    "- memory cell function\n",
    "- non-linear hidden-state function \n",
    "- softmax function (output)\n",
    "\n",
    "**Backward pass**:\n",
    "- Cost-function\n",
    "- Learning procedure (backpropagation)\n",
    "\n",
    "Following the indices for each function requires some definitions. I'll assume we have $h$ hidden units, training sequences of size $n$, and $d$ input units. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{input-units} &= x_i \\in \\mathbb{R}^d \\\\ \n",
    "\\text{training-sequence} &= s_i \\in \\mathbb{R}^n \\\\\n",
    "\\text{output-class} &= y_i \\in \\mathbb{R}^k \\\\\n",
    "\\text{Input-layer} &= X_t \\in \\mathbb{R}^{n\\times d} \\\\\n",
    "\\text{hidden-layer} &= H_t \\in \\mathbb{R}^{n\\times h}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forget function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. We didn't mentioned the bias before, but it is the same bias that all neural networks incorporate, one for each unit in $f$. More formally:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)\n",
    "$$ \n",
    "\n",
    "Each matrix $W$ has dimensionality equal to (number of incoming units, number for connected units). For example, $W_{xf}$ refers to $W_{input-units, forget-units}$. Keep this in mind to read the indices of the $W$ matrices for subsequent definitions.\n",
    "\n",
    "Here is an important insight: What would it happen if $f_t = 0$? If you look at the diagram in **Figure 6**, $f_t$ performs an elementwise multiplication of each element in $c_{t-1}$, meaning that every value would be reduced to $0$. In short, the network would completely \"forget\" past states. Naturally, if $f_t = 1$, the network would keep its memory intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input function and Candidate memory function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. It's defined as:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\n",
    "$$ \n",
    "\n",
    "The **candidate memory function** is an hyperbolic tanget function combining the same elements that $i_t$. It's defined as:\n",
    "\n",
    "$$\n",
    "\\tilde{c}_t = tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\n",
    "$$ \n",
    "\n",
    "Both functions are combined to update the memory cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **output function** is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. Is defined as:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory cell function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **memory cell function** (what I've been calling \"memory storage\" for conceptual clarity), combines the effect of the forget function, input function, and candidate memory function. It's defined as:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot f_t) + (i_t \\odot \\tilde{c_t})\n",
    "$$\n",
    "\n",
    "Where $\\odot$ implies an elementwise multiplication (instead of the usual dot product). This expands to:\n",
    "\n",
    "$$\n",
    "c_t = (c_{t-1} \\odot \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)) + (\\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \\odot tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden-state function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next **hidden-state function** combines the effect of the output function and the contents of the memory cell scaled by a tanh function. It is defined as:\n",
    "\n",
    "$$\n",
    "h_t = O_t \\odot tanh(c_t) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output function will depend upon the problem to be approached. For our our purposes, we will assume a multi-class problem, for which the **softmax function** is appropiated. For this, we first pass the hidden-state by a linear function, and then the softmax as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= (W_{hz}h_t + b_z)\\\\\n",
    "\\hat{y}_t &= softmax(z_t) = \\frac{e^{z_t}}{\\sum_{j=1}^K e^{z_j}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The softmax computes the exponent for each $z_t$ and then normalized by dividing by the sum of every output value exponentiated. In this manner, the output of the softmax can be interpreted as the likelihood value $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the output function, the cost function will depend upon the problem. For regression problems, the Mean-Squared Error can be used. For our purposes (classification), the cross-entropy function is appropriated. It's defined as:\n",
    "\n",
    "$$\n",
    "E_i = - \\sum_t y_ilog(p_i)\n",
    "$$\n",
    "\n",
    "Where $y_i$ is the true label for the $ith$ output unit, and $log(p_i)$ is the log of the softmax value for the $ith$ output unit. The summation indicates we need to aggregate the cost at each time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning procedure: Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, Hochreiter and Schmidhuber (1997) trained LSTMs with a combination of approximate gradient descent computed with a combination of real-time recurrent learning and backpropagation through time (BPTT). Nevertheless, LSTM can be trained with pure backpropagation. Following Graves (2012), I'll only describe BTT because is more accurate,  easier to debug and to describe.\n",
    "\n",
    "**Note**: we call it **backpropagation through time** because of the sequential time-dependent structure of RNNs. Recall that each layer represents a time-step, and forward propagation happens in sequence, one layer computed after the other. Hence, when we backpropagate, we do the same but backward (i.e., \"through time\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 7: Three-layer simplified RNN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/simple-rnn.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reviewed backpropagation for a simple multilayer perceptron [here](https://com-cog-book.github.io/com-cog-book/features/multilayer-perceptron.html#Backpropagation-algorithm). Nevertheless, I'll sketch BPTT for the simplest case as shown in **Figure 7**, this is, with a generic non-linear hidden-layer similar to Elman network without \"context units\" (some like to call it \"vanilla\" RNN, which I avoid because I believe is derogatory against vanilla!). This exercise will allow us to review backpropagation and to understand how it differs from BPTT. We begin by defining a simplified RNN as: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_t &= W_{hz}h_t + b_z\\\\\n",
    "h_t &= \\sigma(W_{hh}h_{t-1} + W_{xh}x_t+b_h)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $h_t$ and $z_t$ indicates a hidden-state (or layer) and  the output respectively. Therefore, **we have to compute gradients w.r.t. five sets of weights**: $\\{W_{hz}, W_{hh}, W_{xh}, b_z, b_h\\}$.\n",
    "\n",
    "First, consider the error derivatives w.r.t. $W_{hz}$ at time $t$, the weight matrix for the linear function at the output layer. Recall that $W_{hz}$ is shared across all time-steps, hence, we can compute the gradients at each time step and then take the sum as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W_{hz}}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{W_{hz}}}\n",
    "$$\n",
    "\n",
    "Same for the bias term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{b_z}} = \\sum_t\\frac{\\partial{E}}{\\partial{z_t}} \\frac{\\partial{z_t}}{\\partial{b_z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That part is straightforward. The issue arises when we try to compute the gradients w.r.t. the wights $W_{hh}$ in the hidden layer. Consider a three layer RNN (i.e., unfolded over three time-steps). In such a case, we have: \n",
    "\n",
    "- $E_3$ depens on $z_3$\n",
    "- $z_3$ depends on $h_3$\n",
    "- $h_3$ depens on $h_2$\n",
    "- $h_2$ depens on $h_1$\n",
    "- $h_1$ depens on $h_0$, where $h_0$ is a random starting state.\n",
    "\n",
    "Now, we have that $E_3$ w.r.t to $h_3$ becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "The issue here is that $h_3$ depends on $h_2$, since according to our definition, the $W_{hh}$ is multiplied by $h_{t-1}$, meaning **we can't compute $\\frac{\\partial{h_3}}{\\partial{W_{hh}}}$ directly**. Othewise, we would be treating $h_2$ as a constant, which is incorrect: is a function. What we need to do is to **compute the gradients separately**: the direct contribution of ${W_{hh}}$ on $E$ and the indirect contribution via $h_2$. Following the rules of calculus in multiple variables, we compute them independently and add them up together as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Again, we have that we can't compute $\\frac{\\partial{h_2}}{\\partial{W_{hh}}}$ directly. Following the same procedure, we have that our full expression becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{hh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{hh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{hh}}}\n",
    "$$\n",
    "\n",
    "Essentially, this means that we compute and add the contribution of $W_{hh}$ to $E$ at each time-step. The expression for $b_h$ is the same:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{b_h}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{b_h}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{b_h}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compute the gradients w.r.t. $W_{xh}$. Here, again, we have to add the contributions of $W_{xh}$ via $h_3$, $h_2$, and $h_1$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E_3}}{\\partial{W_{xh}}} = \n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{W_{xh}}}+\n",
    "\\frac{\\partial{E_3}}{\\partial{z_3}}\n",
    "\\frac{\\partial{z_3}}{\\partial{h_3}}\n",
    "\\frac{\\partial{h_3}}{\\partial{h_2}}\n",
    "\\frac{\\partial{h_2}}{\\partial{h_1}}\n",
    "\\frac{\\partial{h_1}}{\\partial{W_{xh}}}\n",
    "$$\n",
    "\n",
    "That's for BPTT for a simple RNN. The math reviewed here generalizes with minimal changes to more complex architectures as LSTMs. Actually, the only difference regarding LSTMs, is that we have more weights to differentiate for. Instead of a single generic $W_{hh}$, we have $W$ for all the gates: forget, input, output, and candidate cell. The rest remains the same. For a detailed derivation of BPTT for the LSTM see [Graves (2012)](https://www.cs.toronto.edu/~graves/preprint.pdf) and [Chen (2016)](https://arxiv.org/abs/1610.02583)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Sequence-data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with sequence-data, like text or time-series, requires to pre-process it in a manner that is \"digestible\" for RNNs. As with any neural network, RNN can't take raw text as an input, we need to **parse** text sequences and then **\"map\"** them into vectors of numbers. Here I'll briefly review these issues to provide enough context for our example applications. For an extended revision please refer to [Jurafsky and Martin (2019)](https://web.stanford.edu/~jurafsky/slp3/), [Goldberg (2015)](http://u.cs.biu.ac.il/~yogo/nnlp.pdf), [Chollet (2017)](https://www.manning.com/books/deep-learning-with-python), and [Zhang et al (2020)](https://d2l.ai/chapter_recurrent-neural-networks/index.html).\n",
    "\n",
    "Parsing can be done in multiple manners, the most common being: \n",
    "\n",
    "- Using **word** as a unit, which each word represented as a vector\n",
    "- Using **character** as a unit, with each character represented as a vector\n",
    "- Using **n-grams** of words or characters as a unit, with each n-gram represented as a vector. N-grams are sets of words or characters of size \"N\" or less.\n",
    "\n",
    "The process of parsing text into smaller units is called \"tokenization\", and each resulting unit is called a \"token\", the top pane in **Figure 8** displays a sketch of the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Figure 8: Tokenization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rec-net/text-pro.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a corpus of text has been parsed into tokens, we have to map such tokens into numerical vectors. Two common ways to do this are **one-hot encoding** approach and the **word embeddings** approach, as depicted in the bottom pane of **Figure 8**. We used one-hot encodings to transform the MNIST class-labels into vectors of numbers for classification in the [CovNets chapter](https://com-cog-book.github.io/com-cog-book/features/cov-net.html). In a one-hot encoding vector, each token is mapped into a *unique* vector of zeros and ones. The vector size is determined by the vocabullary size. For instance, for the set $x= \\{\"cat\", \"dog\", \"ferret\"\\}$, we could use a 3-dimensional one-hot encoding as:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "One-hot encodings have the advantages of being straightforward to implement and to provide a unique identifier for each token. Its main disadvantage is that tends to create really **sparse** and **high-dimensional** representations for a large corpus of texts. For instance, if you tried a one-hot encoding for 50,000 tokens, you'd end up with a 50,000x50,000-dimensional matrix, which may be unpractical for most tasks. \n",
    "\n",
    "**Word embeddings** represent text by mapping tokens into vectors of real-valued numbers instead of only zeros and ones. This significantly increments the representational capacity of vectors, reducing the required dimensionality for a given corpus of text compared to one-hot encodings. For instance, 50,000 tokens could be represented by as little as 2 or 3 vectors (although the representation may not be very good). Taking the same set $x$ as before, we could have a 2-dimensional word embedding like:\n",
    "\n",
    "$$\n",
    "\\text{cat}=\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.8\n",
    "\\end{bmatrix},\n",
    "\\text{dog}=\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "1 \n",
    "\\end{bmatrix},\n",
    "\\text{ferret}=\n",
    "\\begin{bmatrix}\n",
    "0.6 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You may be wondering why to bother with one-hot encodings when word embeddings are much more space-efficient. The main issue with word-embedding is that **there isn't an obvious way to map tokens into vectors** as with one-hot encodings. For instance, you could assign tokens to vectors at random (assuming every token is assigned to a *unique* vector). The problem with such approach is that the semantic structure in the corpus is broken. Ideally, you want words of similar meaning mapped into similar vectors. We can preserve the semantic structure of a text corpus in the same manner as everything else in machine learning: by **learning from data**. There are two ways to do this:\n",
    "\n",
    "- Learning the word embeddings **at the same time** you train the RNN.\n",
    "- Utilizing **pretrained** word embeddings, this is, embeddings learned in a different task. This is a form of \"transfer learning\".\n",
    "\n",
    "Learning word embeddings for your task is advisable as semantic relationships among words tend to be **context dependent**. For instance, \"exploitation\" in the context of mining is related to resource \"extraction\", hence relative neutral. But, \"exploitation\" in the context of labor rights is related to the idea of \"abuse\", hence a negative connotation. This is more critical when we are dealing with different languages. Nevertheless, learning embeddings for every task sometimes is impractical, either because your corpus is too \"small\" (i.e., not enough data to extract semantic relationships), or too \"large\" (i.e., you don't have enough time and/or resources to learn the embeddings). Examples of freely accessible pretrained word embeddings are Google's [Word2vec](https://code.google.com/archive/p/word2vec/) and the [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) (GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous chapters, I'll use Keras to implement both (a modified version of) the Elman Network for the XOR problem and an LSTM for review prediction based on text-sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, it may be clear to you that Elman networks are a simple RNN with two neurons, one for each input pattern, in the hidden-state. Originally, Elman trained his architecture with a truncated version of BPTT, meaning that only considered two time-steps for computing the gradients, $t$ and $t-1$. We will implement a modified version of Elman's architecture bypassing the \"context\" unit (which does not alter the result at all) and utilizing BPTT instead of its truncated version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, we don't need to generate the 3,000 bits sequence that Elman used in his original work. We can simply generate a single pair of training and testing sets for the XOR problem as in **Table 1**, and pass the training sequence (length two) as the inputs, and the expected outputs as the target. This is very much alike any classification task. An important caveat is that simpleRNN layers in Keras expect an input tensor of shape (number-samples, timesteps, number-input-features). In our case, this has to be: *number-samples*= 4,  *timesteps*=1, *number-input-features*=2. No separate encoding is necessary here because we are manually setting the input and output values to binary vector representations. Finally, we won't worry about training and testing sets for this example, which is way to simple for that (we will do that for the next example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries for this section\n",
    "\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (4, 1, 2)\n",
      "targets data shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "X = np.array([[[0, 0, 1, 1]],\n",
    "              [[0, 1, 0, 1]]]).T\n",
    "\n",
    "# expected values\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "print(f'training data shape: {X.shape}')\n",
    "print(f'targets data shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman network architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a (modified) in Keras is extremely simple as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network as a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add a recurrent layer with 2 units\n",
    "model.add(SimpleRNN(2, input_shape=(1, 2)))\n",
    "\n",
    "# Add the output layer with a sigmoid activation\n",
    "model.add(Dense(1, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary shows that our architecture yields 13 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elman network Application: XOR classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compile and fit our model. I'll utilize [Adadelta](https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L376) (to avoid manually adjusting the learning rate) as the optimizer, and the Mean-Squared Error (as in Elman original work). I'll train the model for 15,000 epochs over the 4 samples dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adadelta', \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['acc'])\n",
    "history = model.fit(X, y,\n",
    "                    epochs=5000,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chart 2** shows the error curve (red, right axis), and the accuracy curve (blue, left axis) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5079619df04242d1bab8ae937d1a8de8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-5079619df04242d1bab8ae937d1a8de8\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}}], \"data\": {\"name\": \"data-aa747393e2ef40c58f3f5ef40964aa40\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": \"Chart 2\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-aa747393e2ef40c58f3f5ef40964aa40\": [{\"accuracy\": 0.5, \"loss\": 0.5388762354850769, \"time-step\": 0}, {\"accuracy\": 0.5, \"loss\": 0.5356122851371765, \"time-step\": 1}, {\"accuracy\": 0.5, \"loss\": 0.532331109046936, \"time-step\": 2}, {\"accuracy\": 0.5, \"loss\": 0.529049277305603, \"time-step\": 3}, {\"accuracy\": 0.5, \"loss\": 0.5257749557495117, \"time-step\": 4}, {\"accuracy\": 0.5, \"loss\": 0.5225127339363098, \"time-step\": 5}, {\"accuracy\": 0.5, \"loss\": 0.5192657709121704, \"time-step\": 6}, {\"accuracy\": 0.5, \"loss\": 0.5160362124443054, \"time-step\": 7}, {\"accuracy\": 0.5, \"loss\": 0.5128251314163208, \"time-step\": 8}, {\"accuracy\": 0.5, \"loss\": 0.5096338391304016, \"time-step\": 9}, {\"accuracy\": 0.5, \"loss\": 0.5064630508422852, \"time-step\": 10}, {\"accuracy\": 0.5, \"loss\": 0.5033132433891296, \"time-step\": 11}, {\"accuracy\": 0.5, \"loss\": 0.5001847743988037, \"time-step\": 12}, {\"accuracy\": 0.5, \"loss\": 0.4970781207084656, \"time-step\": 13}, {\"accuracy\": 0.5, \"loss\": 0.49399334192276, \"time-step\": 14}, {\"accuracy\": 0.5, \"loss\": 0.4909307360649109, \"time-step\": 15}, {\"accuracy\": 0.5, \"loss\": 0.4878902733325958, \"time-step\": 16}, {\"accuracy\": 0.5, \"loss\": 0.48487210273742676, \"time-step\": 17}, {\"accuracy\": 0.5, \"loss\": 0.48187631368637085, \"time-step\": 18}, {\"accuracy\": 0.5, \"loss\": 0.4789029359817505, \"time-step\": 19}, {\"accuracy\": 0.5, \"loss\": 0.4759518504142761, \"time-step\": 20}, {\"accuracy\": 0.5, \"loss\": 0.47302329540252686, \"time-step\": 21}, {\"accuracy\": 0.5, \"loss\": 0.47011712193489075, \"time-step\": 22}, {\"accuracy\": 0.5, \"loss\": 0.4672333598136902, \"time-step\": 23}, {\"accuracy\": 0.5, \"loss\": 0.46437206864356995, \"time-step\": 24}, {\"accuracy\": 0.5, \"loss\": 0.4615331292152405, \"time-step\": 25}, {\"accuracy\": 0.5, \"loss\": 0.45871657133102417, \"time-step\": 26}, {\"accuracy\": 0.5, \"loss\": 0.4559224545955658, \"time-step\": 27}, {\"accuracy\": 0.5, \"loss\": 0.45315083861351013, \"time-step\": 28}, {\"accuracy\": 0.5, \"loss\": 0.4504014551639557, \"time-step\": 29}, {\"accuracy\": 0.5, \"loss\": 0.44767457246780396, \"time-step\": 30}, {\"accuracy\": 0.5, \"loss\": 0.4449700117111206, \"time-step\": 31}, {\"accuracy\": 0.5, \"loss\": 0.44228801131248474, \"time-step\": 32}, {\"accuracy\": 0.5, \"loss\": 0.4396282434463501, \"time-step\": 33}, {\"accuracy\": 0.5, \"loss\": 0.43699103593826294, \"time-step\": 34}, {\"accuracy\": 0.5, \"loss\": 0.4343761205673218, \"time-step\": 35}, {\"accuracy\": 0.5, \"loss\": 0.43178367614746094, \"time-step\": 36}, {\"accuracy\": 0.5, \"loss\": 0.42921367287635803, \"time-step\": 37}, {\"accuracy\": 0.5, \"loss\": 0.42666614055633545, \"time-step\": 38}, {\"accuracy\": 0.5, \"loss\": 0.424140989780426, \"time-step\": 39}, {\"accuracy\": 0.5, \"loss\": 0.4216383099555969, \"time-step\": 40}, {\"accuracy\": 0.5, \"loss\": 0.4191581606864929, \"time-step\": 41}, {\"accuracy\": 0.5, \"loss\": 0.41670045256614685, \"time-step\": 42}, {\"accuracy\": 0.5, \"loss\": 0.4142651855945587, \"time-step\": 43}, {\"accuracy\": 0.5, \"loss\": 0.4118524491786957, \"time-step\": 44}, {\"accuracy\": 0.5, \"loss\": 0.40946218371391296, \"time-step\": 45}, {\"accuracy\": 0.5, \"loss\": 0.40709438920021057, \"time-step\": 46}, {\"accuracy\": 0.5, \"loss\": 0.4047490954399109, \"time-step\": 47}, {\"accuracy\": 0.5, \"loss\": 0.40242624282836914, \"time-step\": 48}, {\"accuracy\": 0.5, \"loss\": 0.4001258313655853, \"time-step\": 49}, {\"accuracy\": 0.5, \"loss\": 0.39784786105155945, \"time-step\": 50}, {\"accuracy\": 0.5, \"loss\": 0.3955923318862915, \"time-step\": 51}, {\"accuracy\": 0.5, \"loss\": 0.3933592438697815, \"time-step\": 52}, {\"accuracy\": 0.5, \"loss\": 0.39114850759506226, \"time-step\": 53}, {\"accuracy\": 0.5, \"loss\": 0.388960063457489, \"time-step\": 54}, {\"accuracy\": 0.5, \"loss\": 0.38679400086402893, \"time-step\": 55}, {\"accuracy\": 0.5, \"loss\": 0.3846501111984253, \"time-step\": 56}, {\"accuracy\": 0.5, \"loss\": 0.38252848386764526, \"time-step\": 57}, {\"accuracy\": 0.5, \"loss\": 0.3804289996623993, \"time-step\": 58}, {\"accuracy\": 0.5, \"loss\": 0.378351628780365, \"time-step\": 59}, {\"accuracy\": 0.5, \"loss\": 0.3762963116168976, \"time-step\": 60}, {\"accuracy\": 0.5, \"loss\": 0.3742629885673523, \"time-step\": 61}, {\"accuracy\": 0.5, \"loss\": 0.37225157022476196, \"time-step\": 62}, {\"accuracy\": 0.5, \"loss\": 0.3702619969844818, \"time-step\": 63}, {\"accuracy\": 0.5, \"loss\": 0.3682941794395447, \"time-step\": 64}, {\"accuracy\": 0.5, \"loss\": 0.3663480281829834, \"time-step\": 65}, {\"accuracy\": 0.5, \"loss\": 0.3644234538078308, \"time-step\": 66}, {\"accuracy\": 0.5, \"loss\": 0.36252039670944214, \"time-step\": 67}, {\"accuracy\": 0.5, \"loss\": 0.36063870787620544, \"time-step\": 68}, {\"accuracy\": 0.5, \"loss\": 0.3587782680988312, \"time-step\": 69}, {\"accuracy\": 0.5, \"loss\": 0.35693901777267456, \"time-step\": 70}, {\"accuracy\": 0.5, \"loss\": 0.35512083768844604, \"time-step\": 71}, {\"accuracy\": 0.5, \"loss\": 0.3533235490322113, \"time-step\": 72}, {\"accuracy\": 0.5, \"loss\": 0.3515471816062927, \"time-step\": 73}, {\"accuracy\": 0.5, \"loss\": 0.34979134798049927, \"time-step\": 74}, {\"accuracy\": 0.5, \"loss\": 0.3480561673641205, \"time-step\": 75}, {\"accuracy\": 0.5, \"loss\": 0.34634140133857727, \"time-step\": 76}, {\"accuracy\": 0.5, \"loss\": 0.3446469008922577, \"time-step\": 77}, {\"accuracy\": 0.5, \"loss\": 0.3429725468158722, \"time-step\": 78}, {\"accuracy\": 0.5, \"loss\": 0.34131819009780884, \"time-step\": 79}, {\"accuracy\": 0.5, \"loss\": 0.3396836817264557, \"time-step\": 80}, {\"accuracy\": 0.5, \"loss\": 0.3380688428878784, \"time-step\": 81}, {\"accuracy\": 0.5, \"loss\": 0.33647358417510986, \"time-step\": 82}, {\"accuracy\": 0.5, \"loss\": 0.3348976671695709, \"time-step\": 83}, {\"accuracy\": 0.5, \"loss\": 0.3333410322666168, \"time-step\": 84}, {\"accuracy\": 0.5, \"loss\": 0.3318033218383789, \"time-step\": 85}, {\"accuracy\": 0.5, \"loss\": 0.33028462529182434, \"time-step\": 86}, {\"accuracy\": 0.5, \"loss\": 0.32878464460372925, \"time-step\": 87}, {\"accuracy\": 0.5, \"loss\": 0.3273031711578369, \"time-step\": 88}, {\"accuracy\": 0.5, \"loss\": 0.32584014534950256, \"time-step\": 89}, {\"accuracy\": 0.5, \"loss\": 0.3243952989578247, \"time-step\": 90}, {\"accuracy\": 0.5, \"loss\": 0.3229685127735138, \"time-step\": 91}, {\"accuracy\": 0.5, \"loss\": 0.32155951857566833, \"time-step\": 92}, {\"accuracy\": 0.5, \"loss\": 0.32016825675964355, \"time-step\": 93}, {\"accuracy\": 0.5, \"loss\": 0.31879448890686035, \"time-step\": 94}, {\"accuracy\": 0.5, \"loss\": 0.3174380362033844, \"time-step\": 95}, {\"accuracy\": 0.5, \"loss\": 0.31609874963760376, \"time-step\": 96}, {\"accuracy\": 0.5, \"loss\": 0.3147764205932617, \"time-step\": 97}, {\"accuracy\": 0.5, \"loss\": 0.31347087025642395, \"time-step\": 98}, {\"accuracy\": 0.5, \"loss\": 0.31218191981315613, \"time-step\": 99}, {\"accuracy\": 0.5, \"loss\": 0.31090936064720154, \"time-step\": 100}, {\"accuracy\": 0.5, \"loss\": 0.30965307354927063, \"time-step\": 101}, {\"accuracy\": 0.5, \"loss\": 0.3084128201007843, \"time-step\": 102}, {\"accuracy\": 0.5, \"loss\": 0.30718839168548584, \"time-step\": 103}, {\"accuracy\": 0.5, \"loss\": 0.3059796988964081, \"time-step\": 104}, {\"accuracy\": 0.5, \"loss\": 0.30478644371032715, \"time-step\": 105}, {\"accuracy\": 0.5, \"loss\": 0.30360859632492065, \"time-step\": 106}, {\"accuracy\": 0.5, \"loss\": 0.30244576930999756, \"time-step\": 107}, {\"accuracy\": 0.5, \"loss\": 0.30129799246788025, \"time-step\": 108}, {\"accuracy\": 0.5, \"loss\": 0.3001649081707001, \"time-step\": 109}, {\"accuracy\": 0.5, \"loss\": 0.29904645681381226, \"time-step\": 110}, {\"accuracy\": 0.5, \"loss\": 0.2979423999786377, \"time-step\": 111}, {\"accuracy\": 0.5, \"loss\": 0.2968525290489197, \"time-step\": 112}, {\"accuracy\": 0.5, \"loss\": 0.29577672481536865, \"time-step\": 113}, {\"accuracy\": 0.5, \"loss\": 0.2947148084640503, \"time-step\": 114}, {\"accuracy\": 0.5, \"loss\": 0.2936665713787079, \"time-step\": 115}, {\"accuracy\": 0.5, \"loss\": 0.2926318347454071, \"time-step\": 116}, {\"accuracy\": 0.5, \"loss\": 0.2916104197502136, \"time-step\": 117}, {\"accuracy\": 0.5, \"loss\": 0.2906021773815155, \"time-step\": 118}, {\"accuracy\": 0.5, \"loss\": 0.2896069288253784, \"time-step\": 119}, {\"accuracy\": 0.5, \"loss\": 0.28862449526786804, \"time-step\": 120}, {\"accuracy\": 0.5, \"loss\": 0.28765466809272766, \"time-step\": 121}, {\"accuracy\": 0.5, \"loss\": 0.2866973876953125, \"time-step\": 122}, {\"accuracy\": 0.5, \"loss\": 0.2857523262500763, \"time-step\": 123}, {\"accuracy\": 0.5, \"loss\": 0.28481945395469666, \"time-step\": 124}, {\"accuracy\": 0.5, \"loss\": 0.2838984727859497, \"time-step\": 125}, {\"accuracy\": 0.5, \"loss\": 0.28298938274383545, \"time-step\": 126}, {\"accuracy\": 0.5, \"loss\": 0.2820919454097748, \"time-step\": 127}, {\"accuracy\": 0.5, \"loss\": 0.2812059223651886, \"time-step\": 128}, {\"accuracy\": 0.5, \"loss\": 0.28033125400543213, \"time-step\": 129}, {\"accuracy\": 0.5, \"loss\": 0.27946773171424866, \"time-step\": 130}, {\"accuracy\": 0.5, \"loss\": 0.27861523628234863, \"time-step\": 131}, {\"accuracy\": 0.5, \"loss\": 0.27777352929115295, \"time-step\": 132}, {\"accuracy\": 0.5, \"loss\": 0.27694255113601685, \"time-step\": 133}, {\"accuracy\": 0.5, \"loss\": 0.276122123003006, \"time-step\": 134}, {\"accuracy\": 0.5, \"loss\": 0.27531203627586365, \"time-step\": 135}, {\"accuracy\": 0.5, \"loss\": 0.27451223134994507, \"time-step\": 136}, {\"accuracy\": 0.5, \"loss\": 0.2737225294113159, \"time-step\": 137}, {\"accuracy\": 0.5, \"loss\": 0.2729426920413971, \"time-step\": 138}, {\"accuracy\": 0.5, \"loss\": 0.2721727192401886, \"time-step\": 139}, {\"accuracy\": 0.5, \"loss\": 0.27141234278678894, \"time-step\": 140}, {\"accuracy\": 0.5, \"loss\": 0.27066153287887573, \"time-step\": 141}, {\"accuracy\": 0.5, \"loss\": 0.2699200510978699, \"time-step\": 142}, {\"accuracy\": 0.5, \"loss\": 0.2691877782344818, \"time-step\": 143}, {\"accuracy\": 0.5, \"loss\": 0.2684646248817444, \"time-step\": 144}, {\"accuracy\": 0.5, \"loss\": 0.26775041222572327, \"time-step\": 145}, {\"accuracy\": 0.5, \"loss\": 0.2670450210571289, \"time-step\": 146}, {\"accuracy\": 0.5, \"loss\": 0.26634836196899414, \"time-step\": 147}, {\"accuracy\": 0.5, \"loss\": 0.26566022634506226, \"time-step\": 148}, {\"accuracy\": 0.5, \"loss\": 0.2649804949760437, \"time-step\": 149}, {\"accuracy\": 0.5, \"loss\": 0.2643090486526489, \"time-step\": 150}, {\"accuracy\": 0.5, \"loss\": 0.26364579796791077, \"time-step\": 151}, {\"accuracy\": 0.5, \"loss\": 0.2629906237125397, \"time-step\": 152}, {\"accuracy\": 0.5, \"loss\": 0.2623433470726013, \"time-step\": 153}, {\"accuracy\": 0.5, \"loss\": 0.26170384883880615, \"time-step\": 154}, {\"accuracy\": 0.5, \"loss\": 0.261072039604187, \"time-step\": 155}, {\"accuracy\": 0.5, \"loss\": 0.26044777035713196, \"time-step\": 156}, {\"accuracy\": 0.5, \"loss\": 0.25983095169067383, \"time-step\": 157}, {\"accuracy\": 0.5, \"loss\": 0.25922149419784546, \"time-step\": 158}, {\"accuracy\": 0.5, \"loss\": 0.2586192190647125, \"time-step\": 159}, {\"accuracy\": 0.5, \"loss\": 0.25802403688430786, \"time-step\": 160}, {\"accuracy\": 0.5, \"loss\": 0.2574358284473419, \"time-step\": 161}, {\"accuracy\": 0.5, \"loss\": 0.25685450434684753, \"time-step\": 162}, {\"accuracy\": 0.5, \"loss\": 0.25627991557121277, \"time-step\": 163}, {\"accuracy\": 0.5, \"loss\": 0.25571197271347046, \"time-step\": 164}, {\"accuracy\": 0.5, \"loss\": 0.25515061616897583, \"time-step\": 165}, {\"accuracy\": 0.5, \"loss\": 0.25459566712379456, \"time-step\": 166}, {\"accuracy\": 0.5, \"loss\": 0.2540470361709595, \"time-step\": 167}, {\"accuracy\": 0.5, \"loss\": 0.2535046637058258, \"time-step\": 168}, {\"accuracy\": 0.5, \"loss\": 0.252968430519104, \"time-step\": 169}, {\"accuracy\": 0.5, \"loss\": 0.2524382174015045, \"time-step\": 170}, {\"accuracy\": 0.5, \"loss\": 0.2519139051437378, \"time-step\": 171}, {\"accuracy\": 0.5, \"loss\": 0.25139540433883667, \"time-step\": 172}, {\"accuracy\": 0.5, \"loss\": 0.25088265538215637, \"time-step\": 173}, {\"accuracy\": 0.5, \"loss\": 0.25037553906440735, \"time-step\": 174}, {\"accuracy\": 0.5, \"loss\": 0.24987393617630005, \"time-step\": 175}, {\"accuracy\": 0.5, \"loss\": 0.24937781691551208, \"time-step\": 176}, {\"accuracy\": 0.5, \"loss\": 0.24888700246810913, \"time-step\": 177}, {\"accuracy\": 0.5, \"loss\": 0.2484014630317688, \"time-step\": 178}, {\"accuracy\": 0.5, \"loss\": 0.24792104959487915, \"time-step\": 179}, {\"accuracy\": 0.5, \"loss\": 0.24744576215744019, \"time-step\": 180}, {\"accuracy\": 0.5, \"loss\": 0.246975377202034, \"time-step\": 181}, {\"accuracy\": 0.5, \"loss\": 0.24650993943214417, \"time-step\": 182}, {\"accuracy\": 0.5, \"loss\": 0.24604931473731995, \"time-step\": 183}, {\"accuracy\": 0.5, \"loss\": 0.2455933541059494, \"time-step\": 184}, {\"accuracy\": 0.5, \"loss\": 0.24514207243919373, \"time-step\": 185}, {\"accuracy\": 0.5, \"loss\": 0.2446952760219574, \"time-step\": 186}, {\"accuracy\": 0.5, \"loss\": 0.2442529797554016, \"time-step\": 187}, {\"accuracy\": 0.5, \"loss\": 0.24381506443023682, \"time-step\": 188}, {\"accuracy\": 0.5, \"loss\": 0.24338144063949585, \"time-step\": 189}, {\"accuracy\": 0.5, \"loss\": 0.24295204877853394, \"time-step\": 190}, {\"accuracy\": 0.5, \"loss\": 0.24252678453922272, \"time-step\": 191}, {\"accuracy\": 0.5, \"loss\": 0.24210557341575623, \"time-step\": 192}, {\"accuracy\": 0.5, \"loss\": 0.2416883111000061, \"time-step\": 193}, {\"accuracy\": 0.5, \"loss\": 0.24127502739429474, \"time-step\": 194}, {\"accuracy\": 0.5, \"loss\": 0.2408655434846878, \"time-step\": 195}, {\"accuracy\": 0.5, \"loss\": 0.24045982956886292, \"time-step\": 196}, {\"accuracy\": 0.5, \"loss\": 0.24005776643753052, \"time-step\": 197}, {\"accuracy\": 0.5, \"loss\": 0.23965932428836823, \"time-step\": 198}, {\"accuracy\": 0.5, \"loss\": 0.23926447331905365, \"time-step\": 199}, {\"accuracy\": 0.5, \"loss\": 0.23887306451797485, \"time-step\": 200}, {\"accuracy\": 0.5, \"loss\": 0.23848506808280945, \"time-step\": 201}, {\"accuracy\": 0.5, \"loss\": 0.23810042440891266, \"time-step\": 202}, {\"accuracy\": 0.5, \"loss\": 0.23771905899047852, \"time-step\": 203}, {\"accuracy\": 0.5, \"loss\": 0.23734092712402344, \"time-step\": 204}, {\"accuracy\": 0.5, \"loss\": 0.23696589469909668, \"time-step\": 205}, {\"accuracy\": 0.5, \"loss\": 0.23659399151802063, \"time-step\": 206}, {\"accuracy\": 0.5, \"loss\": 0.23622509837150574, \"time-step\": 207}, {\"accuracy\": 0.5, \"loss\": 0.235859215259552, \"time-step\": 208}, {\"accuracy\": 0.5, \"loss\": 0.23549625277519226, \"time-step\": 209}, {\"accuracy\": 0.5, \"loss\": 0.23513606190681458, \"time-step\": 210}, {\"accuracy\": 0.5, \"loss\": 0.23477868735790253, \"time-step\": 211}, {\"accuracy\": 0.5, \"loss\": 0.23442411422729492, \"time-step\": 212}, {\"accuracy\": 0.5, \"loss\": 0.23407217860221863, \"time-step\": 213}, {\"accuracy\": 0.5, \"loss\": 0.23372286558151245, \"time-step\": 214}, {\"accuracy\": 0.5, \"loss\": 0.2333761751651764, \"time-step\": 215}, {\"accuracy\": 0.5, \"loss\": 0.2330319881439209, \"time-step\": 216}, {\"accuracy\": 0.5, \"loss\": 0.2326902598142624, \"time-step\": 217}, {\"accuracy\": 0.5, \"loss\": 0.23235100507736206, \"time-step\": 218}, {\"accuracy\": 0.5, \"loss\": 0.23201406002044678, \"time-step\": 219}, {\"accuracy\": 0.5, \"loss\": 0.2316794991493225, \"time-step\": 220}, {\"accuracy\": 0.5, \"loss\": 0.2313472181558609, \"time-step\": 221}, {\"accuracy\": 0.5, \"loss\": 0.2310171276330948, \"time-step\": 222}, {\"accuracy\": 0.5, \"loss\": 0.23068924248218536, \"time-step\": 223}, {\"accuracy\": 0.5, \"loss\": 0.23036348819732666, \"time-step\": 224}, {\"accuracy\": 0.5, \"loss\": 0.23003986477851868, \"time-step\": 225}, {\"accuracy\": 0.5, \"loss\": 0.22971829771995544, \"time-step\": 226}, {\"accuracy\": 0.5, \"loss\": 0.22939874231815338, \"time-step\": 227}, {\"accuracy\": 0.5, \"loss\": 0.2290811389684677, \"time-step\": 228}, {\"accuracy\": 0.5, \"loss\": 0.22876548767089844, \"time-step\": 229}, {\"accuracy\": 0.5, \"loss\": 0.2284516841173172, \"time-step\": 230}, {\"accuracy\": 0.5, \"loss\": 0.22813977301120758, \"time-step\": 231}, {\"accuracy\": 0.5, \"loss\": 0.22782960534095764, \"time-step\": 232}, {\"accuracy\": 0.5, \"loss\": 0.22752127051353455, \"time-step\": 233}, {\"accuracy\": 0.5, \"loss\": 0.22721469402313232, \"time-step\": 234}, {\"accuracy\": 0.5, \"loss\": 0.22690972685813904, \"time-step\": 235}, {\"accuracy\": 0.5, \"loss\": 0.22660645842552185, \"time-step\": 236}, {\"accuracy\": 0.5, \"loss\": 0.22630485892295837, \"time-step\": 237}, {\"accuracy\": 0.5, \"loss\": 0.22600480914115906, \"time-step\": 238}, {\"accuracy\": 0.5, \"loss\": 0.2257063239812851, \"time-step\": 239}, {\"accuracy\": 0.5, \"loss\": 0.2254093438386917, \"time-step\": 240}, {\"accuracy\": 0.5, \"loss\": 0.2251138985157013, \"time-step\": 241}, {\"accuracy\": 0.5, \"loss\": 0.22481989860534668, \"time-step\": 242}, {\"accuracy\": 0.5, \"loss\": 0.22452731430530548, \"time-step\": 243}, {\"accuracy\": 0.5, \"loss\": 0.2242361307144165, \"time-step\": 244}, {\"accuracy\": 0.5, \"loss\": 0.22394630312919617, \"time-step\": 245}, {\"accuracy\": 0.5, \"loss\": 0.2236577719449997, \"time-step\": 246}, {\"accuracy\": 0.5, \"loss\": 0.22337064146995544, \"time-step\": 247}, {\"accuracy\": 0.5, \"loss\": 0.2230847328901291, \"time-step\": 248}, {\"accuracy\": 0.5, \"loss\": 0.22279998660087585, \"time-step\": 249}, {\"accuracy\": 0.5, \"loss\": 0.22251658141613007, \"time-step\": 250}, {\"accuracy\": 0.5, \"loss\": 0.2222343236207962, \"time-step\": 251}, {\"accuracy\": 0.5, \"loss\": 0.22195321321487427, \"time-step\": 252}, {\"accuracy\": 0.5, \"loss\": 0.22167328000068665, \"time-step\": 253}, {\"accuracy\": 0.5, \"loss\": 0.2213944047689438, \"time-step\": 254}, {\"accuracy\": 0.5, \"loss\": 0.22111663222312927, \"time-step\": 255}, {\"accuracy\": 0.5, \"loss\": 0.22083988785743713, \"time-step\": 256}, {\"accuracy\": 0.5, \"loss\": 0.22056418657302856, \"time-step\": 257}, {\"accuracy\": 0.5, \"loss\": 0.22028948366641998, \"time-step\": 258}, {\"accuracy\": 0.5, \"loss\": 0.2200157642364502, \"time-step\": 259}, {\"accuracy\": 0.5, \"loss\": 0.21974296867847443, \"time-step\": 260}, {\"accuracy\": 0.5, \"loss\": 0.21947115659713745, \"time-step\": 261}, {\"accuracy\": 0.5, \"loss\": 0.21920020878314972, \"time-step\": 262}, {\"accuracy\": 0.5, \"loss\": 0.21893009543418884, \"time-step\": 263}, {\"accuracy\": 0.5, \"loss\": 0.21866083145141602, \"time-step\": 264}, {\"accuracy\": 0.5, \"loss\": 0.2183924913406372, \"time-step\": 265}, {\"accuracy\": 0.5, \"loss\": 0.21812483668327332, \"time-step\": 266}, {\"accuracy\": 0.5, \"loss\": 0.21785801649093628, \"time-step\": 267}, {\"accuracy\": 0.5, \"loss\": 0.21759188175201416, \"time-step\": 268}, {\"accuracy\": 0.5, \"loss\": 0.21732652187347412, \"time-step\": 269}, {\"accuracy\": 0.5, \"loss\": 0.21706189215183258, \"time-step\": 270}, {\"accuracy\": 0.5, \"loss\": 0.21679791808128357, \"time-step\": 271}, {\"accuracy\": 0.5, \"loss\": 0.2165345847606659, \"time-step\": 272}, {\"accuracy\": 0.75, \"loss\": 0.21627184748649597, \"time-step\": 273}, {\"accuracy\": 0.75, \"loss\": 0.21600975096225739, \"time-step\": 274}, {\"accuracy\": 0.75, \"loss\": 0.21574823558330536, \"time-step\": 275}, {\"accuracy\": 0.75, \"loss\": 0.2154873162508011, \"time-step\": 276}, {\"accuracy\": 0.75, \"loss\": 0.21522684395313263, \"time-step\": 277}, {\"accuracy\": 0.75, \"loss\": 0.21496692299842834, \"time-step\": 278}, {\"accuracy\": 0.75, \"loss\": 0.21470746397972107, \"time-step\": 279}, {\"accuracy\": 0.75, \"loss\": 0.21444851160049438, \"time-step\": 280}, {\"accuracy\": 0.75, \"loss\": 0.21418994665145874, \"time-step\": 281}, {\"accuracy\": 0.75, \"loss\": 0.21393179893493652, \"time-step\": 282}, {\"accuracy\": 0.75, \"loss\": 0.21367405354976654, \"time-step\": 283}, {\"accuracy\": 0.75, \"loss\": 0.2134166657924652, \"time-step\": 284}, {\"accuracy\": 0.75, \"loss\": 0.21315962076187134, \"time-step\": 285}, {\"accuracy\": 0.75, \"loss\": 0.21290285885334015, \"time-step\": 286}, {\"accuracy\": 0.75, \"loss\": 0.21264642477035522, \"time-step\": 287}, {\"accuracy\": 0.75, \"loss\": 0.2123902440071106, \"time-step\": 288}, {\"accuracy\": 0.75, \"loss\": 0.21213433146476746, \"time-step\": 289}, {\"accuracy\": 0.75, \"loss\": 0.2118786722421646, \"time-step\": 290}, {\"accuracy\": 0.75, \"loss\": 0.2116231620311737, \"time-step\": 291}, {\"accuracy\": 0.75, \"loss\": 0.21136784553527832, \"time-step\": 292}, {\"accuracy\": 0.75, \"loss\": 0.21111276745796204, \"time-step\": 293}, {\"accuracy\": 0.75, \"loss\": 0.21085777878761292, \"time-step\": 294}, {\"accuracy\": 0.75, \"loss\": 0.21060298383235931, \"time-step\": 295}, {\"accuracy\": 0.75, \"loss\": 0.2103482335805893, \"time-step\": 296}, {\"accuracy\": 0.75, \"loss\": 0.2100936323404312, \"time-step\": 297}, {\"accuracy\": 0.75, \"loss\": 0.2098391056060791, \"time-step\": 298}, {\"accuracy\": 0.75, \"loss\": 0.20958465337753296, \"time-step\": 299}, {\"accuracy\": 0.75, \"loss\": 0.2093302309513092, \"time-step\": 300}, {\"accuracy\": 0.75, \"loss\": 0.20907583832740784, \"time-step\": 301}, {\"accuracy\": 0.75, \"loss\": 0.20882155001163483, \"time-step\": 302}, {\"accuracy\": 0.75, \"loss\": 0.20856726169586182, \"time-step\": 303}, {\"accuracy\": 0.75, \"loss\": 0.20831292867660522, \"time-step\": 304}, {\"accuracy\": 0.75, \"loss\": 0.2080586552619934, \"time-step\": 305}, {\"accuracy\": 0.75, \"loss\": 0.207804337143898, \"time-step\": 306}, {\"accuracy\": 0.75, \"loss\": 0.2075500190258026, \"time-step\": 307}, {\"accuracy\": 0.75, \"loss\": 0.20729570090770721, \"time-step\": 308}, {\"accuracy\": 0.5, \"loss\": 0.20704127848148346, \"time-step\": 309}, {\"accuracy\": 0.5, \"loss\": 0.2067868411540985, \"time-step\": 310}, {\"accuracy\": 0.5, \"loss\": 0.20653235912322998, \"time-step\": 311}, {\"accuracy\": 0.5, \"loss\": 0.20627786219120026, \"time-step\": 312}, {\"accuracy\": 0.5, \"loss\": 0.20602327585220337, \"time-step\": 313}, {\"accuracy\": 0.5, \"loss\": 0.2057686150074005, \"time-step\": 314}, {\"accuracy\": 0.5, \"loss\": 0.20551392436027527, \"time-step\": 315}, {\"accuracy\": 0.5, \"loss\": 0.20525917410850525, \"time-step\": 316}, {\"accuracy\": 0.5, \"loss\": 0.2050042748451233, \"time-step\": 317}, {\"accuracy\": 0.5, \"loss\": 0.20474931597709656, \"time-step\": 318}, {\"accuracy\": 0.5, \"loss\": 0.20449437201023102, \"time-step\": 319}, {\"accuracy\": 0.5, \"loss\": 0.20423924922943115, \"time-step\": 320}, {\"accuracy\": 0.5, \"loss\": 0.2039840668439865, \"time-step\": 321}, {\"accuracy\": 0.5, \"loss\": 0.2037288248538971, \"time-step\": 322}, {\"accuracy\": 0.5, \"loss\": 0.20347346365451813, \"time-step\": 323}, {\"accuracy\": 0.5, \"loss\": 0.2032180279493332, \"time-step\": 324}, {\"accuracy\": 0.5, \"loss\": 0.20296251773834229, \"time-step\": 325}, {\"accuracy\": 0.5, \"loss\": 0.202706977725029, \"time-step\": 326}, {\"accuracy\": 0.5, \"loss\": 0.20245125889778137, \"time-step\": 327}, {\"accuracy\": 0.5, \"loss\": 0.20219549536705017, \"time-step\": 328}, {\"accuracy\": 0.5, \"loss\": 0.2019396871328354, \"time-step\": 329}, {\"accuracy\": 0.5, \"loss\": 0.20168371498584747, \"time-step\": 330}, {\"accuracy\": 0.5, \"loss\": 0.20142772793769836, \"time-step\": 331}, {\"accuracy\": 0.5, \"loss\": 0.20117168128490448, \"time-step\": 332}, {\"accuracy\": 0.5, \"loss\": 0.20091550052165985, \"time-step\": 333}, {\"accuracy\": 0.5, \"loss\": 0.20065930485725403, \"time-step\": 334}, {\"accuracy\": 0.5, \"loss\": 0.20040300488471985, \"time-step\": 335}, {\"accuracy\": 0.5, \"loss\": 0.2001466602087021, \"time-step\": 336}, {\"accuracy\": 0.5, \"loss\": 0.19989025592803955, \"time-step\": 337}, {\"accuracy\": 0.5, \"loss\": 0.19963376224040985, \"time-step\": 338}, {\"accuracy\": 0.5, \"loss\": 0.19937726855278015, \"time-step\": 339}, {\"accuracy\": 0.5, \"loss\": 0.19912071526050568, \"time-step\": 340}, {\"accuracy\": 0.5, \"loss\": 0.19886410236358643, \"time-step\": 341}, {\"accuracy\": 0.5, \"loss\": 0.1986074447631836, \"time-step\": 342}, {\"accuracy\": 0.5, \"loss\": 0.198350727558136, \"time-step\": 343}, {\"accuracy\": 0.5, \"loss\": 0.19809399545192719, \"time-step\": 344}, {\"accuracy\": 0.5, \"loss\": 0.19783729314804077, \"time-step\": 345}, {\"accuracy\": 0.5, \"loss\": 0.1975805163383484, \"time-step\": 346}, {\"accuracy\": 0.5, \"loss\": 0.1973237246274948, \"time-step\": 347}, {\"accuracy\": 0.5, \"loss\": 0.19706688821315765, \"time-step\": 348}, {\"accuracy\": 0.5, \"loss\": 0.19681009650230408, \"time-step\": 349}, {\"accuracy\": 0.5, \"loss\": 0.1965532898902893, \"time-step\": 350}, {\"accuracy\": 0.5, \"loss\": 0.1962965428829193, \"time-step\": 351}, {\"accuracy\": 0.5, \"loss\": 0.19603970646858215, \"time-step\": 352}, {\"accuracy\": 0.5, \"loss\": 0.19578295946121216, \"time-step\": 353}, {\"accuracy\": 0.5, \"loss\": 0.19552624225616455, \"time-step\": 354}, {\"accuracy\": 0.5, \"loss\": 0.19526953995227814, \"time-step\": 355}, {\"accuracy\": 0.5, \"loss\": 0.1950128674507141, \"time-step\": 356}, {\"accuracy\": 0.5, \"loss\": 0.19475623965263367, \"time-step\": 357}, {\"accuracy\": 0.5, \"loss\": 0.194499671459198, \"time-step\": 358}, {\"accuracy\": 0.5, \"loss\": 0.19424313306808472, \"time-step\": 359}, {\"accuracy\": 0.5, \"loss\": 0.1939866542816162, \"time-step\": 360}, {\"accuracy\": 0.5, \"loss\": 0.19373023509979248, \"time-step\": 361}, {\"accuracy\": 0.5, \"loss\": 0.1934739351272583, \"time-step\": 362}, {\"accuracy\": 0.5, \"loss\": 0.1932176798582077, \"time-step\": 363}, {\"accuracy\": 0.5, \"loss\": 0.19296149909496307, \"time-step\": 364}, {\"accuracy\": 0.5, \"loss\": 0.192705437541008, \"time-step\": 365}, {\"accuracy\": 0.5, \"loss\": 0.1924494206905365, \"time-step\": 366}, {\"accuracy\": 0.5, \"loss\": 0.19219356775283813, \"time-step\": 367}, {\"accuracy\": 0.5, \"loss\": 0.19193781912326813, \"time-step\": 368}, {\"accuracy\": 0.5, \"loss\": 0.1916821300983429, \"time-step\": 369}, {\"accuracy\": 0.5, \"loss\": 0.19142656028270721, \"time-step\": 370}, {\"accuracy\": 0.5, \"loss\": 0.19117116928100586, \"time-step\": 371}, {\"accuracy\": 0.5, \"loss\": 0.19091586768627167, \"time-step\": 372}, {\"accuracy\": 0.5, \"loss\": 0.19066071510314941, \"time-step\": 373}, {\"accuracy\": 0.5, \"loss\": 0.1904057115316391, \"time-step\": 374}, {\"accuracy\": 0.5, \"loss\": 0.19015084207057953, \"time-step\": 375}, {\"accuracy\": 0.5, \"loss\": 0.18989618122577667, \"time-step\": 376}, {\"accuracy\": 0.5, \"loss\": 0.18964159488677979, \"time-step\": 377}, {\"accuracy\": 0.5, \"loss\": 0.18938720226287842, \"time-step\": 378}, {\"accuracy\": 0.5, \"loss\": 0.18913297355175018, \"time-step\": 379}, {\"accuracy\": 0.5, \"loss\": 0.18887893855571747, \"time-step\": 380}, {\"accuracy\": 0.5, \"loss\": 0.18862508237361908, \"time-step\": 381}, {\"accuracy\": 0.5, \"loss\": 0.18837139010429382, \"time-step\": 382}, {\"accuracy\": 0.5, \"loss\": 0.1881178766489029, \"time-step\": 383}, {\"accuracy\": 0.5, \"loss\": 0.18786457180976868, \"time-step\": 384}, {\"accuracy\": 0.5, \"loss\": 0.18761146068572998, \"time-step\": 385}, {\"accuracy\": 0.5, \"loss\": 0.187358558177948, \"time-step\": 386}, {\"accuracy\": 0.5, \"loss\": 0.18710584938526154, \"time-step\": 387}, {\"accuracy\": 0.5, \"loss\": 0.1868533492088318, \"time-step\": 388}, {\"accuracy\": 0.5, \"loss\": 0.18660105764865875, \"time-step\": 389}, {\"accuracy\": 0.5, \"loss\": 0.18634900450706482, \"time-step\": 390}, {\"accuracy\": 0.5, \"loss\": 0.18609720468521118, \"time-step\": 391}, {\"accuracy\": 0.5, \"loss\": 0.18584555387496948, \"time-step\": 392}, {\"accuracy\": 0.5, \"loss\": 0.18559420108795166, \"time-step\": 393}, {\"accuracy\": 0.5, \"loss\": 0.18534307181835175, \"time-step\": 394}, {\"accuracy\": 0.5, \"loss\": 0.18509218096733093, \"time-step\": 395}, {\"accuracy\": 0.5, \"loss\": 0.1848416030406952, \"time-step\": 396}, {\"accuracy\": 0.5, \"loss\": 0.1845911592245102, \"time-step\": 397}, {\"accuracy\": 0.5, \"loss\": 0.18434108793735504, \"time-step\": 398}, {\"accuracy\": 0.5, \"loss\": 0.1840912401676178, \"time-step\": 399}, {\"accuracy\": 0.5, \"loss\": 0.18384161591529846, \"time-step\": 400}, {\"accuracy\": 0.5, \"loss\": 0.18359234929084778, \"time-step\": 401}, {\"accuracy\": 0.5, \"loss\": 0.1833432912826538, \"time-step\": 402}, {\"accuracy\": 0.5, \"loss\": 0.18309453129768372, \"time-step\": 403}, {\"accuracy\": 0.5, \"loss\": 0.1828460693359375, \"time-step\": 404}, {\"accuracy\": 0.5, \"loss\": 0.18259792029857635, \"time-step\": 405}, {\"accuracy\": 0.5, \"loss\": 0.18235008418560028, \"time-step\": 406}, {\"accuracy\": 0.5, \"loss\": 0.18210247159004211, \"time-step\": 407}, {\"accuracy\": 0.5, \"loss\": 0.181855246424675, \"time-step\": 408}, {\"accuracy\": 0.5, \"loss\": 0.18160827457904816, \"time-step\": 409}, {\"accuracy\": 0.5, \"loss\": 0.18136166036128998, \"time-step\": 410}, {\"accuracy\": 0.5, \"loss\": 0.18111541867256165, \"time-step\": 411}, {\"accuracy\": 0.5, \"loss\": 0.180869460105896, \"time-step\": 412}, {\"accuracy\": 0.5, \"loss\": 0.18062379956245422, \"time-step\": 413}, {\"accuracy\": 0.5, \"loss\": 0.1803785115480423, \"time-step\": 414}, {\"accuracy\": 0.5, \"loss\": 0.1801336109638214, \"time-step\": 415}, {\"accuracy\": 0.5, \"loss\": 0.1798890233039856, \"time-step\": 416}, {\"accuracy\": 0.5, \"loss\": 0.1796448528766632, \"time-step\": 417}, {\"accuracy\": 0.5, \"loss\": 0.17940093576908112, \"time-step\": 418}, {\"accuracy\": 0.5, \"loss\": 0.17915746569633484, \"time-step\": 419}, {\"accuracy\": 0.5, \"loss\": 0.17891442775726318, \"time-step\": 420}, {\"accuracy\": 0.5, \"loss\": 0.1786716878414154, \"time-step\": 421}, {\"accuracy\": 0.5, \"loss\": 0.17842935025691986, \"time-step\": 422}, {\"accuracy\": 0.5, \"loss\": 0.17818742990493774, \"time-step\": 423}, {\"accuracy\": 0.5, \"loss\": 0.17794592678546906, \"time-step\": 424}, {\"accuracy\": 0.5, \"loss\": 0.17770478129386902, \"time-step\": 425}, {\"accuracy\": 0.5, \"loss\": 0.1774640679359436, \"time-step\": 426}, {\"accuracy\": 0.5, \"loss\": 0.1772238165140152, \"time-step\": 427}, {\"accuracy\": 0.5, \"loss\": 0.17698392271995544, \"time-step\": 428}, {\"accuracy\": 0.5, \"loss\": 0.1767445057630539, \"time-step\": 429}, {\"accuracy\": 0.5, \"loss\": 0.17650547623634338, \"time-step\": 430}, {\"accuracy\": 0.5, \"loss\": 0.17626698315143585, \"time-step\": 431}, {\"accuracy\": 0.5, \"loss\": 0.17602887749671936, \"time-step\": 432}, {\"accuracy\": 0.5, \"loss\": 0.17579123377799988, \"time-step\": 433}, {\"accuracy\": 0.5, \"loss\": 0.1755540668964386, \"time-step\": 434}, {\"accuracy\": 0.5, \"loss\": 0.17531734704971313, \"time-step\": 435}, {\"accuracy\": 0.5, \"loss\": 0.17508114874362946, \"time-step\": 436}, {\"accuracy\": 0.5, \"loss\": 0.17484541237354279, \"time-step\": 437}, {\"accuracy\": 0.5, \"loss\": 0.17461015284061432, \"time-step\": 438}, {\"accuracy\": 0.5, \"loss\": 0.17437541484832764, \"time-step\": 439}, {\"accuracy\": 0.5, \"loss\": 0.17414118349552155, \"time-step\": 440}, {\"accuracy\": 0.5, \"loss\": 0.17390741407871246, \"time-step\": 441}, {\"accuracy\": 0.5, \"loss\": 0.17367421090602875, \"time-step\": 442}, {\"accuracy\": 0.5, \"loss\": 0.17344152927398682, \"time-step\": 443}, {\"accuracy\": 0.5, \"loss\": 0.17320935428142548, \"time-step\": 444}, {\"accuracy\": 0.5, \"loss\": 0.1729777306318283, \"time-step\": 445}, {\"accuracy\": 0.5, \"loss\": 0.1727466583251953, \"time-step\": 446}, {\"accuracy\": 0.5, \"loss\": 0.1725161373615265, \"time-step\": 447}, {\"accuracy\": 0.5, \"loss\": 0.17228618264198303, \"time-step\": 448}, {\"accuracy\": 0.5, \"loss\": 0.17205674946308136, \"time-step\": 449}, {\"accuracy\": 0.5, \"loss\": 0.17182792723178864, \"time-step\": 450}, {\"accuracy\": 0.5, \"loss\": 0.17159965634346008, \"time-step\": 451}, {\"accuracy\": 0.5, \"loss\": 0.17137198150157928, \"time-step\": 452}, {\"accuracy\": 0.5, \"loss\": 0.1711449772119522, \"time-step\": 453}, {\"accuracy\": 0.5, \"loss\": 0.17091846466064453, \"time-step\": 454}, {\"accuracy\": 0.5, \"loss\": 0.170692577958107, \"time-step\": 455}, {\"accuracy\": 0.5, \"loss\": 0.17046736180782318, \"time-step\": 456}, {\"accuracy\": 0.5, \"loss\": 0.17024271190166473, \"time-step\": 457}, {\"accuracy\": 0.5, \"loss\": 0.17001867294311523, \"time-step\": 458}, {\"accuracy\": 0.5, \"loss\": 0.16979533433914185, \"time-step\": 459}, {\"accuracy\": 0.5, \"loss\": 0.16957257688045502, \"time-step\": 460}, {\"accuracy\": 0.5, \"loss\": 0.16935047507286072, \"time-step\": 461}, {\"accuracy\": 0.5, \"loss\": 0.16912904381752014, \"time-step\": 462}, {\"accuracy\": 0.5, \"loss\": 0.1689082682132721, \"time-step\": 463}, {\"accuracy\": 0.5, \"loss\": 0.16868819296360016, \"time-step\": 464}, {\"accuracy\": 0.5, \"loss\": 0.16846874356269836, \"time-step\": 465}, {\"accuracy\": 0.5, \"loss\": 0.1682499796152115, \"time-step\": 466}, {\"accuracy\": 0.5, \"loss\": 0.1680319458246231, \"time-step\": 467}, {\"accuracy\": 0.5, \"loss\": 0.16781458258628845, \"time-step\": 468}, {\"accuracy\": 0.5, \"loss\": 0.16759788990020752, \"time-step\": 469}, {\"accuracy\": 0.5, \"loss\": 0.16738192737102509, \"time-step\": 470}, {\"accuracy\": 0.5, \"loss\": 0.16716663539409637, \"time-step\": 471}, {\"accuracy\": 0.5, \"loss\": 0.16695211827754974, \"time-step\": 472}, {\"accuracy\": 0.5, \"loss\": 0.16673830151557922, \"time-step\": 473}, {\"accuracy\": 0.5, \"loss\": 0.1665252298116684, \"time-step\": 474}, {\"accuracy\": 0.5, \"loss\": 0.16631287336349487, \"time-step\": 475}, {\"accuracy\": 0.5, \"loss\": 0.16610129177570343, \"time-step\": 476}, {\"accuracy\": 0.5, \"loss\": 0.1658904254436493, \"time-step\": 477}, {\"accuracy\": 0.5, \"loss\": 0.16568031907081604, \"time-step\": 478}, {\"accuracy\": 0.5, \"loss\": 0.16547095775604248, \"time-step\": 479}, {\"accuracy\": 0.5, \"loss\": 0.16526241600513458, \"time-step\": 480}, {\"accuracy\": 0.5, \"loss\": 0.1650545597076416, \"time-step\": 481}, {\"accuracy\": 0.5, \"loss\": 0.16484755277633667, \"time-step\": 482}, {\"accuracy\": 0.5, \"loss\": 0.16464130580425262, \"time-step\": 483}, {\"accuracy\": 0.5, \"loss\": 0.16443580389022827, \"time-step\": 484}, {\"accuracy\": 0.5, \"loss\": 0.16423116624355316, \"time-step\": 485}, {\"accuracy\": 0.5, \"loss\": 0.16402727365493774, \"time-step\": 486}, {\"accuracy\": 0.5, \"loss\": 0.16382423043251038, \"time-step\": 487}, {\"accuracy\": 0.5, \"loss\": 0.1636219620704651, \"time-step\": 488}, {\"accuracy\": 0.5, \"loss\": 0.16342049837112427, \"time-step\": 489}, {\"accuracy\": 0.5, \"loss\": 0.1632198691368103, \"time-step\": 490}, {\"accuracy\": 0.5, \"loss\": 0.1630200296640396, \"time-step\": 491}, {\"accuracy\": 0.5, \"loss\": 0.16282106935977936, \"time-step\": 492}, {\"accuracy\": 0.5, \"loss\": 0.1626228392124176, \"time-step\": 493}, {\"accuracy\": 0.5, \"loss\": 0.16242556273937225, \"time-step\": 494}, {\"accuracy\": 0.5, \"loss\": 0.16222906112670898, \"time-step\": 495}, {\"accuracy\": 0.5, \"loss\": 0.16203342378139496, \"time-step\": 496}, {\"accuracy\": 0.5, \"loss\": 0.1618386059999466, \"time-step\": 497}, {\"accuracy\": 0.5, \"loss\": 0.16164468228816986, \"time-step\": 498}, {\"accuracy\": 0.5, \"loss\": 0.1614515632390976, \"time-step\": 499}, {\"accuracy\": 0.5, \"loss\": 0.16125929355621338, \"time-step\": 500}, {\"accuracy\": 0.5, \"loss\": 0.16106794774532318, \"time-step\": 501}, {\"accuracy\": 0.5, \"loss\": 0.16087743639945984, \"time-step\": 502}, {\"accuracy\": 0.5, \"loss\": 0.16068777441978455, \"time-step\": 503}, {\"accuracy\": 0.5, \"loss\": 0.16049900650978088, \"time-step\": 504}, {\"accuracy\": 0.5, \"loss\": 0.16031110286712646, \"time-step\": 505}, {\"accuracy\": 0.5, \"loss\": 0.1601240336894989, \"time-step\": 506}, {\"accuracy\": 0.5, \"loss\": 0.15993782877922058, \"time-step\": 507}, {\"accuracy\": 0.5, \"loss\": 0.15975257754325867, \"time-step\": 508}, {\"accuracy\": 0.5, \"loss\": 0.1595681607723236, \"time-step\": 509}, {\"accuracy\": 0.5, \"loss\": 0.15938466787338257, \"time-step\": 510}, {\"accuracy\": 0.5, \"loss\": 0.159201979637146, \"time-step\": 511}, {\"accuracy\": 0.5, \"loss\": 0.15902025997638702, \"time-step\": 512}, {\"accuracy\": 0.5, \"loss\": 0.1588394194841385, \"time-step\": 513}, {\"accuracy\": 0.5, \"loss\": 0.158659428358078, \"time-step\": 514}, {\"accuracy\": 0.5, \"loss\": 0.15848037600517273, \"time-step\": 515}, {\"accuracy\": 0.5, \"loss\": 0.1583022177219391, \"time-step\": 516}, {\"accuracy\": 0.5, \"loss\": 0.1581249237060547, \"time-step\": 517}, {\"accuracy\": 0.5, \"loss\": 0.15794852375984192, \"time-step\": 518}, {\"accuracy\": 0.5, \"loss\": 0.15777301788330078, \"time-step\": 519}, {\"accuracy\": 0.5, \"loss\": 0.15759846568107605, \"time-step\": 520}, {\"accuracy\": 0.5, \"loss\": 0.15742477774620056, \"time-step\": 521}, {\"accuracy\": 0.5, \"loss\": 0.1572519838809967, \"time-step\": 522}, {\"accuracy\": 0.5, \"loss\": 0.15708008408546448, \"time-step\": 523}, {\"accuracy\": 0.5, \"loss\": 0.15690907835960388, \"time-step\": 524}, {\"accuracy\": 0.5, \"loss\": 0.1567390114068985, \"time-step\": 525}, {\"accuracy\": 0.5, \"loss\": 0.15656983852386475, \"time-step\": 526}, {\"accuracy\": 0.5, \"loss\": 0.15640157461166382, \"time-step\": 527}, {\"accuracy\": 0.5, \"loss\": 0.1562342345714569, \"time-step\": 528}, {\"accuracy\": 0.5, \"loss\": 0.15606772899627686, \"time-step\": 529}, {\"accuracy\": 0.5, \"loss\": 0.15590216219425201, \"time-step\": 530}, {\"accuracy\": 0.5, \"loss\": 0.15573745965957642, \"time-step\": 531}, {\"accuracy\": 0.5, \"loss\": 0.15557369589805603, \"time-step\": 532}, {\"accuracy\": 0.5, \"loss\": 0.15541081130504608, \"time-step\": 533}, {\"accuracy\": 0.5, \"loss\": 0.15524886548519135, \"time-step\": 534}, {\"accuracy\": 0.5, \"loss\": 0.15508776903152466, \"time-step\": 535}, {\"accuracy\": 0.5, \"loss\": 0.15492762625217438, \"time-step\": 536}, {\"accuracy\": 0.5, \"loss\": 0.15476834774017334, \"time-step\": 537}, {\"accuracy\": 0.5, \"loss\": 0.15460993349552155, \"time-step\": 538}, {\"accuracy\": 0.5, \"loss\": 0.15445244312286377, \"time-step\": 539}, {\"accuracy\": 0.5, \"loss\": 0.15429586172103882, \"time-step\": 540}, {\"accuracy\": 0.5, \"loss\": 0.15414012968540192, \"time-step\": 541}, {\"accuracy\": 0.5, \"loss\": 0.15398532152175903, \"time-step\": 542}, {\"accuracy\": 0.5, \"loss\": 0.1538313925266266, \"time-step\": 543}, {\"accuracy\": 0.5, \"loss\": 0.15367834270000458, \"time-step\": 544}, {\"accuracy\": 0.5, \"loss\": 0.1535261571407318, \"time-step\": 545}, {\"accuracy\": 0.5, \"loss\": 0.15337488055229187, \"time-step\": 546}, {\"accuracy\": 0.5, \"loss\": 0.15322451293468475, \"time-step\": 547}, {\"accuracy\": 0.5, \"loss\": 0.1530749648809433, \"time-step\": 548}, {\"accuracy\": 0.5, \"loss\": 0.15292634069919586, \"time-step\": 549}, {\"accuracy\": 0.5, \"loss\": 0.15277859568595886, \"time-step\": 550}, {\"accuracy\": 0.5, \"loss\": 0.15263167023658752, \"time-step\": 551}, {\"accuracy\": 0.5, \"loss\": 0.15248559415340424, \"time-step\": 552}, {\"accuracy\": 0.5, \"loss\": 0.15234044194221497, \"time-step\": 553}, {\"accuracy\": 0.5, \"loss\": 0.15219616889953613, \"time-step\": 554}, {\"accuracy\": 0.5, \"loss\": 0.15205271542072296, \"time-step\": 555}, {\"accuracy\": 0.5, \"loss\": 0.15191015601158142, \"time-step\": 556}, {\"accuracy\": 0.5, \"loss\": 0.15176841616630554, \"time-step\": 557}, {\"accuracy\": 0.5, \"loss\": 0.1516275256872177, \"time-step\": 558}, {\"accuracy\": 0.5, \"loss\": 0.15148746967315674, \"time-step\": 559}, {\"accuracy\": 0.5, \"loss\": 0.1513483226299286, \"time-step\": 560}, {\"accuracy\": 0.5, \"loss\": 0.15120995044708252, \"time-step\": 561}, {\"accuracy\": 0.5, \"loss\": 0.1510724425315857, \"time-step\": 562}, {\"accuracy\": 0.5, \"loss\": 0.1509358137845993, \"time-step\": 563}, {\"accuracy\": 0.5, \"loss\": 0.150799959897995, \"time-step\": 564}, {\"accuracy\": 0.5, \"loss\": 0.15066495537757874, \"time-step\": 565}, {\"accuracy\": 0.5, \"loss\": 0.15053074061870575, \"time-step\": 566}, {\"accuracy\": 0.5, \"loss\": 0.1503974199295044, \"time-step\": 567}, {\"accuracy\": 0.5, \"loss\": 0.15026487410068512, \"time-step\": 568}, {\"accuracy\": 0.5, \"loss\": 0.1501331329345703, \"time-step\": 569}, {\"accuracy\": 0.5, \"loss\": 0.15000227093696594, \"time-step\": 570}, {\"accuracy\": 0.5, \"loss\": 0.14987210929393768, \"time-step\": 571}, {\"accuracy\": 0.5, \"loss\": 0.14974281191825867, \"time-step\": 572}, {\"accuracy\": 0.5, \"loss\": 0.14961431920528412, \"time-step\": 573}, {\"accuracy\": 0.5, \"loss\": 0.14948660135269165, \"time-step\": 574}, {\"accuracy\": 0.5, \"loss\": 0.14935968816280365, \"time-step\": 575}, {\"accuracy\": 0.5, \"loss\": 0.14923354983329773, \"time-step\": 576}, {\"accuracy\": 0.5, \"loss\": 0.1491081565618515, \"time-step\": 577}, {\"accuracy\": 0.5, \"loss\": 0.14898362755775452, \"time-step\": 578}, {\"accuracy\": 0.5, \"loss\": 0.14885982871055603, \"time-step\": 579}, {\"accuracy\": 0.5, \"loss\": 0.148736834526062, \"time-step\": 580}, {\"accuracy\": 0.5, \"loss\": 0.14861458539962769, \"time-step\": 581}, {\"accuracy\": 0.5, \"loss\": 0.14849308133125305, \"time-step\": 582}, {\"accuracy\": 0.5, \"loss\": 0.1483723223209381, \"time-step\": 583}, {\"accuracy\": 0.5, \"loss\": 0.14825233817100525, \"time-step\": 584}, {\"accuracy\": 0.5, \"loss\": 0.1481330841779709, \"time-step\": 585}, {\"accuracy\": 0.5, \"loss\": 0.148014634847641, \"time-step\": 586}, {\"accuracy\": 0.5, \"loss\": 0.14789685606956482, \"time-step\": 587}, {\"accuracy\": 0.5, \"loss\": 0.14777985215187073, \"time-step\": 588}, {\"accuracy\": 0.5, \"loss\": 0.14766359329223633, \"time-step\": 589}, {\"accuracy\": 0.5, \"loss\": 0.14754801988601685, \"time-step\": 590}, {\"accuracy\": 0.5, \"loss\": 0.14743314683437347, \"time-step\": 591}, {\"accuracy\": 0.5, \"loss\": 0.14731907844543457, \"time-step\": 592}, {\"accuracy\": 0.5, \"loss\": 0.1472056806087494, \"time-step\": 593}, {\"accuracy\": 0.5, \"loss\": 0.14709296822547913, \"time-step\": 594}, {\"accuracy\": 0.5, \"loss\": 0.14698098599910736, \"time-step\": 595}, {\"accuracy\": 0.5, \"loss\": 0.14686965942382812, \"time-step\": 596}, {\"accuracy\": 0.5, \"loss\": 0.1467590183019638, \"time-step\": 597}, {\"accuracy\": 0.5, \"loss\": 0.14664916694164276, \"time-step\": 598}, {\"accuracy\": 0.5, \"loss\": 0.14653995633125305, \"time-step\": 599}, {\"accuracy\": 0.5, \"loss\": 0.14643138647079468, \"time-step\": 600}, {\"accuracy\": 0.5, \"loss\": 0.146323561668396, \"time-step\": 601}, {\"accuracy\": 0.5, \"loss\": 0.14621630311012268, \"time-step\": 602}, {\"accuracy\": 0.5, \"loss\": 0.14610977470874786, \"time-step\": 603}, {\"accuracy\": 0.5, \"loss\": 0.1460038721561432, \"time-step\": 604}, {\"accuracy\": 0.5, \"loss\": 0.145898699760437, \"time-step\": 605}, {\"accuracy\": 0.5, \"loss\": 0.14579418301582336, \"time-step\": 606}, {\"accuracy\": 0.5, \"loss\": 0.14569026231765747, \"time-step\": 607}, {\"accuracy\": 0.5, \"loss\": 0.1455869972705841, \"time-step\": 608}, {\"accuracy\": 0.5, \"loss\": 0.14548440277576447, \"time-step\": 609}, {\"accuracy\": 0.5, \"loss\": 0.14538241922855377, \"time-step\": 610}, {\"accuracy\": 0.5, \"loss\": 0.14528106153011322, \"time-step\": 611}, {\"accuracy\": 0.5, \"loss\": 0.145180344581604, \"time-step\": 612}, {\"accuracy\": 0.5, \"loss\": 0.14508023858070374, \"time-step\": 613}, {\"accuracy\": 0.5, \"loss\": 0.14498074352741241, \"time-step\": 614}, {\"accuracy\": 0.5, \"loss\": 0.14488190412521362, \"time-step\": 615}, {\"accuracy\": 0.5, \"loss\": 0.14478367567062378, \"time-step\": 616}, {\"accuracy\": 0.5, \"loss\": 0.1446859985589981, \"time-step\": 617}, {\"accuracy\": 0.5, \"loss\": 0.14458893239498138, \"time-step\": 618}, {\"accuracy\": 0.5, \"loss\": 0.14449246227741241, \"time-step\": 619}, {\"accuracy\": 0.5, \"loss\": 0.14439667761325836, \"time-step\": 620}, {\"accuracy\": 0.5, \"loss\": 0.1443013995885849, \"time-step\": 621}, {\"accuracy\": 0.5, \"loss\": 0.14420674741268158, \"time-step\": 622}, {\"accuracy\": 0.5, \"loss\": 0.14411264657974243, \"time-step\": 623}, {\"accuracy\": 0.5, \"loss\": 0.14401914179325104, \"time-step\": 624}, {\"accuracy\": 0.5, \"loss\": 0.14392626285552979, \"time-step\": 625}, {\"accuracy\": 0.5, \"loss\": 0.14383387565612793, \"time-step\": 626}, {\"accuracy\": 0.5, \"loss\": 0.1437421292066574, \"time-step\": 627}, {\"accuracy\": 0.5, \"loss\": 0.14365090429782867, \"time-step\": 628}, {\"accuracy\": 0.5, \"loss\": 0.1435602456331253, \"time-step\": 629}, {\"accuracy\": 0.5, \"loss\": 0.1434701383113861, \"time-step\": 630}, {\"accuracy\": 0.5, \"loss\": 0.14338064193725586, \"time-step\": 631}, {\"accuracy\": 0.5, \"loss\": 0.14329159259796143, \"time-step\": 632}, {\"accuracy\": 0.5, \"loss\": 0.14320319890975952, \"time-step\": 633}, {\"accuracy\": 0.5, \"loss\": 0.14311525225639343, \"time-step\": 634}, {\"accuracy\": 0.5, \"loss\": 0.1430279016494751, \"time-step\": 635}, {\"accuracy\": 0.5, \"loss\": 0.14294101297855377, \"time-step\": 636}, {\"accuracy\": 0.5, \"loss\": 0.14285477995872498, \"time-step\": 637}, {\"accuracy\": 0.5, \"loss\": 0.14276903867721558, \"time-step\": 638}, {\"accuracy\": 0.5, \"loss\": 0.14268380403518677, \"time-step\": 639}, {\"accuracy\": 0.5, \"loss\": 0.14259910583496094, \"time-step\": 640}, {\"accuracy\": 0.5, \"loss\": 0.1425148844718933, \"time-step\": 641}, {\"accuracy\": 0.5, \"loss\": 0.14243122935295105, \"time-step\": 642}, {\"accuracy\": 0.5, \"loss\": 0.1423480063676834, \"time-step\": 643}, {\"accuracy\": 0.5, \"loss\": 0.14226539433002472, \"time-step\": 644}, {\"accuracy\": 0.5, \"loss\": 0.14218324422836304, \"time-step\": 645}, {\"accuracy\": 0.5, \"loss\": 0.14210158586502075, \"time-step\": 646}, {\"accuracy\": 0.5, \"loss\": 0.14202043414115906, \"time-step\": 647}, {\"accuracy\": 0.5, \"loss\": 0.14193980395793915, \"time-step\": 648}, {\"accuracy\": 0.5, \"loss\": 0.14185968041419983, \"time-step\": 649}, {\"accuracy\": 0.5, \"loss\": 0.14177998900413513, \"time-step\": 650}, {\"accuracy\": 0.5, \"loss\": 0.14170080423355103, \"time-step\": 651}, {\"accuracy\": 0.5, \"loss\": 0.14162208139896393, \"time-step\": 652}, {\"accuracy\": 0.5, \"loss\": 0.1415438950061798, \"time-step\": 653}, {\"accuracy\": 0.5, \"loss\": 0.14146611094474792, \"time-step\": 654}, {\"accuracy\": 0.5, \"loss\": 0.14138886332511902, \"time-step\": 655}, {\"accuracy\": 0.5, \"loss\": 0.14131206274032593, \"time-step\": 656}, {\"accuracy\": 0.5, \"loss\": 0.14123576879501343, \"time-step\": 657}, {\"accuracy\": 0.5, \"loss\": 0.14115984737873077, \"time-step\": 658}, {\"accuracy\": 0.5, \"loss\": 0.14108441770076752, \"time-step\": 659}, {\"accuracy\": 0.5, \"loss\": 0.14100952446460724, \"time-step\": 660}, {\"accuracy\": 0.5, \"loss\": 0.1409350335597992, \"time-step\": 661}, {\"accuracy\": 0.5, \"loss\": 0.14086097478866577, \"time-step\": 662}, {\"accuracy\": 0.5, \"loss\": 0.14078739285469055, \"time-step\": 663}, {\"accuracy\": 0.5, \"loss\": 0.14071428775787354, \"time-step\": 664}, {\"accuracy\": 0.5, \"loss\": 0.1406414955854416, \"time-step\": 665}, {\"accuracy\": 0.5, \"loss\": 0.1405692994594574, \"time-step\": 666}, {\"accuracy\": 0.5, \"loss\": 0.14049747586250305, \"time-step\": 667}, {\"accuracy\": 0.5, \"loss\": 0.14042609930038452, \"time-step\": 668}, {\"accuracy\": 0.5, \"loss\": 0.14035511016845703, \"time-step\": 669}, {\"accuracy\": 0.5, \"loss\": 0.14028455317020416, \"time-step\": 670}, {\"accuracy\": 0.5, \"loss\": 0.1402144730091095, \"time-step\": 671}, {\"accuracy\": 0.5, \"loss\": 0.14014482498168945, \"time-step\": 672}, {\"accuracy\": 0.5, \"loss\": 0.14007553458213806, \"time-step\": 673}, {\"accuracy\": 0.5, \"loss\": 0.14000670611858368, \"time-step\": 674}, {\"accuracy\": 0.5, \"loss\": 0.13993826508522034, \"time-step\": 675}, {\"accuracy\": 0.5, \"loss\": 0.13987022638320923, \"time-step\": 676}, {\"accuracy\": 0.5, \"loss\": 0.13980261981487274, \"time-step\": 677}, {\"accuracy\": 0.5, \"loss\": 0.1397354006767273, \"time-step\": 678}, {\"accuracy\": 0.5, \"loss\": 0.13966859877109528, \"time-step\": 679}, {\"accuracy\": 0.5, \"loss\": 0.1396021991968155, \"time-step\": 680}, {\"accuracy\": 0.5, \"loss\": 0.13953618705272675, \"time-step\": 681}, {\"accuracy\": 0.5, \"loss\": 0.13947059214115143, \"time-step\": 682}, {\"accuracy\": 0.5, \"loss\": 0.13940538465976715, \"time-step\": 683}, {\"accuracy\": 0.5, \"loss\": 0.13934051990509033, \"time-step\": 684}, {\"accuracy\": 0.5, \"loss\": 0.13927613198757172, \"time-step\": 685}, {\"accuracy\": 0.5, \"loss\": 0.13921207189559937, \"time-step\": 686}, {\"accuracy\": 0.5, \"loss\": 0.13914835453033447, \"time-step\": 687}, {\"accuracy\": 0.5, \"loss\": 0.1390850991010666, \"time-step\": 688}, {\"accuracy\": 0.5, \"loss\": 0.1390221267938614, \"time-step\": 689}, {\"accuracy\": 0.5, \"loss\": 0.138959601521492, \"time-step\": 690}, {\"accuracy\": 0.5, \"loss\": 0.13889744877815247, \"time-step\": 691}, {\"accuracy\": 0.5, \"loss\": 0.13883565366268158, \"time-step\": 692}, {\"accuracy\": 0.5, \"loss\": 0.13877421617507935, \"time-step\": 693}, {\"accuracy\": 0.5, \"loss\": 0.13871312141418457, \"time-step\": 694}, {\"accuracy\": 0.5, \"loss\": 0.13865241408348083, \"time-step\": 695}, {\"accuracy\": 0.5, \"loss\": 0.13859206438064575, \"time-step\": 696}, {\"accuracy\": 0.5, \"loss\": 0.13853205740451813, \"time-step\": 697}, {\"accuracy\": 0.5, \"loss\": 0.13847237825393677, \"time-step\": 698}, {\"accuracy\": 0.5, \"loss\": 0.13841313123703003, \"time-step\": 699}, {\"accuracy\": 0.5, \"loss\": 0.13835419714450836, \"time-step\": 700}, {\"accuracy\": 0.5, \"loss\": 0.13829559087753296, \"time-step\": 701}, {\"accuracy\": 0.5, \"loss\": 0.13823732733726501, \"time-step\": 702}, {\"accuracy\": 0.5, \"loss\": 0.13817942142486572, \"time-step\": 703}, {\"accuracy\": 0.5, \"loss\": 0.13812187314033508, \"time-step\": 704}, {\"accuracy\": 0.5, \"loss\": 0.13806459307670593, \"time-step\": 705}, {\"accuracy\": 0.5, \"loss\": 0.1380077600479126, \"time-step\": 706}, {\"accuracy\": 0.5, \"loss\": 0.13795118033885956, \"time-step\": 707}, {\"accuracy\": 0.5, \"loss\": 0.13789492845535278, \"time-step\": 708}, {\"accuracy\": 0.5, \"loss\": 0.13783901929855347, \"time-step\": 709}, {\"accuracy\": 0.5, \"loss\": 0.1377834528684616, \"time-step\": 710}, {\"accuracy\": 0.5, \"loss\": 0.13772819936275482, \"time-step\": 711}, {\"accuracy\": 0.5, \"loss\": 0.1376732587814331, \"time-step\": 712}, {\"accuracy\": 0.5, \"loss\": 0.13761861622333527, \"time-step\": 713}, {\"accuracy\": 0.5, \"loss\": 0.13756433129310608, \"time-step\": 714}, {\"accuracy\": 0.5, \"loss\": 0.13751034438610077, \"time-step\": 715}, {\"accuracy\": 0.5, \"loss\": 0.13745670020580292, \"time-step\": 716}, {\"accuracy\": 0.5, \"loss\": 0.13740333914756775, \"time-step\": 717}, {\"accuracy\": 0.5, \"loss\": 0.13735030591487885, \"time-step\": 718}, {\"accuracy\": 0.5, \"loss\": 0.13729755580425262, \"time-step\": 719}, {\"accuracy\": 0.5, \"loss\": 0.13724511861801147, \"time-step\": 720}, {\"accuracy\": 0.5, \"loss\": 0.1371929794549942, \"time-step\": 721}, {\"accuracy\": 0.5, \"loss\": 0.1371411234140396, \"time-step\": 722}, {\"accuracy\": 0.5, \"loss\": 0.13708961009979248, \"time-step\": 723}, {\"accuracy\": 0.5, \"loss\": 0.13703835010528564, \"time-step\": 724}, {\"accuracy\": 0.5, \"loss\": 0.13698741793632507, \"time-step\": 725}, {\"accuracy\": 0.5, \"loss\": 0.136936753988266, \"time-step\": 726}, {\"accuracy\": 0.5, \"loss\": 0.13688640296459198, \"time-step\": 727}, {\"accuracy\": 0.5, \"loss\": 0.13683633506298065, \"time-step\": 728}, {\"accuracy\": 0.5, \"loss\": 0.13678652048110962, \"time-step\": 729}, {\"accuracy\": 0.5, \"loss\": 0.13673700392246246, \"time-step\": 730}, {\"accuracy\": 0.5, \"loss\": 0.1366877555847168, \"time-step\": 731}, {\"accuracy\": 0.5, \"loss\": 0.13663887977600098, \"time-step\": 732}, {\"accuracy\": 0.5, \"loss\": 0.1365901529788971, \"time-step\": 733}, {\"accuracy\": 0.5, \"loss\": 0.13654178380966187, \"time-step\": 734}, {\"accuracy\": 0.5, \"loss\": 0.1364937126636505, \"time-step\": 735}, {\"accuracy\": 0.5, \"loss\": 0.13644585013389587, \"time-step\": 736}, {\"accuracy\": 0.5, \"loss\": 0.13639827072620392, \"time-step\": 737}, {\"accuracy\": 0.5, \"loss\": 0.13635095953941345, \"time-step\": 738}, {\"accuracy\": 0.5, \"loss\": 0.13630390167236328, \"time-step\": 739}, {\"accuracy\": 0.5, \"loss\": 0.13625715672969818, \"time-step\": 740}, {\"accuracy\": 0.5, \"loss\": 0.136210635304451, \"time-step\": 741}, {\"accuracy\": 0.5, \"loss\": 0.13616439700126648, \"time-step\": 742}, {\"accuracy\": 0.5, \"loss\": 0.13611841201782227, \"time-step\": 743}, {\"accuracy\": 0.5, \"loss\": 0.13607271015644073, \"time-step\": 744}, {\"accuracy\": 0.5, \"loss\": 0.1360272467136383, \"time-step\": 745}, {\"accuracy\": 0.5, \"loss\": 0.13598202168941498, \"time-step\": 746}, {\"accuracy\": 0.5, \"loss\": 0.13593704998493195, \"time-step\": 747}, {\"accuracy\": 0.5, \"loss\": 0.1358923614025116, \"time-step\": 748}, {\"accuracy\": 0.5, \"loss\": 0.13584786653518677, \"time-step\": 749}, {\"accuracy\": 0.5, \"loss\": 0.13580365478992462, \"time-step\": 750}, {\"accuracy\": 0.5, \"loss\": 0.13575968146324158, \"time-step\": 751}, {\"accuracy\": 0.5, \"loss\": 0.1357160061597824, \"time-step\": 752}, {\"accuracy\": 0.5, \"loss\": 0.13567252457141876, \"time-step\": 753}, {\"accuracy\": 0.5, \"loss\": 0.1356292963027954, \"time-step\": 754}, {\"accuracy\": 0.5, \"loss\": 0.13558627665042877, \"time-step\": 755}, {\"accuracy\": 0.5, \"loss\": 0.13554349541664124, \"time-step\": 756}, {\"accuracy\": 0.5, \"loss\": 0.135500967502594, \"time-step\": 757}, {\"accuracy\": 0.5, \"loss\": 0.13545870780944824, \"time-step\": 758}, {\"accuracy\": 0.5, \"loss\": 0.1354166716337204, \"time-step\": 759}, {\"accuracy\": 0.5, \"loss\": 0.13537481427192688, \"time-step\": 760}, {\"accuracy\": 0.5, \"loss\": 0.13533319532871246, \"time-step\": 761}, {\"accuracy\": 0.5, \"loss\": 0.13529182970523834, \"time-step\": 762}, {\"accuracy\": 0.5, \"loss\": 0.1352507323026657, \"time-step\": 763}, {\"accuracy\": 0.5, \"loss\": 0.1352098435163498, \"time-step\": 764}, {\"accuracy\": 0.5, \"loss\": 0.135169118642807, \"time-step\": 765}, {\"accuracy\": 0.5, \"loss\": 0.13512861728668213, \"time-step\": 766}, {\"accuracy\": 0.5, \"loss\": 0.13508842885494232, \"time-step\": 767}, {\"accuracy\": 0.5, \"loss\": 0.13504837453365326, \"time-step\": 768}, {\"accuracy\": 0.5, \"loss\": 0.1350085735321045, \"time-step\": 769}, {\"accuracy\": 0.5, \"loss\": 0.13496898114681244, \"time-step\": 770}, {\"accuracy\": 0.5, \"loss\": 0.1349296122789383, \"time-step\": 771}, {\"accuracy\": 0.5, \"loss\": 0.13489045202732086, \"time-step\": 772}, {\"accuracy\": 0.5, \"loss\": 0.13485153019428253, \"time-step\": 773}, {\"accuracy\": 0.5, \"loss\": 0.13481274247169495, \"time-step\": 774}, {\"accuracy\": 0.5, \"loss\": 0.13477423787117004, \"time-step\": 775}, {\"accuracy\": 0.5, \"loss\": 0.13473588228225708, \"time-step\": 776}, {\"accuracy\": 0.5, \"loss\": 0.1346977949142456, \"time-step\": 777}, {\"accuracy\": 0.5, \"loss\": 0.13465988636016846, \"time-step\": 778}, {\"accuracy\": 0.5, \"loss\": 0.13462218642234802, \"time-step\": 779}, {\"accuracy\": 0.5, \"loss\": 0.1345846951007843, \"time-step\": 780}, {\"accuracy\": 0.5, \"loss\": 0.1345473825931549, \"time-step\": 781}, {\"accuracy\": 0.5, \"loss\": 0.1345103234052658, \"time-step\": 782}, {\"accuracy\": 0.5, \"loss\": 0.13447338342666626, \"time-step\": 783}, {\"accuracy\": 0.5, \"loss\": 0.13443665206432343, \"time-step\": 784}, {\"accuracy\": 0.5, \"loss\": 0.1344001740217209, \"time-step\": 785}, {\"accuracy\": 0.5, \"loss\": 0.13436384499073029, \"time-step\": 786}, {\"accuracy\": 0.5, \"loss\": 0.1343277245759964, \"time-step\": 787}, {\"accuracy\": 0.5, \"loss\": 0.13429179787635803, \"time-step\": 788}, {\"accuracy\": 0.5, \"loss\": 0.13425609469413757, \"time-step\": 789}, {\"accuracy\": 0.5, \"loss\": 0.13422051072120667, \"time-step\": 790}, {\"accuracy\": 0.5, \"loss\": 0.13418515026569366, \"time-step\": 791}, {\"accuracy\": 0.5, \"loss\": 0.1341499537229538, \"time-step\": 792}, {\"accuracy\": 0.5, \"loss\": 0.13411498069763184, \"time-step\": 793}, {\"accuracy\": 0.5, \"loss\": 0.1340802162885666, \"time-step\": 794}, {\"accuracy\": 0.5, \"loss\": 0.1340455710887909, \"time-step\": 795}, {\"accuracy\": 0.5, \"loss\": 0.1340111345052719, \"time-step\": 796}, {\"accuracy\": 0.5, \"loss\": 0.13397687673568726, \"time-step\": 797}, {\"accuracy\": 0.5, \"loss\": 0.13394276797771454, \"time-step\": 798}, {\"accuracy\": 0.5, \"loss\": 0.13390888273715973, \"time-step\": 799}, {\"accuracy\": 0.5, \"loss\": 0.13387513160705566, \"time-step\": 800}, {\"accuracy\": 0.5, \"loss\": 0.1338416039943695, \"time-step\": 801}, {\"accuracy\": 0.5, \"loss\": 0.13380829989910126, \"time-step\": 802}, {\"accuracy\": 0.5, \"loss\": 0.13377505540847778, \"time-step\": 803}, {\"accuracy\": 0.5, \"loss\": 0.1337420642375946, \"time-step\": 804}, {\"accuracy\": 0.5, \"loss\": 0.13370917737483978, \"time-step\": 805}, {\"accuracy\": 0.5, \"loss\": 0.13367648422718048, \"time-step\": 806}, {\"accuracy\": 0.5, \"loss\": 0.1336439847946167, \"time-step\": 807}, {\"accuracy\": 0.5, \"loss\": 0.13361164927482605, \"time-step\": 808}, {\"accuracy\": 0.5, \"loss\": 0.13357949256896973, \"time-step\": 809}, {\"accuracy\": 0.5, \"loss\": 0.13354744017124176, \"time-step\": 810}, {\"accuracy\": 0.5, \"loss\": 0.13351565599441528, \"time-step\": 811}, {\"accuracy\": 0.5, \"loss\": 0.13348397612571716, \"time-step\": 812}, {\"accuracy\": 0.5, \"loss\": 0.1334524303674698, \"time-step\": 813}, {\"accuracy\": 0.5, \"loss\": 0.13342107832431793, \"time-step\": 814}, {\"accuracy\": 0.5, \"loss\": 0.13338983058929443, \"time-step\": 815}, {\"accuracy\": 0.5, \"loss\": 0.13335882127285004, \"time-step\": 816}, {\"accuracy\": 0.5, \"loss\": 0.13332799077033997, \"time-step\": 817}, {\"accuracy\": 0.5, \"loss\": 0.13329726457595825, \"time-step\": 818}, {\"accuracy\": 0.5, \"loss\": 0.1332666575908661, \"time-step\": 819}, {\"accuracy\": 0.5, \"loss\": 0.13323631882667542, \"time-step\": 820}, {\"accuracy\": 0.5, \"loss\": 0.1332060545682907, \"time-step\": 821}, {\"accuracy\": 0.5, \"loss\": 0.13317592442035675, \"time-step\": 822}, {\"accuracy\": 0.5, \"loss\": 0.1331459879875183, \"time-step\": 823}, {\"accuracy\": 0.5, \"loss\": 0.133116215467453, \"time-step\": 824}, {\"accuracy\": 0.5, \"loss\": 0.13308656215667725, \"time-step\": 825}, {\"accuracy\": 0.5, \"loss\": 0.13305707275867462, \"time-step\": 826}, {\"accuracy\": 0.5, \"loss\": 0.13302770256996155, \"time-step\": 827}, {\"accuracy\": 0.5, \"loss\": 0.132998526096344, \"time-step\": 828}, {\"accuracy\": 0.5, \"loss\": 0.13296948373317719, \"time-step\": 829}, {\"accuracy\": 0.5, \"loss\": 0.1329406052827835, \"time-step\": 830}, {\"accuracy\": 0.5, \"loss\": 0.132911816239357, \"time-step\": 831}, {\"accuracy\": 0.5, \"loss\": 0.1328832507133484, \"time-step\": 832}, {\"accuracy\": 0.5, \"loss\": 0.13285477459430695, \"time-step\": 833}, {\"accuracy\": 0.5, \"loss\": 0.13282647728919983, \"time-step\": 834}, {\"accuracy\": 0.5, \"loss\": 0.1327982395887375, \"time-step\": 835}, {\"accuracy\": 0.5, \"loss\": 0.13277021050453186, \"time-step\": 836}, {\"accuracy\": 0.5, \"loss\": 0.13274233043193817, \"time-step\": 837}, {\"accuracy\": 0.5, \"loss\": 0.13271455466747284, \"time-step\": 838}, {\"accuracy\": 0.5, \"loss\": 0.13268694281578064, \"time-step\": 839}, {\"accuracy\": 0.5, \"loss\": 0.1326594352722168, \"time-step\": 840}, {\"accuracy\": 0.5, \"loss\": 0.13263213634490967, \"time-step\": 841}, {\"accuracy\": 0.5, \"loss\": 0.13260488212108612, \"time-step\": 842}, {\"accuracy\": 0.5, \"loss\": 0.1325778067111969, \"time-step\": 843}, {\"accuracy\": 0.5, \"loss\": 0.13255082070827484, \"time-step\": 844}, {\"accuracy\": 0.5, \"loss\": 0.1325240582227707, \"time-step\": 845}, {\"accuracy\": 0.5, \"loss\": 0.1324973702430725, \"time-step\": 846}, {\"accuracy\": 0.5, \"loss\": 0.13247081637382507, \"time-step\": 847}, {\"accuracy\": 0.5, \"loss\": 0.13244439661502838, \"time-step\": 848}, {\"accuracy\": 0.5, \"loss\": 0.13241808116436005, \"time-step\": 849}, {\"accuracy\": 0.5, \"loss\": 0.13239194452762604, \"time-step\": 850}, {\"accuracy\": 0.5, \"loss\": 0.13236591219902039, \"time-step\": 851}, {\"accuracy\": 0.5, \"loss\": 0.1323399692773819, \"time-step\": 852}, {\"accuracy\": 0.5, \"loss\": 0.1323142647743225, \"time-step\": 853}, {\"accuracy\": 0.5, \"loss\": 0.13228856027126312, \"time-step\": 854}, {\"accuracy\": 0.5, \"loss\": 0.13226306438446045, \"time-step\": 855}, {\"accuracy\": 0.5, \"loss\": 0.13223762810230255, \"time-step\": 856}, {\"accuracy\": 0.5, \"loss\": 0.13221237063407898, \"time-step\": 857}, {\"accuracy\": 0.5, \"loss\": 0.13218723237514496, \"time-step\": 858}, {\"accuracy\": 0.5, \"loss\": 0.1321621686220169, \"time-step\": 859}, {\"accuracy\": 0.5, \"loss\": 0.13213729858398438, \"time-step\": 860}, {\"accuracy\": 0.5, \"loss\": 0.13211245834827423, \"time-step\": 861}, {\"accuracy\": 0.5, \"loss\": 0.13208776712417603, \"time-step\": 862}, {\"accuracy\": 0.5, \"loss\": 0.13206326961517334, \"time-step\": 863}, {\"accuracy\": 0.5, \"loss\": 0.13203881680965424, \"time-step\": 864}, {\"accuracy\": 0.5, \"loss\": 0.1320144534111023, \"time-step\": 865}, {\"accuracy\": 0.5, \"loss\": 0.13199029862880707, \"time-step\": 866}, {\"accuracy\": 0.5, \"loss\": 0.13196618854999542, \"time-step\": 867}, {\"accuracy\": 0.5, \"loss\": 0.1319422423839569, \"time-step\": 868}, {\"accuracy\": 0.5, \"loss\": 0.13191841542720795, \"time-step\": 869}, {\"accuracy\": 0.5, \"loss\": 0.13189463317394257, \"time-step\": 870}, {\"accuracy\": 0.5, \"loss\": 0.13187099993228912, \"time-step\": 871}, {\"accuracy\": 0.5, \"loss\": 0.13184748589992523, \"time-step\": 872}, {\"accuracy\": 0.5, \"loss\": 0.13182410597801208, \"time-step\": 873}, {\"accuracy\": 0.5, \"loss\": 0.13180077075958252, \"time-step\": 874}, {\"accuracy\": 0.5, \"loss\": 0.1317775845527649, \"time-step\": 875}, {\"accuracy\": 0.5, \"loss\": 0.1317545473575592, \"time-step\": 876}, {\"accuracy\": 0.5, \"loss\": 0.13173161447048187, \"time-step\": 877}, {\"accuracy\": 0.5, \"loss\": 0.13170871138572693, \"time-step\": 878}, {\"accuracy\": 0.5, \"loss\": 0.13168595731258392, \"time-step\": 879}, {\"accuracy\": 0.5, \"loss\": 0.13166332244873047, \"time-step\": 880}, {\"accuracy\": 0.5, \"loss\": 0.13164082169532776, \"time-step\": 881}, {\"accuracy\": 0.5, \"loss\": 0.13161838054656982, \"time-step\": 882}, {\"accuracy\": 0.5, \"loss\": 0.13159605860710144, \"time-step\": 883}, {\"accuracy\": 0.5, \"loss\": 0.1315738558769226, \"time-step\": 884}, {\"accuracy\": 0.5, \"loss\": 0.13155172765254974, \"time-step\": 885}, {\"accuracy\": 0.5, \"loss\": 0.13152968883514404, \"time-step\": 886}, {\"accuracy\": 0.5, \"loss\": 0.13150779902935028, \"time-step\": 887}, {\"accuracy\": 0.5, \"loss\": 0.1314859688282013, \"time-step\": 888}, {\"accuracy\": 0.5, \"loss\": 0.13146430253982544, \"time-step\": 889}, {\"accuracy\": 0.5, \"loss\": 0.13144271075725555, \"time-step\": 890}, {\"accuracy\": 0.5, \"loss\": 0.13142120838165283, \"time-step\": 891}, {\"accuracy\": 0.5, \"loss\": 0.1313997507095337, \"time-step\": 892}, {\"accuracy\": 0.5, \"loss\": 0.13137848675251007, \"time-step\": 893}, {\"accuracy\": 0.5, \"loss\": 0.13135728240013123, \"time-step\": 894}, {\"accuracy\": 0.5, \"loss\": 0.13133618235588074, \"time-step\": 895}, {\"accuracy\": 0.5, \"loss\": 0.13131515681743622, \"time-step\": 896}, {\"accuracy\": 0.5, \"loss\": 0.13129428029060364, \"time-step\": 897}, {\"accuracy\": 0.5, \"loss\": 0.13127344846725464, \"time-step\": 898}, {\"accuracy\": 0.5, \"loss\": 0.1312527358531952, \"time-step\": 899}, {\"accuracy\": 0.5, \"loss\": 0.1312321275472641, \"time-step\": 900}, {\"accuracy\": 0.5, \"loss\": 0.13121160864830017, \"time-step\": 901}, {\"accuracy\": 0.5, \"loss\": 0.1311911940574646, \"time-step\": 902}, {\"accuracy\": 0.5, \"loss\": 0.1311708241701126, \"time-step\": 903}, {\"accuracy\": 0.5, \"loss\": 0.13115054368972778, \"time-step\": 904}, {\"accuracy\": 0.5, \"loss\": 0.1311304271221161, \"time-step\": 905}, {\"accuracy\": 0.5, \"loss\": 0.13111035525798798, \"time-step\": 906}, {\"accuracy\": 0.5, \"loss\": 0.13109037280082703, \"time-step\": 907}, {\"accuracy\": 0.5, \"loss\": 0.13107046484947205, \"time-step\": 908}, {\"accuracy\": 0.5, \"loss\": 0.13105067610740662, \"time-step\": 909}, {\"accuracy\": 0.5, \"loss\": 0.13103099167346954, \"time-step\": 910}, {\"accuracy\": 0.5, \"loss\": 0.13101135194301605, \"time-step\": 911}, {\"accuracy\": 0.5, \"loss\": 0.1309918761253357, \"time-step\": 912}, {\"accuracy\": 0.5, \"loss\": 0.13097238540649414, \"time-step\": 913}, {\"accuracy\": 0.5, \"loss\": 0.13095305860042572, \"time-step\": 914}, {\"accuracy\": 0.5, \"loss\": 0.13093379139900208, \"time-step\": 915}, {\"accuracy\": 0.5, \"loss\": 0.1309146285057068, \"time-step\": 916}, {\"accuracy\": 0.5, \"loss\": 0.13089556992053986, \"time-step\": 917}, {\"accuracy\": 0.5, \"loss\": 0.13087652623653412, \"time-step\": 918}, {\"accuracy\": 0.5, \"loss\": 0.1308576464653015, \"time-step\": 919}, {\"accuracy\": 0.5, \"loss\": 0.1308387815952301, \"time-step\": 920}, {\"accuracy\": 0.5, \"loss\": 0.13082002103328705, \"time-step\": 921}, {\"accuracy\": 0.5, \"loss\": 0.13080140948295593, \"time-step\": 922}, {\"accuracy\": 0.5, \"loss\": 0.1307828426361084, \"time-step\": 923}, {\"accuracy\": 0.5, \"loss\": 0.13076427578926086, \"time-step\": 924}, {\"accuracy\": 0.5, \"loss\": 0.13074591755867004, \"time-step\": 925}, {\"accuracy\": 0.5, \"loss\": 0.13072755932807922, \"time-step\": 926}, {\"accuracy\": 0.5, \"loss\": 0.13070929050445557, \"time-step\": 927}, {\"accuracy\": 0.5, \"loss\": 0.13069114089012146, \"time-step\": 928}, {\"accuracy\": 0.5, \"loss\": 0.13067305088043213, \"time-step\": 929}, {\"accuracy\": 0.5, \"loss\": 0.13065503537654877, \"time-step\": 930}, {\"accuracy\": 0.5, \"loss\": 0.13063710927963257, \"time-step\": 931}, {\"accuracy\": 0.5, \"loss\": 0.13061922788619995, \"time-step\": 932}, {\"accuracy\": 0.5, \"loss\": 0.13060148060321808, \"time-step\": 933}, {\"accuracy\": 0.5, \"loss\": 0.1305837482213974, \"time-step\": 934}, {\"accuracy\": 0.5, \"loss\": 0.13056616485118866, \"time-step\": 935}, {\"accuracy\": 0.5, \"loss\": 0.1305486559867859, \"time-step\": 936}, {\"accuracy\": 0.5, \"loss\": 0.1305311769247055, \"time-step\": 937}, {\"accuracy\": 0.5, \"loss\": 0.1305137574672699, \"time-step\": 938}, {\"accuracy\": 0.5, \"loss\": 0.13049647212028503, \"time-step\": 939}, {\"accuracy\": 0.5, \"loss\": 0.13047921657562256, \"time-step\": 940}, {\"accuracy\": 0.5, \"loss\": 0.13046206533908844, \"time-step\": 941}, {\"accuracy\": 0.5, \"loss\": 0.1304449737071991, \"time-step\": 942}, {\"accuracy\": 0.5, \"loss\": 0.1304279863834381, \"time-step\": 943}, {\"accuracy\": 0.5, \"loss\": 0.1304110288619995, \"time-step\": 944}, {\"accuracy\": 0.5, \"loss\": 0.1303941309452057, \"time-step\": 945}, {\"accuracy\": 0.5, \"loss\": 0.13037735223770142, \"time-step\": 946}, {\"accuracy\": 0.5, \"loss\": 0.13036063313484192, \"time-step\": 947}, {\"accuracy\": 0.5, \"loss\": 0.13034400343894958, \"time-step\": 948}, {\"accuracy\": 0.5, \"loss\": 0.13032744824886322, \"time-step\": 949}, {\"accuracy\": 0.5, \"loss\": 0.13031095266342163, \"time-step\": 950}, {\"accuracy\": 0.5, \"loss\": 0.13029447197914124, \"time-step\": 951}, {\"accuracy\": 0.5, \"loss\": 0.13027812540531158, \"time-step\": 952}, {\"accuracy\": 0.5, \"loss\": 0.1302618682384491, \"time-step\": 953}, {\"accuracy\": 0.5, \"loss\": 0.1302456110715866, \"time-step\": 954}, {\"accuracy\": 0.5, \"loss\": 0.13022947311401367, \"time-step\": 955}, {\"accuracy\": 0.5, \"loss\": 0.1302133947610855, \"time-step\": 956}, {\"accuracy\": 0.5, \"loss\": 0.13019736111164093, \"time-step\": 957}, {\"accuracy\": 0.5, \"loss\": 0.1301814317703247, \"time-step\": 958}, {\"accuracy\": 0.5, \"loss\": 0.13016559183597565, \"time-step\": 959}, {\"accuracy\": 0.5, \"loss\": 0.1301497220993042, \"time-step\": 960}, {\"accuracy\": 0.5, \"loss\": 0.1301340013742447, \"time-step\": 961}, {\"accuracy\": 0.5, \"loss\": 0.13011837005615234, \"time-step\": 962}, {\"accuracy\": 0.5, \"loss\": 0.1301027089357376, \"time-step\": 963}, {\"accuracy\": 0.5, \"loss\": 0.13008716702461243, \"time-step\": 964}, {\"accuracy\": 0.5, \"loss\": 0.1300716996192932, \"time-step\": 965}, {\"accuracy\": 0.5, \"loss\": 0.1300562620162964, \"time-step\": 966}, {\"accuracy\": 0.5, \"loss\": 0.1300409436225891, \"time-step\": 967}, {\"accuracy\": 0.5, \"loss\": 0.13002562522888184, \"time-step\": 968}, {\"accuracy\": 0.5, \"loss\": 0.13001039624214172, \"time-step\": 969}, {\"accuracy\": 0.5, \"loss\": 0.12999525666236877, \"time-step\": 970}, {\"accuracy\": 0.5, \"loss\": 0.129980206489563, \"time-step\": 971}, {\"accuracy\": 0.5, \"loss\": 0.1299651861190796, \"time-step\": 972}, {\"accuracy\": 0.5, \"loss\": 0.12995024025440216, \"time-step\": 973}, {\"accuracy\": 0.5, \"loss\": 0.12993527948856354, \"time-step\": 974}, {\"accuracy\": 0.5, \"loss\": 0.12992049753665924, \"time-step\": 975}, {\"accuracy\": 0.75, \"loss\": 0.12990568578243256, \"time-step\": 976}, {\"accuracy\": 0.75, \"loss\": 0.12989100813865662, \"time-step\": 977}, {\"accuracy\": 0.75, \"loss\": 0.12987633049488068, \"time-step\": 978}, {\"accuracy\": 0.75, \"loss\": 0.12986169755458832, \"time-step\": 979}, {\"accuracy\": 0.75, \"loss\": 0.1298471987247467, \"time-step\": 980}, {\"accuracy\": 0.75, \"loss\": 0.12983272969722748, \"time-step\": 981}, {\"accuracy\": 0.75, \"loss\": 0.12981830537319183, \"time-step\": 982}, {\"accuracy\": 0.75, \"loss\": 0.12980400025844574, \"time-step\": 983}, {\"accuracy\": 0.75, \"loss\": 0.12978969514369965, \"time-step\": 984}, {\"accuracy\": 0.75, \"loss\": 0.12977543473243713, \"time-step\": 985}, {\"accuracy\": 0.75, \"loss\": 0.12976127862930298, \"time-step\": 986}, {\"accuracy\": 0.75, \"loss\": 0.12974712252616882, \"time-step\": 987}, {\"accuracy\": 0.75, \"loss\": 0.1297331005334854, \"time-step\": 988}, {\"accuracy\": 0.75, \"loss\": 0.1297191083431244, \"time-step\": 989}, {\"accuracy\": 0.75, \"loss\": 0.12970516085624695, \"time-step\": 990}, {\"accuracy\": 0.75, \"loss\": 0.12969127297401428, \"time-step\": 991}, {\"accuracy\": 0.75, \"loss\": 0.12967748939990997, \"time-step\": 992}, {\"accuracy\": 0.75, \"loss\": 0.12966367602348328, \"time-step\": 993}, {\"accuracy\": 0.75, \"loss\": 0.12964996695518494, \"time-step\": 994}, {\"accuracy\": 0.75, \"loss\": 0.12963631749153137, \"time-step\": 995}, {\"accuracy\": 0.75, \"loss\": 0.129622682929039, \"time-step\": 996}, {\"accuracy\": 0.75, \"loss\": 0.1296091377735138, \"time-step\": 997}, {\"accuracy\": 0.75, \"loss\": 0.12959563732147217, \"time-step\": 998}, {\"accuracy\": 0.75, \"loss\": 0.12958219647407532, \"time-step\": 999}, {\"accuracy\": 0.75, \"loss\": 0.12956883013248444, \"time-step\": 1000}, {\"accuracy\": 0.75, \"loss\": 0.12955547869205475, \"time-step\": 1001}, {\"accuracy\": 0.75, \"loss\": 0.1295422464609146, \"time-step\": 1002}, {\"accuracy\": 0.75, \"loss\": 0.12952899932861328, \"time-step\": 1003}, {\"accuracy\": 0.75, \"loss\": 0.1295158863067627, \"time-step\": 1004}, {\"accuracy\": 0.75, \"loss\": 0.12950271368026733, \"time-step\": 1005}, {\"accuracy\": 0.75, \"loss\": 0.12948966026306152, \"time-step\": 1006}, {\"accuracy\": 0.75, \"loss\": 0.1294766664505005, \"time-step\": 1007}, {\"accuracy\": 0.75, \"loss\": 0.12946373224258423, \"time-step\": 1008}, {\"accuracy\": 0.75, \"loss\": 0.12945078313350677, \"time-step\": 1009}, {\"accuracy\": 0.75, \"loss\": 0.12943792343139648, \"time-step\": 1010}, {\"accuracy\": 0.75, \"loss\": 0.12942512333393097, \"time-step\": 1011}, {\"accuracy\": 0.75, \"loss\": 0.12941238284111023, \"time-step\": 1012}, {\"accuracy\": 0.75, \"loss\": 0.1293996274471283, \"time-step\": 1013}, {\"accuracy\": 0.75, \"loss\": 0.1293870359659195, \"time-step\": 1014}, {\"accuracy\": 0.75, \"loss\": 0.1293744593858719, \"time-step\": 1015}, {\"accuracy\": 0.75, \"loss\": 0.1293618530035019, \"time-step\": 1016}, {\"accuracy\": 0.75, \"loss\": 0.12934939563274384, \"time-step\": 1017}, {\"accuracy\": 0.75, \"loss\": 0.12933696806430817, \"time-step\": 1018}, {\"accuracy\": 0.75, \"loss\": 0.1293245404958725, \"time-step\": 1019}, {\"accuracy\": 0.75, \"loss\": 0.129312202334404, \"time-step\": 1020}, {\"accuracy\": 0.75, \"loss\": 0.12929990887641907, \"time-step\": 1021}, {\"accuracy\": 0.75, \"loss\": 0.12928763031959534, \"time-step\": 1022}, {\"accuracy\": 0.75, \"loss\": 0.12927544116973877, \"time-step\": 1023}, {\"accuracy\": 0.75, \"loss\": 0.1292632669210434, \"time-step\": 1024}, {\"accuracy\": 0.75, \"loss\": 0.12925118207931519, \"time-step\": 1025}, {\"accuracy\": 0.75, \"loss\": 0.12923909723758698, \"time-step\": 1026}, {\"accuracy\": 0.75, \"loss\": 0.12922713160514832, \"time-step\": 1027}, {\"accuracy\": 0.75, \"loss\": 0.12921518087387085, \"time-step\": 1028}, {\"accuracy\": 0.75, \"loss\": 0.1292032152414322, \"time-step\": 1029}, {\"accuracy\": 0.75, \"loss\": 0.12919136881828308, \"time-step\": 1030}, {\"accuracy\": 0.75, \"loss\": 0.12917953729629517, \"time-step\": 1031}, {\"accuracy\": 0.75, \"loss\": 0.12916776537895203, \"time-step\": 1032}, {\"accuracy\": 0.75, \"loss\": 0.12915600836277008, \"time-step\": 1033}, {\"accuracy\": 0.75, \"loss\": 0.1291443258523941, \"time-step\": 1034}, {\"accuracy\": 0.75, \"loss\": 0.12913267314434052, \"time-step\": 1035}, {\"accuracy\": 0.75, \"loss\": 0.1291210651397705, \"time-step\": 1036}, {\"accuracy\": 0.75, \"loss\": 0.12910956144332886, \"time-step\": 1037}, {\"accuracy\": 0.75, \"loss\": 0.129098042845726, \"time-step\": 1038}, {\"accuracy\": 0.75, \"loss\": 0.12908662855625153, \"time-step\": 1039}, {\"accuracy\": 0.75, \"loss\": 0.12907516956329346, \"time-step\": 1040}, {\"accuracy\": 0.75, \"loss\": 0.12906385958194733, \"time-step\": 1041}, {\"accuracy\": 0.75, \"loss\": 0.12905244529247284, \"time-step\": 1042}, {\"accuracy\": 0.75, \"loss\": 0.12904125452041626, \"time-step\": 1043}, {\"accuracy\": 0.75, \"loss\": 0.12902992963790894, \"time-step\": 1044}, {\"accuracy\": 0.75, \"loss\": 0.12901881337165833, \"time-step\": 1045}, {\"accuracy\": 0.75, \"loss\": 0.12900756299495697, \"time-step\": 1046}, {\"accuracy\": 0.75, \"loss\": 0.12899650633335114, \"time-step\": 1047}, {\"accuracy\": 0.75, \"loss\": 0.12898539006710052, \"time-step\": 1048}, {\"accuracy\": 0.75, \"loss\": 0.12897439301013947, \"time-step\": 1049}, {\"accuracy\": 0.75, \"loss\": 0.12896335124969482, \"time-step\": 1050}, {\"accuracy\": 0.75, \"loss\": 0.1289524883031845, \"time-step\": 1051}, {\"accuracy\": 0.75, \"loss\": 0.12894153594970703, \"time-step\": 1052}, {\"accuracy\": 0.75, \"loss\": 0.1289307177066803, \"time-step\": 1053}, {\"accuracy\": 0.75, \"loss\": 0.12891986966133118, \"time-step\": 1054}, {\"accuracy\": 0.75, \"loss\": 0.12890909612178802, \"time-step\": 1055}, {\"accuracy\": 0.75, \"loss\": 0.12889838218688965, \"time-step\": 1056}, {\"accuracy\": 0.75, \"loss\": 0.12888763844966888, \"time-step\": 1057}, {\"accuracy\": 0.75, \"loss\": 0.12887702882289886, \"time-step\": 1058}, {\"accuracy\": 0.75, \"loss\": 0.12886638939380646, \"time-step\": 1059}, {\"accuracy\": 0.75, \"loss\": 0.12885582447052002, \"time-step\": 1060}, {\"accuracy\": 0.75, \"loss\": 0.12884530425071716, \"time-step\": 1061}, {\"accuracy\": 0.75, \"loss\": 0.1288347840309143, \"time-step\": 1062}, {\"accuracy\": 0.75, \"loss\": 0.12882433831691742, \"time-step\": 1063}, {\"accuracy\": 0.75, \"loss\": 0.1288139522075653, \"time-step\": 1064}, {\"accuracy\": 0.75, \"loss\": 0.1288035809993744, \"time-step\": 1065}, {\"accuracy\": 0.75, \"loss\": 0.12879323959350586, \"time-step\": 1066}, {\"accuracy\": 0.75, \"loss\": 0.12878289818763733, \"time-step\": 1067}, {\"accuracy\": 0.75, \"loss\": 0.12877267599105835, \"time-step\": 1068}, {\"accuracy\": 0.75, \"loss\": 0.12876248359680176, \"time-step\": 1069}, {\"accuracy\": 0.75, \"loss\": 0.12875230610370636, \"time-step\": 1070}, {\"accuracy\": 0.75, \"loss\": 0.12874212861061096, \"time-step\": 1071}, {\"accuracy\": 0.75, \"loss\": 0.12873202562332153, \"time-step\": 1072}, {\"accuracy\": 0.75, \"loss\": 0.1287219524383545, \"time-step\": 1073}, {\"accuracy\": 0.75, \"loss\": 0.12871193885803223, \"time-step\": 1074}, {\"accuracy\": 0.75, \"loss\": 0.12870194017887115, \"time-step\": 1075}, {\"accuracy\": 0.75, \"loss\": 0.12869197130203247, \"time-step\": 1076}, {\"accuracy\": 0.75, \"loss\": 0.12868207693099976, \"time-step\": 1077}, {\"accuracy\": 0.75, \"loss\": 0.12867216765880585, \"time-step\": 1078}, {\"accuracy\": 0.75, \"loss\": 0.1286623477935791, \"time-step\": 1079}, {\"accuracy\": 0.75, \"loss\": 0.12865251302719116, \"time-step\": 1080}, {\"accuracy\": 0.75, \"loss\": 0.128642737865448, \"time-step\": 1081}, {\"accuracy\": 0.75, \"loss\": 0.12863308191299438, \"time-step\": 1082}, {\"accuracy\": 0.75, \"loss\": 0.1286233365535736, \"time-step\": 1083}, {\"accuracy\": 0.75, \"loss\": 0.1286136656999588, \"time-step\": 1084}, {\"accuracy\": 0.75, \"loss\": 0.12860405445098877, \"time-step\": 1085}, {\"accuracy\": 0.75, \"loss\": 0.12859448790550232, \"time-step\": 1086}, {\"accuracy\": 0.75, \"loss\": 0.12858492136001587, \"time-step\": 1087}, {\"accuracy\": 0.75, \"loss\": 0.12857535481452942, \"time-step\": 1088}, {\"accuracy\": 0.75, \"loss\": 0.12856583297252655, \"time-step\": 1089}, {\"accuracy\": 0.75, \"loss\": 0.12855646014213562, \"time-step\": 1090}, {\"accuracy\": 0.75, \"loss\": 0.12854699790477753, \"time-step\": 1091}, {\"accuracy\": 0.75, \"loss\": 0.1285376399755478, \"time-step\": 1092}, {\"accuracy\": 0.75, \"loss\": 0.12852823734283447, \"time-step\": 1093}, {\"accuracy\": 0.75, \"loss\": 0.1285189390182495, \"time-step\": 1094}, {\"accuracy\": 0.75, \"loss\": 0.12850968539714813, \"time-step\": 1095}, {\"accuracy\": 0.75, \"loss\": 0.12850043177604675, \"time-step\": 1096}, {\"accuracy\": 0.75, \"loss\": 0.12849120795726776, \"time-step\": 1097}, {\"accuracy\": 0.75, \"loss\": 0.12848204374313354, \"time-step\": 1098}, {\"accuracy\": 0.75, \"loss\": 0.12847289443016052, \"time-step\": 1099}, {\"accuracy\": 0.75, \"loss\": 0.12846378982067108, \"time-step\": 1100}, {\"accuracy\": 0.75, \"loss\": 0.12845468521118164, \"time-step\": 1101}, {\"accuracy\": 0.75, \"loss\": 0.12844569981098175, \"time-step\": 1102}, {\"accuracy\": 0.75, \"loss\": 0.12843665480613708, \"time-step\": 1103}, {\"accuracy\": 0.75, \"loss\": 0.128427654504776, \"time-step\": 1104}, {\"accuracy\": 0.75, \"loss\": 0.12841874361038208, \"time-step\": 1105}, {\"accuracy\": 0.75, \"loss\": 0.12840984761714935, \"time-step\": 1106}, {\"accuracy\": 0.75, \"loss\": 0.12840095162391663, \"time-step\": 1107}, {\"accuracy\": 0.75, \"loss\": 0.1283920556306839, \"time-step\": 1108}, {\"accuracy\": 0.75, \"loss\": 0.12838327884674072, \"time-step\": 1109}, {\"accuracy\": 0.75, \"loss\": 0.12837445735931396, \"time-step\": 1110}, {\"accuracy\": 0.75, \"loss\": 0.12836569547653198, \"time-step\": 1111}, {\"accuracy\": 0.75, \"loss\": 0.12835703790187836, \"time-step\": 1112}, {\"accuracy\": 0.75, \"loss\": 0.12834829092025757, \"time-step\": 1113}, {\"accuracy\": 0.75, \"loss\": 0.12833963334560394, \"time-step\": 1114}, {\"accuracy\": 0.75, \"loss\": 0.1283309906721115, \"time-step\": 1115}, {\"accuracy\": 0.75, \"loss\": 0.12832242250442505, \"time-step\": 1116}, {\"accuracy\": 0.75, \"loss\": 0.1283138394355774, \"time-step\": 1117}, {\"accuracy\": 0.75, \"loss\": 0.12830527126789093, \"time-step\": 1118}, {\"accuracy\": 0.75, \"loss\": 0.12829680740833282, \"time-step\": 1119}, {\"accuracy\": 0.75, \"loss\": 0.12828831374645233, \"time-step\": 1120}, {\"accuracy\": 0.75, \"loss\": 0.12827980518341064, \"time-step\": 1121}, {\"accuracy\": 0.75, \"loss\": 0.12827147543430328, \"time-step\": 1122}, {\"accuracy\": 0.75, \"loss\": 0.12826302647590637, \"time-step\": 1123}, {\"accuracy\": 0.75, \"loss\": 0.1282547116279602, \"time-step\": 1124}, {\"accuracy\": 0.75, \"loss\": 0.12824636697769165, \"time-step\": 1125}, {\"accuracy\": 0.75, \"loss\": 0.12823808193206787, \"time-step\": 1126}, {\"accuracy\": 0.75, \"loss\": 0.1282297670841217, \"time-step\": 1127}, {\"accuracy\": 0.75, \"loss\": 0.1282215416431427, \"time-step\": 1128}, {\"accuracy\": 0.75, \"loss\": 0.12821337580680847, \"time-step\": 1129}, {\"accuracy\": 0.75, \"loss\": 0.12820515036582947, \"time-step\": 1130}, {\"accuracy\": 0.75, \"loss\": 0.12819696962833405, \"time-step\": 1131}, {\"accuracy\": 0.75, \"loss\": 0.1281888782978058, \"time-step\": 1132}, {\"accuracy\": 0.75, \"loss\": 0.12818074226379395, \"time-step\": 1133}, {\"accuracy\": 0.75, \"loss\": 0.12817268073558807, \"time-step\": 1134}, {\"accuracy\": 0.75, \"loss\": 0.1281646490097046, \"time-step\": 1135}, {\"accuracy\": 0.75, \"loss\": 0.1281566172838211, \"time-step\": 1136}, {\"accuracy\": 0.75, \"loss\": 0.1281486302614212, \"time-step\": 1137}, {\"accuracy\": 0.75, \"loss\": 0.1281406730413437, \"time-step\": 1138}, {\"accuracy\": 0.75, \"loss\": 0.12813280522823334, \"time-step\": 1139}, {\"accuracy\": 0.75, \"loss\": 0.1281248927116394, \"time-step\": 1140}, {\"accuracy\": 0.75, \"loss\": 0.12811702489852905, \"time-step\": 1141}, {\"accuracy\": 0.75, \"loss\": 0.1281091272830963, \"time-step\": 1142}, {\"accuracy\": 0.75, \"loss\": 0.12810128927230835, \"time-step\": 1143}, {\"accuracy\": 0.75, \"loss\": 0.12809352576732635, \"time-step\": 1144}, {\"accuracy\": 0.75, \"loss\": 0.12808577716350555, \"time-step\": 1145}, {\"accuracy\": 0.75, \"loss\": 0.12807805836200714, \"time-step\": 1146}, {\"accuracy\": 0.75, \"loss\": 0.12807030975818634, \"time-step\": 1147}, {\"accuracy\": 0.75, \"loss\": 0.12806262075901031, \"time-step\": 1148}, {\"accuracy\": 0.75, \"loss\": 0.1280549168586731, \"time-step\": 1149}, {\"accuracy\": 0.75, \"loss\": 0.12804734706878662, \"time-step\": 1150}, {\"accuracy\": 0.75, \"loss\": 0.12803973257541656, \"time-step\": 1151}, {\"accuracy\": 0.75, \"loss\": 0.1280321180820465, \"time-step\": 1152}, {\"accuracy\": 0.75, \"loss\": 0.1280246078968048, \"time-step\": 1153}, {\"accuracy\": 0.75, \"loss\": 0.12801703810691833, \"time-step\": 1154}, {\"accuracy\": 0.75, \"loss\": 0.12800955772399902, \"time-step\": 1155}, {\"accuracy\": 0.75, \"loss\": 0.1280021071434021, \"time-step\": 1156}, {\"accuracy\": 0.75, \"loss\": 0.12799465656280518, \"time-step\": 1157}, {\"accuracy\": 0.75, \"loss\": 0.12798717617988586, \"time-step\": 1158}, {\"accuracy\": 0.75, \"loss\": 0.12797978520393372, \"time-step\": 1159}, {\"accuracy\": 0.75, \"loss\": 0.12797249853610992, \"time-step\": 1160}, {\"accuracy\": 0.75, \"loss\": 0.1279650628566742, \"time-step\": 1161}, {\"accuracy\": 0.75, \"loss\": 0.1279577910900116, \"time-step\": 1162}, {\"accuracy\": 0.75, \"loss\": 0.12795042991638184, \"time-step\": 1163}, {\"accuracy\": 0.75, \"loss\": 0.12794317305088043, \"time-step\": 1164}, {\"accuracy\": 0.75, \"loss\": 0.12793593108654022, \"time-step\": 1165}, {\"accuracy\": 0.75, \"loss\": 0.1279287338256836, \"time-step\": 1166}, {\"accuracy\": 0.75, \"loss\": 0.12792149186134338, \"time-step\": 1167}, {\"accuracy\": 0.75, \"loss\": 0.12791430950164795, \"time-step\": 1168}, {\"accuracy\": 0.75, \"loss\": 0.1279071867465973, \"time-step\": 1169}, {\"accuracy\": 0.75, \"loss\": 0.12790004909038544, \"time-step\": 1170}, {\"accuracy\": 0.75, \"loss\": 0.12789295613765717, \"time-step\": 1171}, {\"accuracy\": 0.75, \"loss\": 0.1278858184814453, \"time-step\": 1172}, {\"accuracy\": 0.75, \"loss\": 0.12787878513336182, \"time-step\": 1173}, {\"accuracy\": 0.75, \"loss\": 0.1278718113899231, \"time-step\": 1174}, {\"accuracy\": 0.75, \"loss\": 0.1278647780418396, \"time-step\": 1175}, {\"accuracy\": 0.75, \"loss\": 0.12785780429840088, \"time-step\": 1176}, {\"accuracy\": 0.75, \"loss\": 0.12785086035728455, \"time-step\": 1177}, {\"accuracy\": 0.75, \"loss\": 0.1278439164161682, \"time-step\": 1178}, {\"accuracy\": 0.75, \"loss\": 0.12783700227737427, \"time-step\": 1179}, {\"accuracy\": 0.75, \"loss\": 0.12783008813858032, \"time-step\": 1180}, {\"accuracy\": 0.5, \"loss\": 0.12782320380210876, \"time-step\": 1181}, {\"accuracy\": 0.5, \"loss\": 0.1278163641691208, \"time-step\": 1182}, {\"accuracy\": 0.5, \"loss\": 0.1278095245361328, \"time-step\": 1183}, {\"accuracy\": 0.5, \"loss\": 0.1278027445077896, \"time-step\": 1184}, {\"accuracy\": 0.5, \"loss\": 0.1277959942817688, \"time-step\": 1185}, {\"accuracy\": 0.5, \"loss\": 0.1277892291545868, \"time-step\": 1186}, {\"accuracy\": 0.5, \"loss\": 0.12778246402740479, \"time-step\": 1187}, {\"accuracy\": 0.5, \"loss\": 0.12777578830718994, \"time-step\": 1188}, {\"accuracy\": 0.5, \"loss\": 0.1277691125869751, \"time-step\": 1189}, {\"accuracy\": 0.5, \"loss\": 0.12776242196559906, \"time-step\": 1190}, {\"accuracy\": 0.5, \"loss\": 0.1277557611465454, \"time-step\": 1191}, {\"accuracy\": 0.5, \"loss\": 0.12774910032749176, \"time-step\": 1192}, {\"accuracy\": 0.5, \"loss\": 0.12774255871772766, \"time-step\": 1193}, {\"accuracy\": 0.5, \"loss\": 0.12773600220680237, \"time-step\": 1194}, {\"accuracy\": 0.5, \"loss\": 0.1277294158935547, \"time-step\": 1195}, {\"accuracy\": 0.5, \"loss\": 0.1277228593826294, \"time-step\": 1196}, {\"accuracy\": 0.5, \"loss\": 0.12771636247634888, \"time-step\": 1197}, {\"accuracy\": 0.5, \"loss\": 0.12770985066890717, \"time-step\": 1198}, {\"accuracy\": 0.5, \"loss\": 0.12770342826843262, \"time-step\": 1199}, {\"accuracy\": 0.5, \"loss\": 0.1276969611644745, \"time-step\": 1200}, {\"accuracy\": 0.5, \"loss\": 0.12769052386283875, \"time-step\": 1201}, {\"accuracy\": 0.5, \"loss\": 0.12768414616584778, \"time-step\": 1202}, {\"accuracy\": 0.5, \"loss\": 0.12767775356769562, \"time-step\": 1203}, {\"accuracy\": 0.5, \"loss\": 0.12767139077186584, \"time-step\": 1204}, {\"accuracy\": 0.5, \"loss\": 0.12766510248184204, \"time-step\": 1205}, {\"accuracy\": 0.5, \"loss\": 0.12765870988368988, \"time-step\": 1206}, {\"accuracy\": 0.5, \"loss\": 0.12765245139598846, \"time-step\": 1207}, {\"accuracy\": 0.5, \"loss\": 0.12764610350131989, \"time-step\": 1208}, {\"accuracy\": 0.5, \"loss\": 0.12763988971710205, \"time-step\": 1209}, {\"accuracy\": 0.5, \"loss\": 0.12763357162475586, \"time-step\": 1210}, {\"accuracy\": 0.5, \"loss\": 0.1276274025440216, \"time-step\": 1211}, {\"accuracy\": 0.5, \"loss\": 0.12762117385864258, \"time-step\": 1212}, {\"accuracy\": 0.5, \"loss\": 0.12761500477790833, \"time-step\": 1213}, {\"accuracy\": 0.5, \"loss\": 0.12760880589485168, \"time-step\": 1214}, {\"accuracy\": 0.5, \"loss\": 0.12760265171527863, \"time-step\": 1215}, {\"accuracy\": 0.5, \"loss\": 0.12759654223918915, \"time-step\": 1216}, {\"accuracy\": 0.5, \"loss\": 0.12759052217006683, \"time-step\": 1217}, {\"accuracy\": 0.5, \"loss\": 0.12758445739746094, \"time-step\": 1218}, {\"accuracy\": 0.5, \"loss\": 0.12757831811904907, \"time-step\": 1219}, {\"accuracy\": 0.5, \"loss\": 0.12757228314876556, \"time-step\": 1220}, {\"accuracy\": 0.5, \"loss\": 0.12756624817848206, \"time-step\": 1221}, {\"accuracy\": 0.5, \"loss\": 0.12756025791168213, \"time-step\": 1222}, {\"accuracy\": 0.5, \"loss\": 0.1275542825460434, \"time-step\": 1223}, {\"accuracy\": 0.5, \"loss\": 0.12754836678504944, \"time-step\": 1224}, {\"accuracy\": 0.5, \"loss\": 0.12754236161708832, \"time-step\": 1225}, {\"accuracy\": 0.5, \"loss\": 0.12753646075725555, \"time-step\": 1226}, {\"accuracy\": 0.5, \"loss\": 0.1275305300951004, \"time-step\": 1227}, {\"accuracy\": 0.5, \"loss\": 0.12752458453178406, \"time-step\": 1228}, {\"accuracy\": 0.5, \"loss\": 0.12751884758472443, \"time-step\": 1229}, {\"accuracy\": 0.5, \"loss\": 0.12751296162605286, \"time-step\": 1230}, {\"accuracy\": 0.5, \"loss\": 0.12750715017318726, \"time-step\": 1231}, {\"accuracy\": 0.5, \"loss\": 0.12750127911567688, \"time-step\": 1232}, {\"accuracy\": 0.5, \"loss\": 0.12749554216861725, \"time-step\": 1233}, {\"accuracy\": 0.5, \"loss\": 0.12748979032039642, \"time-step\": 1234}, {\"accuracy\": 0.5, \"loss\": 0.12748393416404724, \"time-step\": 1235}, {\"accuracy\": 0.5, \"loss\": 0.12747825682163239, \"time-step\": 1236}, {\"accuracy\": 0.5, \"loss\": 0.12747250497341156, \"time-step\": 1237}, {\"accuracy\": 0.5, \"loss\": 0.1274668574333191, \"time-step\": 1238}, {\"accuracy\": 0.5, \"loss\": 0.12746115028858185, \"time-step\": 1239}, {\"accuracy\": 0.5, \"loss\": 0.1274554580450058, \"time-step\": 1240}, {\"accuracy\": 0.5, \"loss\": 0.12744981050491333, \"time-step\": 1241}, {\"accuracy\": 0.5, \"loss\": 0.12744417786598206, \"time-step\": 1242}, {\"accuracy\": 0.5, \"loss\": 0.12743857502937317, \"time-step\": 1243}, {\"accuracy\": 0.5, \"loss\": 0.12743297219276428, \"time-step\": 1244}, {\"accuracy\": 0.5, \"loss\": 0.12742742896080017, \"time-step\": 1245}, {\"accuracy\": 0.5, \"loss\": 0.1274217665195465, \"time-step\": 1246}, {\"accuracy\": 0.5, \"loss\": 0.12741628289222717, \"time-step\": 1247}, {\"accuracy\": 0.5, \"loss\": 0.12741078436374664, \"time-step\": 1248}, {\"accuracy\": 0.5, \"loss\": 0.12740525603294373, \"time-step\": 1249}, {\"accuracy\": 0.5, \"loss\": 0.1273997277021408, \"time-step\": 1250}, {\"accuracy\": 0.5, \"loss\": 0.12739437818527222, \"time-step\": 1251}, {\"accuracy\": 0.5, \"loss\": 0.1273888498544693, \"time-step\": 1252}, {\"accuracy\": 0.5, \"loss\": 0.12738338112831116, \"time-step\": 1253}, {\"accuracy\": 0.5, \"loss\": 0.12737798690795898, \"time-step\": 1254}, {\"accuracy\": 0.5, \"loss\": 0.1273725926876068, \"time-step\": 1255}, {\"accuracy\": 0.5, \"loss\": 0.12736724317073822, \"time-step\": 1256}, {\"accuracy\": 0.5, \"loss\": 0.12736180424690247, \"time-step\": 1257}, {\"accuracy\": 0.5, \"loss\": 0.12735649943351746, \"time-step\": 1258}, {\"accuracy\": 0.5, \"loss\": 0.12735116481781006, \"time-step\": 1259}, {\"accuracy\": 0.5, \"loss\": 0.12734581530094147, \"time-step\": 1260}, {\"accuracy\": 0.5, \"loss\": 0.12734054028987885, \"time-step\": 1261}, {\"accuracy\": 0.5, \"loss\": 0.12733528017997742, \"time-step\": 1262}, {\"accuracy\": 0.5, \"loss\": 0.1273299753665924, \"time-step\": 1263}, {\"accuracy\": 0.5, \"loss\": 0.12732474505901337, \"time-step\": 1264}, {\"accuracy\": 0.5, \"loss\": 0.12731945514678955, \"time-step\": 1265}, {\"accuracy\": 0.5, \"loss\": 0.12731429934501648, \"time-step\": 1266}, {\"accuracy\": 0.5, \"loss\": 0.12730905413627625, \"time-step\": 1267}, {\"accuracy\": 0.5, \"loss\": 0.1273038536310196, \"time-step\": 1268}, {\"accuracy\": 0.5, \"loss\": 0.12729868292808533, \"time-step\": 1269}, {\"accuracy\": 0.5, \"loss\": 0.12729357182979584, \"time-step\": 1270}, {\"accuracy\": 0.5, \"loss\": 0.12728846073150635, \"time-step\": 1271}, {\"accuracy\": 0.5, \"loss\": 0.1272832155227661, \"time-step\": 1272}, {\"accuracy\": 0.5, \"loss\": 0.1272781789302826, \"time-step\": 1273}, {\"accuracy\": 0.5, \"loss\": 0.12727299332618713, \"time-step\": 1274}, {\"accuracy\": 0.5, \"loss\": 0.1272680163383484, \"time-step\": 1275}, {\"accuracy\": 0.5, \"loss\": 0.1272629201412201, \"time-step\": 1276}, {\"accuracy\": 0.5, \"loss\": 0.12725785374641418, \"time-step\": 1277}, {\"accuracy\": 0.5, \"loss\": 0.12725283205509186, \"time-step\": 1278}, {\"accuracy\": 0.5, \"loss\": 0.12724781036376953, \"time-step\": 1279}, {\"accuracy\": 0.5, \"loss\": 0.1272428333759308, \"time-step\": 1280}, {\"accuracy\": 0.5, \"loss\": 0.12723782658576965, \"time-step\": 1281}, {\"accuracy\": 0.5, \"loss\": 0.12723292410373688, \"time-step\": 1282}, {\"accuracy\": 0.5, \"loss\": 0.12722794711589813, \"time-step\": 1283}, {\"accuracy\": 0.5, \"loss\": 0.12722304463386536, \"time-step\": 1284}, {\"accuracy\": 0.5, \"loss\": 0.1272181123495102, \"time-step\": 1285}, {\"accuracy\": 0.5, \"loss\": 0.12721315026283264, \"time-step\": 1286}, {\"accuracy\": 0.5, \"loss\": 0.12720832228660583, \"time-step\": 1287}, {\"accuracy\": 0.5, \"loss\": 0.12720343470573425, \"time-step\": 1288}, {\"accuracy\": 0.5, \"loss\": 0.1271984726190567, \"time-step\": 1289}, {\"accuracy\": 0.5, \"loss\": 0.12719368934631348, \"time-step\": 1290}, {\"accuracy\": 0.5, \"loss\": 0.12718886137008667, \"time-step\": 1291}, {\"accuracy\": 0.5, \"loss\": 0.12718400359153748, \"time-step\": 1292}, {\"accuracy\": 0.5, \"loss\": 0.12717929482460022, \"time-step\": 1293}, {\"accuracy\": 0.5, \"loss\": 0.12717443704605103, \"time-step\": 1294}, {\"accuracy\": 0.5, \"loss\": 0.127169668674469, \"time-step\": 1295}, {\"accuracy\": 0.5, \"loss\": 0.12716495990753174, \"time-step\": 1296}, {\"accuracy\": 0.5, \"loss\": 0.1271601766347885, \"time-step\": 1297}, {\"accuracy\": 0.5, \"loss\": 0.12715543806552887, \"time-step\": 1298}, {\"accuracy\": 0.5, \"loss\": 0.12715071439743042, \"time-step\": 1299}, {\"accuracy\": 0.5, \"loss\": 0.12714606523513794, \"time-step\": 1300}, {\"accuracy\": 0.5, \"loss\": 0.12714137136936188, \"time-step\": 1301}, {\"accuracy\": 0.5, \"loss\": 0.12713667750358582, \"time-step\": 1302}, {\"accuracy\": 0.5, \"loss\": 0.12713204324245453, \"time-step\": 1303}, {\"accuracy\": 0.5, \"loss\": 0.12712734937667847, \"time-step\": 1304}, {\"accuracy\": 0.5, \"loss\": 0.1271226704120636, \"time-step\": 1305}, {\"accuracy\": 0.5, \"loss\": 0.12711815536022186, \"time-step\": 1306}, {\"accuracy\": 0.5, \"loss\": 0.12711352109909058, \"time-step\": 1307}, {\"accuracy\": 0.5, \"loss\": 0.12710893154144287, \"time-step\": 1308}, {\"accuracy\": 0.5, \"loss\": 0.12710432708263397, \"time-step\": 1309}, {\"accuracy\": 0.5, \"loss\": 0.12709981203079224, \"time-step\": 1310}, {\"accuracy\": 0.5, \"loss\": 0.1270953118801117, \"time-step\": 1311}, {\"accuracy\": 0.5, \"loss\": 0.127090722322464, \"time-step\": 1312}, {\"accuracy\": 0.5, \"loss\": 0.12708622217178345, \"time-step\": 1313}, {\"accuracy\": 0.5, \"loss\": 0.12708169221878052, \"time-step\": 1314}, {\"accuracy\": 0.5, \"loss\": 0.12707719206809998, \"time-step\": 1315}, {\"accuracy\": 0.5, \"loss\": 0.12707269191741943, \"time-step\": 1316}, {\"accuracy\": 0.5, \"loss\": 0.12706822156906128, \"time-step\": 1317}, {\"accuracy\": 0.5, \"loss\": 0.1270637810230255, \"time-step\": 1318}, {\"accuracy\": 0.5, \"loss\": 0.12705932557582855, \"time-step\": 1319}, {\"accuracy\": 0.5, \"loss\": 0.12705492973327637, \"time-step\": 1320}, {\"accuracy\": 0.5, \"loss\": 0.127050518989563, \"time-step\": 1321}, {\"accuracy\": 0.5, \"loss\": 0.1270461082458496, \"time-step\": 1322}, {\"accuracy\": 0.5, \"loss\": 0.127041757106781, \"time-step\": 1323}, {\"accuracy\": 0.5, \"loss\": 0.1270374357700348, \"time-step\": 1324}, {\"accuracy\": 0.5, \"loss\": 0.12703299522399902, \"time-step\": 1325}, {\"accuracy\": 0.5, \"loss\": 0.1270286738872528, \"time-step\": 1326}, {\"accuracy\": 0.5, \"loss\": 0.12702436745166779, \"time-step\": 1327}, {\"accuracy\": 0.5, \"loss\": 0.1270199716091156, \"time-step\": 1328}, {\"accuracy\": 0.5, \"loss\": 0.12701573967933655, \"time-step\": 1329}, {\"accuracy\": 0.5, \"loss\": 0.12701144814491272, \"time-step\": 1330}, {\"accuracy\": 0.5, \"loss\": 0.1270071566104889, \"time-step\": 1331}, {\"accuracy\": 0.5, \"loss\": 0.12700292468070984, \"time-step\": 1332}, {\"accuracy\": 0.5, \"loss\": 0.12699860334396362, \"time-step\": 1333}, {\"accuracy\": 0.5, \"loss\": 0.12699438631534576, \"time-step\": 1334}, {\"accuracy\": 0.5, \"loss\": 0.12699013948440552, \"time-step\": 1335}, {\"accuracy\": 0.5, \"loss\": 0.12698595225811005, \"time-step\": 1336}, {\"accuracy\": 0.5, \"loss\": 0.1269817352294922, \"time-step\": 1337}, {\"accuracy\": 0.5, \"loss\": 0.1269775629043579, \"time-step\": 1338}, {\"accuracy\": 0.5, \"loss\": 0.12697339057922363, \"time-step\": 1339}, {\"accuracy\": 0.5, \"loss\": 0.12696923315525055, \"time-step\": 1340}, {\"accuracy\": 0.75, \"loss\": 0.12696512043476105, \"time-step\": 1341}, {\"accuracy\": 0.5, \"loss\": 0.12696108222007751, \"time-step\": 1342}, {\"accuracy\": 0.75, \"loss\": 0.1269569844007492, \"time-step\": 1343}, {\"accuracy\": 0.5, \"loss\": 0.1269528716802597, \"time-step\": 1344}, {\"accuracy\": 0.75, \"loss\": 0.12694886326789856, \"time-step\": 1345}, {\"accuracy\": 0.5, \"loss\": 0.12694492936134338, \"time-step\": 1346}, {\"accuracy\": 0.75, \"loss\": 0.12694084644317627, \"time-step\": 1347}, {\"accuracy\": 0.5, \"loss\": 0.1269369125366211, \"time-step\": 1348}, {\"accuracy\": 0.75, \"loss\": 0.12693294882774353, \"time-step\": 1349}, {\"accuracy\": 0.5, \"loss\": 0.12692908942699432, \"time-step\": 1350}, {\"accuracy\": 0.75, \"loss\": 0.12692520022392273, \"time-step\": 1351}, {\"accuracy\": 0.5, \"loss\": 0.12692132592201233, \"time-step\": 1352}, {\"accuracy\": 0.75, \"loss\": 0.12691742181777954, \"time-step\": 1353}, {\"accuracy\": 0.5, \"loss\": 0.1269136220216751, \"time-step\": 1354}, {\"accuracy\": 0.75, \"loss\": 0.1269097775220871, \"time-step\": 1355}, {\"accuracy\": 0.5, \"loss\": 0.12690594792366028, \"time-step\": 1356}, {\"accuracy\": 0.75, \"loss\": 0.12690207362174988, \"time-step\": 1357}, {\"accuracy\": 0.5, \"loss\": 0.1268983781337738, \"time-step\": 1358}, {\"accuracy\": 0.75, \"loss\": 0.12689454853534698, \"time-step\": 1359}, {\"accuracy\": 0.5, \"loss\": 0.1268908679485321, \"time-step\": 1360}, {\"accuracy\": 0.75, \"loss\": 0.1268870234489441, \"time-step\": 1361}, {\"accuracy\": 0.5, \"loss\": 0.12688331305980682, \"time-step\": 1362}, {\"accuracy\": 0.75, \"loss\": 0.1268795281648636, \"time-step\": 1363}, {\"accuracy\": 0.5, \"loss\": 0.12687578797340393, \"time-step\": 1364}, {\"accuracy\": 0.75, \"loss\": 0.12687212228775024, \"time-step\": 1365}, {\"accuracy\": 0.5, \"loss\": 0.1268683671951294, \"time-step\": 1366}, {\"accuracy\": 0.75, \"loss\": 0.1268647015094757, \"time-step\": 1367}, {\"accuracy\": 0.5, \"loss\": 0.12686091661453247, \"time-step\": 1368}, {\"accuracy\": 0.75, \"loss\": 0.1268572211265564, \"time-step\": 1369}, {\"accuracy\": 0.5, \"loss\": 0.12685361504554749, \"time-step\": 1370}, {\"accuracy\": 0.75, \"loss\": 0.12684999406337738, \"time-step\": 1371}, {\"accuracy\": 0.5, \"loss\": 0.1268463432788849, \"time-step\": 1372}, {\"accuracy\": 0.75, \"loss\": 0.12684275209903717, \"time-step\": 1373}, {\"accuracy\": 0.5, \"loss\": 0.1268390715122223, \"time-step\": 1374}, {\"accuracy\": 0.75, \"loss\": 0.12683549523353577, \"time-step\": 1375}, {\"accuracy\": 0.5, \"loss\": 0.12683185935020447, \"time-step\": 1376}, {\"accuracy\": 0.75, \"loss\": 0.12682829797267914, \"time-step\": 1377}, {\"accuracy\": 0.5, \"loss\": 0.126824751496315, \"time-step\": 1378}, {\"accuracy\": 0.75, \"loss\": 0.1268211305141449, \"time-step\": 1379}, {\"accuracy\": 0.5, \"loss\": 0.12681765854358673, \"time-step\": 1380}, {\"accuracy\": 0.75, \"loss\": 0.12681400775909424, \"time-step\": 1381}, {\"accuracy\": 0.5, \"loss\": 0.12681053578853607, \"time-step\": 1382}, {\"accuracy\": 0.75, \"loss\": 0.12680694460868835, \"time-step\": 1383}, {\"accuracy\": 0.5, \"loss\": 0.12680348753929138, \"time-step\": 1384}, {\"accuracy\": 0.75, \"loss\": 0.12679997086524963, \"time-step\": 1385}, {\"accuracy\": 0.5, \"loss\": 0.12679646909236908, \"time-step\": 1386}, {\"accuracy\": 0.75, \"loss\": 0.12679295241832733, \"time-step\": 1387}, {\"accuracy\": 0.5, \"loss\": 0.12678952515125275, \"time-step\": 1388}, {\"accuracy\": 0.75, \"loss\": 0.12678611278533936, \"time-step\": 1389}, {\"accuracy\": 0.5, \"loss\": 0.12678262591362, \"time-step\": 1390}, {\"accuracy\": 0.75, \"loss\": 0.1267792135477066, \"time-step\": 1391}, {\"accuracy\": 0.5, \"loss\": 0.12677575647830963, \"time-step\": 1392}, {\"accuracy\": 0.75, \"loss\": 0.12677231431007385, \"time-step\": 1393}, {\"accuracy\": 0.5, \"loss\": 0.12676891684532166, \"time-step\": 1394}, {\"accuracy\": 0.75, \"loss\": 0.12676557898521423, \"time-step\": 1395}, {\"accuracy\": 0.5, \"loss\": 0.12676218152046204, \"time-step\": 1396}, {\"accuracy\": 0.75, \"loss\": 0.12675879895687103, \"time-step\": 1397}, {\"accuracy\": 0.5, \"loss\": 0.12675537168979645, \"time-step\": 1398}, {\"accuracy\": 0.75, \"loss\": 0.1267520785331726, \"time-step\": 1399}, {\"accuracy\": 0.5, \"loss\": 0.12674874067306519, \"time-step\": 1400}, {\"accuracy\": 0.75, \"loss\": 0.12674543261528015, \"time-step\": 1401}, {\"accuracy\": 0.5, \"loss\": 0.1267421543598175, \"time-step\": 1402}, {\"accuracy\": 0.75, \"loss\": 0.1267387568950653, \"time-step\": 1403}, {\"accuracy\": 0.5, \"loss\": 0.12673552334308624, \"time-step\": 1404}, {\"accuracy\": 0.75, \"loss\": 0.12673215568065643, \"time-step\": 1405}, {\"accuracy\": 0.5, \"loss\": 0.1267288476228714, \"time-step\": 1406}, {\"accuracy\": 0.75, \"loss\": 0.12672559916973114, \"time-step\": 1407}, {\"accuracy\": 0.5, \"loss\": 0.12672239542007446, \"time-step\": 1408}, {\"accuracy\": 0.75, \"loss\": 0.12671911716461182, \"time-step\": 1409}, {\"accuracy\": 0.5, \"loss\": 0.12671589851379395, \"time-step\": 1410}, {\"accuracy\": 0.75, \"loss\": 0.1267126500606537, \"time-step\": 1411}, {\"accuracy\": 0.5, \"loss\": 0.12670938670635223, \"time-step\": 1412}, {\"accuracy\": 0.75, \"loss\": 0.12670619785785675, \"time-step\": 1413}, {\"accuracy\": 0.5, \"loss\": 0.12670300900936127, \"time-step\": 1414}, {\"accuracy\": 0.75, \"loss\": 0.12669983506202698, \"time-step\": 1415}, {\"accuracy\": 0.5, \"loss\": 0.1266966313123703, \"time-step\": 1416}, {\"accuracy\": 0.75, \"loss\": 0.1266934871673584, \"time-step\": 1417}, {\"accuracy\": 0.5, \"loss\": 0.12669029831886292, \"time-step\": 1418}, {\"accuracy\": 0.75, \"loss\": 0.12668712437152863, \"time-step\": 1419}, {\"accuracy\": 0.5, \"loss\": 0.12668408453464508, \"time-step\": 1420}, {\"accuracy\": 0.75, \"loss\": 0.12668085098266602, \"time-step\": 1421}, {\"accuracy\": 0.5, \"loss\": 0.12667778134346008, \"time-step\": 1422}, {\"accuracy\": 0.75, \"loss\": 0.12667463719844818, \"time-step\": 1423}, {\"accuracy\": 0.5, \"loss\": 0.12667152285575867, \"time-step\": 1424}, {\"accuracy\": 0.75, \"loss\": 0.12666839361190796, \"time-step\": 1425}, {\"accuracy\": 0.5, \"loss\": 0.12666533887386322, \"time-step\": 1426}, {\"accuracy\": 0.75, \"loss\": 0.1266622543334961, \"time-step\": 1427}, {\"accuracy\": 0.5, \"loss\": 0.12665919959545135, \"time-step\": 1428}, {\"accuracy\": 0.75, \"loss\": 0.12665612995624542, \"time-step\": 1429}, {\"accuracy\": 0.5, \"loss\": 0.12665307521820068, \"time-step\": 1430}, {\"accuracy\": 0.75, \"loss\": 0.12665003538131714, \"time-step\": 1431}, {\"accuracy\": 0.5, \"loss\": 0.12664705514907837, \"time-step\": 1432}, {\"accuracy\": 0.75, \"loss\": 0.12664398550987244, \"time-step\": 1433}, {\"accuracy\": 0.5, \"loss\": 0.12664100527763367, \"time-step\": 1434}, {\"accuracy\": 0.75, \"loss\": 0.12663798034191132, \"time-step\": 1435}, {\"accuracy\": 0.5, \"loss\": 0.12663497030735016, \"time-step\": 1436}, {\"accuracy\": 0.75, \"loss\": 0.1266319751739502, \"time-step\": 1437}, {\"accuracy\": 0.5, \"loss\": 0.1266290545463562, \"time-step\": 1438}, {\"accuracy\": 0.75, \"loss\": 0.12662601470947266, \"time-step\": 1439}, {\"accuracy\": 0.5, \"loss\": 0.12662309408187866, \"time-step\": 1440}, {\"accuracy\": 0.75, \"loss\": 0.1266201287508011, \"time-step\": 1441}, {\"accuracy\": 0.5, \"loss\": 0.12661711871623993, \"time-step\": 1442}, {\"accuracy\": 0.75, \"loss\": 0.1266142874956131, \"time-step\": 1443}, {\"accuracy\": 0.5, \"loss\": 0.12661133706569672, \"time-step\": 1444}, {\"accuracy\": 0.75, \"loss\": 0.12660840153694153, \"time-step\": 1445}, {\"accuracy\": 0.5, \"loss\": 0.12660543620586395, \"time-step\": 1446}, {\"accuracy\": 0.75, \"loss\": 0.12660261988639832, \"time-step\": 1447}, {\"accuracy\": 0.5, \"loss\": 0.12659969925880432, \"time-step\": 1448}, {\"accuracy\": 0.75, \"loss\": 0.1265968233346939, \"time-step\": 1449}, {\"accuracy\": 0.5, \"loss\": 0.1265939474105835, \"time-step\": 1450}, {\"accuracy\": 0.75, \"loss\": 0.1265910565853119, \"time-step\": 1451}, {\"accuracy\": 0.5, \"loss\": 0.12658819556236267, \"time-step\": 1452}, {\"accuracy\": 0.75, \"loss\": 0.12658533453941345, \"time-step\": 1453}, {\"accuracy\": 0.5, \"loss\": 0.12658251821994781, \"time-step\": 1454}, {\"accuracy\": 0.75, \"loss\": 0.1265796720981598, \"time-step\": 1455}, {\"accuracy\": 0.5, \"loss\": 0.12657687067985535, \"time-step\": 1456}, {\"accuracy\": 0.75, \"loss\": 0.12657402455806732, \"time-step\": 1457}, {\"accuracy\": 0.5, \"loss\": 0.12657122313976288, \"time-step\": 1458}, {\"accuracy\": 0.75, \"loss\": 0.12656845152378082, \"time-step\": 1459}, {\"accuracy\": 0.5, \"loss\": 0.1265656054019928, \"time-step\": 1460}, {\"accuracy\": 0.75, \"loss\": 0.12656283378601074, \"time-step\": 1461}, {\"accuracy\": 0.5, \"loss\": 0.1265600621700287, \"time-step\": 1462}, {\"accuracy\": 0.75, \"loss\": 0.12655729055404663, \"time-step\": 1463}, {\"accuracy\": 0.5, \"loss\": 0.12655454874038696, \"time-step\": 1464}, {\"accuracy\": 0.75, \"loss\": 0.1265517771244049, \"time-step\": 1465}, {\"accuracy\": 0.5, \"loss\": 0.12654902040958405, \"time-step\": 1466}, {\"accuracy\": 0.75, \"loss\": 0.12654635310173035, \"time-step\": 1467}, {\"accuracy\": 0.5, \"loss\": 0.12654350697994232, \"time-step\": 1468}, {\"accuracy\": 0.75, \"loss\": 0.1265408843755722, \"time-step\": 1469}, {\"accuracy\": 0.5, \"loss\": 0.12653811275959015, \"time-step\": 1470}, {\"accuracy\": 0.75, \"loss\": 0.12653543055057526, \"time-step\": 1471}, {\"accuracy\": 0.5, \"loss\": 0.1265326589345932, \"time-step\": 1472}, {\"accuracy\": 0.75, \"loss\": 0.1265300214290619, \"time-step\": 1473}, {\"accuracy\": 0.5, \"loss\": 0.12652744352817535, \"time-step\": 1474}, {\"accuracy\": 0.75, \"loss\": 0.1265246719121933, \"time-step\": 1475}, {\"accuracy\": 0.5, \"loss\": 0.126522034406662, \"time-step\": 1476}, {\"accuracy\": 0.75, \"loss\": 0.1265193521976471, \"time-step\": 1477}, {\"accuracy\": 0.5, \"loss\": 0.1265166997909546, \"time-step\": 1478}, {\"accuracy\": 0.75, \"loss\": 0.12651407718658447, \"time-step\": 1479}, {\"accuracy\": 0.5, \"loss\": 0.12651142477989197, \"time-step\": 1480}, {\"accuracy\": 0.75, \"loss\": 0.12650878727436066, \"time-step\": 1481}, {\"accuracy\": 0.5, \"loss\": 0.12650609016418457, \"time-step\": 1482}, {\"accuracy\": 0.75, \"loss\": 0.1265035718679428, \"time-step\": 1483}, {\"accuracy\": 0.5, \"loss\": 0.1265009343624115, \"time-step\": 1484}, {\"accuracy\": 0.75, \"loss\": 0.12649837136268616, \"time-step\": 1485}, {\"accuracy\": 0.5, \"loss\": 0.12649573385715485, \"time-step\": 1486}, {\"accuracy\": 0.75, \"loss\": 0.12649323046207428, \"time-step\": 1487}, {\"accuracy\": 0.5, \"loss\": 0.12649062275886536, \"time-step\": 1488}, {\"accuracy\": 0.75, \"loss\": 0.12648802995681763, \"time-step\": 1489}, {\"accuracy\": 0.5, \"loss\": 0.12648539245128632, \"time-step\": 1490}, {\"accuracy\": 0.75, \"loss\": 0.12648293375968933, \"time-step\": 1491}, {\"accuracy\": 0.5, \"loss\": 0.1264803111553192, \"time-step\": 1492}, {\"accuracy\": 0.75, \"loss\": 0.12647783756256104, \"time-step\": 1493}, {\"accuracy\": 0.5, \"loss\": 0.1264752745628357, \"time-step\": 1494}, {\"accuracy\": 0.75, \"loss\": 0.12647278606891632, \"time-step\": 1495}, {\"accuracy\": 0.5, \"loss\": 0.12647023797035217, \"time-step\": 1496}, {\"accuracy\": 0.75, \"loss\": 0.1264677494764328, \"time-step\": 1497}, {\"accuracy\": 0.5, \"loss\": 0.12646523118019104, \"time-step\": 1498}, {\"accuracy\": 0.75, \"loss\": 0.1264626830816269, \"time-step\": 1499}, {\"accuracy\": 0.5, \"loss\": 0.1264602243900299, \"time-step\": 1500}, {\"accuracy\": 0.75, \"loss\": 0.12645776569843292, \"time-step\": 1501}, {\"accuracy\": 0.5, \"loss\": 0.12645524740219116, \"time-step\": 1502}, {\"accuracy\": 0.75, \"loss\": 0.12645281851291656, \"time-step\": 1503}, {\"accuracy\": 0.5, \"loss\": 0.126450315117836, \"time-step\": 1504}, {\"accuracy\": 0.75, \"loss\": 0.126447856426239, \"time-step\": 1505}, {\"accuracy\": 0.5, \"loss\": 0.1264454424381256, \"time-step\": 1506}, {\"accuracy\": 0.75, \"loss\": 0.126443013548851, \"time-step\": 1507}, {\"accuracy\": 0.5, \"loss\": 0.12644058465957642, \"time-step\": 1508}, {\"accuracy\": 0.75, \"loss\": 0.12643814086914062, \"time-step\": 1509}, {\"accuracy\": 0.5, \"loss\": 0.12643565237522125, \"time-step\": 1510}, {\"accuracy\": 0.75, \"loss\": 0.12643328309059143, \"time-step\": 1511}, {\"accuracy\": 0.5, \"loss\": 0.12643088400363922, \"time-step\": 1512}, {\"accuracy\": 0.75, \"loss\": 0.126428484916687, \"time-step\": 1513}, {\"accuracy\": 0.5, \"loss\": 0.12642605602741241, \"time-step\": 1514}, {\"accuracy\": 0.75, \"loss\": 0.1264236867427826, \"time-step\": 1515}, {\"accuracy\": 0.5, \"loss\": 0.12642130255699158, \"time-step\": 1516}, {\"accuracy\": 0.75, \"loss\": 0.12641896307468414, \"time-step\": 1517}, {\"accuracy\": 0.5, \"loss\": 0.12641654908657074, \"time-step\": 1518}, {\"accuracy\": 0.75, \"loss\": 0.1264142096042633, \"time-step\": 1519}, {\"accuracy\": 0.5, \"loss\": 0.1264118254184723, \"time-step\": 1520}, {\"accuracy\": 0.75, \"loss\": 0.12640950083732605, \"time-step\": 1521}, {\"accuracy\": 0.5, \"loss\": 0.12640713155269623, \"time-step\": 1522}, {\"accuracy\": 0.75, \"loss\": 0.12640482187271118, \"time-step\": 1523}, {\"accuracy\": 0.5, \"loss\": 0.12640249729156494, \"time-step\": 1524}, {\"accuracy\": 0.75, \"loss\": 0.1264001876115799, \"time-step\": 1525}, {\"accuracy\": 0.5, \"loss\": 0.12639787793159485, \"time-step\": 1526}, {\"accuracy\": 0.75, \"loss\": 0.1263955533504486, \"time-step\": 1527}, {\"accuracy\": 0.5, \"loss\": 0.12639325857162476, \"time-step\": 1528}, {\"accuracy\": 0.75, \"loss\": 0.12639100849628448, \"time-step\": 1529}, {\"accuracy\": 0.5, \"loss\": 0.12638865411281586, \"time-step\": 1530}, {\"accuracy\": 0.75, \"loss\": 0.12638640403747559, \"time-step\": 1531}, {\"accuracy\": 0.5, \"loss\": 0.12638407945632935, \"time-step\": 1532}, {\"accuracy\": 0.75, \"loss\": 0.12638182938098907, \"time-step\": 1533}, {\"accuracy\": 0.5, \"loss\": 0.12637948989868164, \"time-step\": 1534}, {\"accuracy\": 0.75, \"loss\": 0.12637731432914734, \"time-step\": 1535}, {\"accuracy\": 0.5, \"loss\": 0.12637504935264587, \"time-step\": 1536}, {\"accuracy\": 0.75, \"loss\": 0.12637288868427277, \"time-step\": 1537}, {\"accuracy\": 0.5, \"loss\": 0.12637051939964294, \"time-step\": 1538}, {\"accuracy\": 0.75, \"loss\": 0.12636832892894745, \"time-step\": 1539}, {\"accuracy\": 0.5, \"loss\": 0.12636613845825195, \"time-step\": 1540}, {\"accuracy\": 0.75, \"loss\": 0.12636391818523407, \"time-step\": 1541}, {\"accuracy\": 0.5, \"loss\": 0.1263616383075714, \"time-step\": 1542}, {\"accuracy\": 0.75, \"loss\": 0.1263595074415207, \"time-step\": 1543}, {\"accuracy\": 0.5, \"loss\": 0.12635724246501923, \"time-step\": 1544}, {\"accuracy\": 0.75, \"loss\": 0.1263550966978073, \"time-step\": 1545}, {\"accuracy\": 0.5, \"loss\": 0.12635281682014465, \"time-step\": 1546}, {\"accuracy\": 0.75, \"loss\": 0.12635071575641632, \"time-step\": 1547}, {\"accuracy\": 0.5, \"loss\": 0.12634849548339844, \"time-step\": 1548}, {\"accuracy\": 0.75, \"loss\": 0.12634634971618652, \"time-step\": 1549}, {\"accuracy\": 0.5, \"loss\": 0.12634414434432983, \"time-step\": 1550}, {\"accuracy\": 0.75, \"loss\": 0.12634208798408508, \"time-step\": 1551}, {\"accuracy\": 0.5, \"loss\": 0.1263398677110672, \"time-step\": 1552}, {\"accuracy\": 0.75, \"loss\": 0.12633776664733887, \"time-step\": 1553}, {\"accuracy\": 0.5, \"loss\": 0.12633559107780457, \"time-step\": 1554}, {\"accuracy\": 0.75, \"loss\": 0.12633347511291504, \"time-step\": 1555}, {\"accuracy\": 0.5, \"loss\": 0.12633129954338074, \"time-step\": 1556}, {\"accuracy\": 0.75, \"loss\": 0.126329243183136, \"time-step\": 1557}, {\"accuracy\": 0.5, \"loss\": 0.12632706761360168, \"time-step\": 1558}, {\"accuracy\": 0.75, \"loss\": 0.12632495164871216, \"time-step\": 1559}, {\"accuracy\": 0.5, \"loss\": 0.12632280588150024, \"time-step\": 1560}, {\"accuracy\": 0.75, \"loss\": 0.1263207346200943, \"time-step\": 1561}, {\"accuracy\": 0.5, \"loss\": 0.12631860375404358, \"time-step\": 1562}, {\"accuracy\": 0.75, \"loss\": 0.126316636800766, \"time-step\": 1563}, {\"accuracy\": 0.5, \"loss\": 0.1263144463300705, \"time-step\": 1564}, {\"accuracy\": 0.75, \"loss\": 0.1263124793767929, \"time-step\": 1565}, {\"accuracy\": 0.5, \"loss\": 0.12631025910377502, \"time-step\": 1566}, {\"accuracy\": 0.75, \"loss\": 0.12630830705165863, \"time-step\": 1567}, {\"accuracy\": 0.5, \"loss\": 0.1263061761856079, \"time-step\": 1568}, {\"accuracy\": 0.75, \"loss\": 0.12630417943000793, \"time-step\": 1569}, {\"accuracy\": 0.5, \"loss\": 0.1263020634651184, \"time-step\": 1570}, {\"accuracy\": 0.75, \"loss\": 0.12630006670951843, \"time-step\": 1571}, {\"accuracy\": 0.5, \"loss\": 0.12629801034927368, \"time-step\": 1572}, {\"accuracy\": 0.75, \"loss\": 0.1262960433959961, \"time-step\": 1573}, {\"accuracy\": 0.5, \"loss\": 0.12629394233226776, \"time-step\": 1574}, {\"accuracy\": 0.75, \"loss\": 0.12629204988479614, \"time-step\": 1575}, {\"accuracy\": 0.5, \"loss\": 0.12628987431526184, \"time-step\": 1576}, {\"accuracy\": 0.75, \"loss\": 0.126288041472435, \"time-step\": 1577}, {\"accuracy\": 0.5, \"loss\": 0.12628589570522308, \"time-step\": 1578}, {\"accuracy\": 0.75, \"loss\": 0.12628398835659027, \"time-step\": 1579}, {\"accuracy\": 0.5, \"loss\": 0.12628190219402313, \"time-step\": 1580}, {\"accuracy\": 0.75, \"loss\": 0.1262800246477127, \"time-step\": 1581}, {\"accuracy\": 0.5, \"loss\": 0.12627799808979034, \"time-step\": 1582}, {\"accuracy\": 0.75, \"loss\": 0.12627606093883514, \"time-step\": 1583}, {\"accuracy\": 0.5, \"loss\": 0.1262740194797516, \"time-step\": 1584}, {\"accuracy\": 0.75, \"loss\": 0.1262720376253128, \"time-step\": 1585}, {\"accuracy\": 0.5, \"loss\": 0.12627002596855164, \"time-step\": 1586}, {\"accuracy\": 0.75, \"loss\": 0.1262681931257248, \"time-step\": 1587}, {\"accuracy\": 0.5, \"loss\": 0.12626619637012482, \"time-step\": 1588}, {\"accuracy\": 0.75, \"loss\": 0.1262642741203308, \"time-step\": 1589}, {\"accuracy\": 0.5, \"loss\": 0.12626224756240845, \"time-step\": 1590}, {\"accuracy\": 0.75, \"loss\": 0.1262604296207428, \"time-step\": 1591}, {\"accuracy\": 0.5, \"loss\": 0.12625843286514282, \"time-step\": 1592}, {\"accuracy\": 0.75, \"loss\": 0.1262565553188324, \"time-step\": 1593}, {\"accuracy\": 0.5, \"loss\": 0.1262545883655548, \"time-step\": 1594}, {\"accuracy\": 0.75, \"loss\": 0.12625271081924438, \"time-step\": 1595}, {\"accuracy\": 0.5, \"loss\": 0.1262507140636444, \"time-step\": 1596}, {\"accuracy\": 0.75, \"loss\": 0.12624894082546234, \"time-step\": 1597}, {\"accuracy\": 0.5, \"loss\": 0.12624695897102356, \"time-step\": 1598}, {\"accuracy\": 0.75, \"loss\": 0.1262451559305191, \"time-step\": 1599}, {\"accuracy\": 0.5, \"loss\": 0.1262432038784027, \"time-step\": 1600}, {\"accuracy\": 0.75, \"loss\": 0.12624141573905945, \"time-step\": 1601}, {\"accuracy\": 0.5, \"loss\": 0.12623943388462067, \"time-step\": 1602}, {\"accuracy\": 0.75, \"loss\": 0.126237690448761, \"time-step\": 1603}, {\"accuracy\": 0.5, \"loss\": 0.12623567879199982, \"time-step\": 1604}, {\"accuracy\": 0.75, \"loss\": 0.12623390555381775, \"time-step\": 1605}, {\"accuracy\": 0.75, \"loss\": 0.12623199820518494, \"time-step\": 1606}, {\"accuracy\": 0.75, \"loss\": 0.12623022496700287, \"time-step\": 1607}, {\"accuracy\": 0.75, \"loss\": 0.12622830271720886, \"time-step\": 1608}, {\"accuracy\": 0.75, \"loss\": 0.1262265294790268, \"time-step\": 1609}, {\"accuracy\": 0.75, \"loss\": 0.1262245923280716, \"time-step\": 1610}, {\"accuracy\": 0.75, \"loss\": 0.1262228637933731, \"time-step\": 1611}, {\"accuracy\": 0.75, \"loss\": 0.1262209415435791, \"time-step\": 1612}, {\"accuracy\": 0.75, \"loss\": 0.1262192577123642, \"time-step\": 1613}, {\"accuracy\": 0.75, \"loss\": 0.12621727585792542, \"time-step\": 1614}, {\"accuracy\": 0.75, \"loss\": 0.1262156218290329, \"time-step\": 1615}, {\"accuracy\": 0.75, \"loss\": 0.12621375918388367, \"time-step\": 1616}, {\"accuracy\": 0.75, \"loss\": 0.12621204555034637, \"time-step\": 1617}, {\"accuracy\": 0.75, \"loss\": 0.12621012330055237, \"time-step\": 1618}, {\"accuracy\": 0.75, \"loss\": 0.12620843946933746, \"time-step\": 1619}, {\"accuracy\": 0.75, \"loss\": 0.12620654702186584, \"time-step\": 1620}, {\"accuracy\": 0.75, \"loss\": 0.12620486319065094, \"time-step\": 1621}, {\"accuracy\": 0.75, \"loss\": 0.12620295584201813, \"time-step\": 1622}, {\"accuracy\": 0.75, \"loss\": 0.126201331615448, \"time-step\": 1623}, {\"accuracy\": 0.75, \"loss\": 0.12619943916797638, \"time-step\": 1624}, {\"accuracy\": 0.75, \"loss\": 0.12619781494140625, \"time-step\": 1625}, {\"accuracy\": 0.75, \"loss\": 0.12619590759277344, \"time-step\": 1626}, {\"accuracy\": 0.75, \"loss\": 0.12619437277317047, \"time-step\": 1627}, {\"accuracy\": 0.75, \"loss\": 0.1261923909187317, \"time-step\": 1628}, {\"accuracy\": 0.75, \"loss\": 0.12619079649448395, \"time-step\": 1629}, {\"accuracy\": 0.75, \"loss\": 0.1261889934539795, \"time-step\": 1630}, {\"accuracy\": 0.75, \"loss\": 0.12618733942508698, \"time-step\": 1631}, {\"accuracy\": 0.75, \"loss\": 0.12618552148342133, \"time-step\": 1632}, {\"accuracy\": 0.75, \"loss\": 0.12618388235569, \"time-step\": 1633}, {\"accuracy\": 0.75, \"loss\": 0.12618204951286316, \"time-step\": 1634}, {\"accuracy\": 0.75, \"loss\": 0.1261804699897766, \"time-step\": 1635}, {\"accuracy\": 0.75, \"loss\": 0.1261785924434662, \"time-step\": 1636}, {\"accuracy\": 0.75, \"loss\": 0.1261770874261856, \"time-step\": 1637}, {\"accuracy\": 0.75, \"loss\": 0.126175194978714, \"time-step\": 1638}, {\"accuracy\": 0.75, \"loss\": 0.12617366015911102, \"time-step\": 1639}, {\"accuracy\": 0.75, \"loss\": 0.12617185711860657, \"time-step\": 1640}, {\"accuracy\": 0.75, \"loss\": 0.1261703222990036, \"time-step\": 1641}, {\"accuracy\": 0.75, \"loss\": 0.12616848945617676, \"time-step\": 1642}, {\"accuracy\": 0.75, \"loss\": 0.12616696953773499, \"time-step\": 1643}, {\"accuracy\": 0.75, \"loss\": 0.12616510689258575, \"time-step\": 1644}, {\"accuracy\": 0.75, \"loss\": 0.12616364657878876, \"time-step\": 1645}, {\"accuracy\": 0.75, \"loss\": 0.1261618435382843, \"time-step\": 1646}, {\"accuracy\": 0.75, \"loss\": 0.12616030871868134, \"time-step\": 1647}, {\"accuracy\": 0.75, \"loss\": 0.12615849077701569, \"time-step\": 1648}, {\"accuracy\": 0.75, \"loss\": 0.1261570155620575, \"time-step\": 1649}, {\"accuracy\": 0.75, \"loss\": 0.12615522742271423, \"time-step\": 1650}, {\"accuracy\": 0.75, \"loss\": 0.12615373730659485, \"time-step\": 1651}, {\"accuracy\": 0.75, \"loss\": 0.1261519342660904, \"time-step\": 1652}, {\"accuracy\": 0.75, \"loss\": 0.1261504888534546, \"time-step\": 1653}, {\"accuracy\": 0.75, \"loss\": 0.12614870071411133, \"time-step\": 1654}, {\"accuracy\": 0.75, \"loss\": 0.12614721059799194, \"time-step\": 1655}, {\"accuracy\": 0.75, \"loss\": 0.12614549696445465, \"time-step\": 1656}, {\"accuracy\": 0.75, \"loss\": 0.12614396214485168, \"time-step\": 1657}, {\"accuracy\": 0.75, \"loss\": 0.12614218890666962, \"time-step\": 1658}, {\"accuracy\": 0.75, \"loss\": 0.1261408030986786, \"time-step\": 1659}, {\"accuracy\": 0.75, \"loss\": 0.12613901495933533, \"time-step\": 1660}, {\"accuracy\": 0.75, \"loss\": 0.1261376142501831, \"time-step\": 1661}, {\"accuracy\": 0.75, \"loss\": 0.12613585591316223, \"time-step\": 1662}, {\"accuracy\": 0.75, \"loss\": 0.1261344850063324, \"time-step\": 1663}, {\"accuracy\": 0.75, \"loss\": 0.12613266706466675, \"time-step\": 1664}, {\"accuracy\": 0.75, \"loss\": 0.12613126635551453, \"time-step\": 1665}, {\"accuracy\": 0.75, \"loss\": 0.12612955272197723, \"time-step\": 1666}, {\"accuracy\": 0.75, \"loss\": 0.126128152012825, \"time-step\": 1667}, {\"accuracy\": 0.75, \"loss\": 0.1261264979839325, \"time-step\": 1668}, {\"accuracy\": 0.75, \"loss\": 0.1261250227689743, \"time-step\": 1669}, {\"accuracy\": 0.75, \"loss\": 0.12612321972846985, \"time-step\": 1670}, {\"accuracy\": 0.75, \"loss\": 0.12612195312976837, \"time-step\": 1671}, {\"accuracy\": 0.75, \"loss\": 0.12612015008926392, \"time-step\": 1672}, {\"accuracy\": 0.75, \"loss\": 0.12611883878707886, \"time-step\": 1673}, {\"accuracy\": 0.75, \"loss\": 0.12611711025238037, \"time-step\": 1674}, {\"accuracy\": 0.75, \"loss\": 0.12611576914787292, \"time-step\": 1675}, {\"accuracy\": 0.75, \"loss\": 0.12611407041549683, \"time-step\": 1676}, {\"accuracy\": 0.75, \"loss\": 0.1261126697063446, \"time-step\": 1677}, {\"accuracy\": 0.75, \"loss\": 0.12611103057861328, \"time-step\": 1678}, {\"accuracy\": 0.75, \"loss\": 0.12610970437526703, \"time-step\": 1679}, {\"accuracy\": 0.75, \"loss\": 0.12610802054405212, \"time-step\": 1680}, {\"accuracy\": 0.75, \"loss\": 0.1261066496372223, \"time-step\": 1681}, {\"accuracy\": 0.75, \"loss\": 0.126104936003685, \"time-step\": 1682}, {\"accuracy\": 0.75, \"loss\": 0.12610368430614471, \"time-step\": 1683}, {\"accuracy\": 0.75, \"loss\": 0.1261020004749298, \"time-step\": 1684}, {\"accuracy\": 0.75, \"loss\": 0.12610065937042236, \"time-step\": 1685}, {\"accuracy\": 0.75, \"loss\": 0.12609899044036865, \"time-step\": 1686}, {\"accuracy\": 0.75, \"loss\": 0.1260976642370224, \"time-step\": 1687}, {\"accuracy\": 0.75, \"loss\": 0.12609604001045227, \"time-step\": 1688}, {\"accuracy\": 0.75, \"loss\": 0.126094788312912, \"time-step\": 1689}, {\"accuracy\": 0.75, \"loss\": 0.1260930597782135, \"time-step\": 1690}, {\"accuracy\": 0.75, \"loss\": 0.126091867685318, \"time-step\": 1691}, {\"accuracy\": 0.75, \"loss\": 0.12609007954597473, \"time-step\": 1692}, {\"accuracy\": 0.75, \"loss\": 0.1260889619588852, \"time-step\": 1693}, {\"accuracy\": 0.75, \"loss\": 0.12608720362186432, \"time-step\": 1694}, {\"accuracy\": 0.75, \"loss\": 0.12608602643013, \"time-step\": 1695}, {\"accuracy\": 0.75, \"loss\": 0.1260843575000763, \"time-step\": 1696}, {\"accuracy\": 0.75, \"loss\": 0.1260831356048584, \"time-step\": 1697}, {\"accuracy\": 0.75, \"loss\": 0.1260814368724823, \"time-step\": 1698}, {\"accuracy\": 0.75, \"loss\": 0.12608025968074799, \"time-step\": 1699}, {\"accuracy\": 0.75, \"loss\": 0.12607857584953308, \"time-step\": 1700}, {\"accuracy\": 0.75, \"loss\": 0.12607741355895996, \"time-step\": 1701}, {\"accuracy\": 0.75, \"loss\": 0.12607575953006744, \"time-step\": 1702}, {\"accuracy\": 0.75, \"loss\": 0.12607455253601074, \"time-step\": 1703}, {\"accuracy\": 0.75, \"loss\": 0.12607291340827942, \"time-step\": 1704}, {\"accuracy\": 0.75, \"loss\": 0.12607169151306152, \"time-step\": 1705}, {\"accuracy\": 0.75, \"loss\": 0.12607015669345856, \"time-step\": 1706}, {\"accuracy\": 0.75, \"loss\": 0.12606891989707947, \"time-step\": 1707}, {\"accuracy\": 0.75, \"loss\": 0.12606726586818695, \"time-step\": 1708}, {\"accuracy\": 0.75, \"loss\": 0.12606610357761383, \"time-step\": 1709}, {\"accuracy\": 0.75, \"loss\": 0.1260644495487213, \"time-step\": 1710}, {\"accuracy\": 0.75, \"loss\": 0.12606331706047058, \"time-step\": 1711}, {\"accuracy\": 0.75, \"loss\": 0.12606172263622284, \"time-step\": 1712}, {\"accuracy\": 0.75, \"loss\": 0.12606056034564972, \"time-step\": 1713}, {\"accuracy\": 0.75, \"loss\": 0.12605896592140198, \"time-step\": 1714}, {\"accuracy\": 0.75, \"loss\": 0.12605784833431244, \"time-step\": 1715}, {\"accuracy\": 0.75, \"loss\": 0.1260562390089035, \"time-step\": 1716}, {\"accuracy\": 0.75, \"loss\": 0.12605513632297516, \"time-step\": 1717}, {\"accuracy\": 0.75, \"loss\": 0.12605345249176025, \"time-step\": 1718}, {\"accuracy\": 0.75, \"loss\": 0.12605233490467072, \"time-step\": 1719}, {\"accuracy\": 0.75, \"loss\": 0.12605077028274536, \"time-step\": 1720}, {\"accuracy\": 0.75, \"loss\": 0.1260496824979782, \"time-step\": 1721}, {\"accuracy\": 0.75, \"loss\": 0.12604805827140808, \"time-step\": 1722}, {\"accuracy\": 0.75, \"loss\": 0.12604697048664093, \"time-step\": 1723}, {\"accuracy\": 0.75, \"loss\": 0.12604539096355438, \"time-step\": 1724}, {\"accuracy\": 0.75, \"loss\": 0.12604427337646484, \"time-step\": 1725}, {\"accuracy\": 0.75, \"loss\": 0.1260426938533783, \"time-step\": 1726}, {\"accuracy\": 0.75, \"loss\": 0.12604165077209473, \"time-step\": 1727}, {\"accuracy\": 0.75, \"loss\": 0.12604008615016937, \"time-step\": 1728}, {\"accuracy\": 0.75, \"loss\": 0.126039057970047, \"time-step\": 1729}, {\"accuracy\": 0.75, \"loss\": 0.1260373741388321, \"time-step\": 1730}, {\"accuracy\": 0.75, \"loss\": 0.12603645026683807, \"time-step\": 1731}, {\"accuracy\": 0.75, \"loss\": 0.12603479623794556, \"time-step\": 1732}, {\"accuracy\": 0.75, \"loss\": 0.12603387236595154, \"time-step\": 1733}, {\"accuracy\": 0.75, \"loss\": 0.1260322630405426, \"time-step\": 1734}, {\"accuracy\": 0.75, \"loss\": 0.12603116035461426, \"time-step\": 1735}, {\"accuracy\": 0.75, \"loss\": 0.12602955102920532, \"time-step\": 1736}, {\"accuracy\": 0.75, \"loss\": 0.12602858245372772, \"time-step\": 1737}, {\"accuracy\": 0.75, \"loss\": 0.12602701783180237, \"time-step\": 1738}, {\"accuracy\": 0.75, \"loss\": 0.12602604925632477, \"time-step\": 1739}, {\"accuracy\": 0.75, \"loss\": 0.12602443993091583, \"time-step\": 1740}, {\"accuracy\": 0.75, \"loss\": 0.12602345645427704, \"time-step\": 1741}, {\"accuracy\": 0.75, \"loss\": 0.12602190673351288, \"time-step\": 1742}, {\"accuracy\": 0.75, \"loss\": 0.12602096796035767, \"time-step\": 1743}, {\"accuracy\": 0.75, \"loss\": 0.12601937353610992, \"time-step\": 1744}, {\"accuracy\": 0.75, \"loss\": 0.1260184496641159, \"time-step\": 1745}, {\"accuracy\": 0.75, \"loss\": 0.12601682543754578, \"time-step\": 1746}, {\"accuracy\": 0.75, \"loss\": 0.12601594626903534, \"time-step\": 1747}, {\"accuracy\": 0.75, \"loss\": 0.1260143220424652, \"time-step\": 1748}, {\"accuracy\": 0.75, \"loss\": 0.12601348757743835, \"time-step\": 1749}, {\"accuracy\": 0.75, \"loss\": 0.12601180374622345, \"time-step\": 1750}, {\"accuracy\": 0.75, \"loss\": 0.126010924577713, \"time-step\": 1751}, {\"accuracy\": 0.75, \"loss\": 0.12600931525230408, \"time-step\": 1752}, {\"accuracy\": 0.75, \"loss\": 0.12600846588611603, \"time-step\": 1753}, {\"accuracy\": 0.75, \"loss\": 0.12600690126419067, \"time-step\": 1754}, {\"accuracy\": 0.75, \"loss\": 0.12600597739219666, \"time-step\": 1755}, {\"accuracy\": 0.75, \"loss\": 0.12600447237491608, \"time-step\": 1756}, {\"accuracy\": 0.75, \"loss\": 0.12600350379943848, \"time-step\": 1757}, {\"accuracy\": 0.75, \"loss\": 0.1260019838809967, \"time-step\": 1758}, {\"accuracy\": 0.75, \"loss\": 0.12600113451480865, \"time-step\": 1759}, {\"accuracy\": 0.75, \"loss\": 0.12599952518939972, \"time-step\": 1760}, {\"accuracy\": 0.75, \"loss\": 0.12599867582321167, \"time-step\": 1761}, {\"accuracy\": 0.75, \"loss\": 0.1259971410036087, \"time-step\": 1762}, {\"accuracy\": 0.75, \"loss\": 0.12599635124206543, \"time-step\": 1763}, {\"accuracy\": 0.75, \"loss\": 0.1259947121143341, \"time-step\": 1764}, {\"accuracy\": 0.75, \"loss\": 0.12599395215511322, \"time-step\": 1765}, {\"accuracy\": 0.75, \"loss\": 0.12599235773086548, \"time-step\": 1766}, {\"accuracy\": 0.75, \"loss\": 0.12599150836467743, \"time-step\": 1767}, {\"accuracy\": 0.75, \"loss\": 0.12598995864391327, \"time-step\": 1768}, {\"accuracy\": 0.75, \"loss\": 0.12598919868469238, \"time-step\": 1769}, {\"accuracy\": 0.75, \"loss\": 0.12598761916160583, \"time-step\": 1770}, {\"accuracy\": 0.75, \"loss\": 0.12598684430122375, \"time-step\": 1771}, {\"accuracy\": 0.75, \"loss\": 0.1259852647781372, \"time-step\": 1772}, {\"accuracy\": 0.75, \"loss\": 0.12598448991775513, \"time-step\": 1773}, {\"accuracy\": 0.75, \"loss\": 0.12598295509815216, \"time-step\": 1774}, {\"accuracy\": 0.75, \"loss\": 0.12598221004009247, \"time-step\": 1775}, {\"accuracy\": 0.75, \"loss\": 0.12598055601119995, \"time-step\": 1776}, {\"accuracy\": 0.75, \"loss\": 0.12597984075546265, \"time-step\": 1777}, {\"accuracy\": 0.75, \"loss\": 0.12597830593585968, \"time-step\": 1778}, {\"accuracy\": 0.75, \"loss\": 0.12597757577896118, \"time-step\": 1779}, {\"accuracy\": 0.75, \"loss\": 0.12597602605819702, \"time-step\": 1780}, {\"accuracy\": 0.75, \"loss\": 0.12597531080245972, \"time-step\": 1781}, {\"accuracy\": 0.75, \"loss\": 0.12597370147705078, \"time-step\": 1782}, {\"accuracy\": 0.75, \"loss\": 0.12597297132015228, \"time-step\": 1783}, {\"accuracy\": 0.75, \"loss\": 0.12597142159938812, \"time-step\": 1784}, {\"accuracy\": 0.75, \"loss\": 0.1259707510471344, \"time-step\": 1785}, {\"accuracy\": 0.75, \"loss\": 0.12596912682056427, \"time-step\": 1786}, {\"accuracy\": 0.75, \"loss\": 0.12596847116947174, \"time-step\": 1787}, {\"accuracy\": 0.75, \"loss\": 0.12596696615219116, \"time-step\": 1788}, {\"accuracy\": 0.75, \"loss\": 0.12596628069877625, \"time-step\": 1789}, {\"accuracy\": 0.75, \"loss\": 0.1259647011756897, \"time-step\": 1790}, {\"accuracy\": 0.75, \"loss\": 0.12596404552459717, \"time-step\": 1791}, {\"accuracy\": 0.75, \"loss\": 0.125962495803833, \"time-step\": 1792}, {\"accuracy\": 0.75, \"loss\": 0.12596185505390167, \"time-step\": 1793}, {\"accuracy\": 0.75, \"loss\": 0.1259603202342987, \"time-step\": 1794}, {\"accuracy\": 0.75, \"loss\": 0.1259595900774002, \"time-step\": 1795}, {\"accuracy\": 0.75, \"loss\": 0.12595808506011963, \"time-step\": 1796}, {\"accuracy\": 0.75, \"loss\": 0.1259574294090271, \"time-step\": 1797}, {\"accuracy\": 0.75, \"loss\": 0.12595590949058533, \"time-step\": 1798}, {\"accuracy\": 0.75, \"loss\": 0.12595531344413757, \"time-step\": 1799}, {\"accuracy\": 0.75, \"loss\": 0.1259537637233734, \"time-step\": 1800}, {\"accuracy\": 0.75, \"loss\": 0.1259530782699585, \"time-step\": 1801}, {\"accuracy\": 0.75, \"loss\": 0.12595146894454956, \"time-step\": 1802}, {\"accuracy\": 0.75, \"loss\": 0.125950887799263, \"time-step\": 1803}, {\"accuracy\": 0.75, \"loss\": 0.12594948709011078, \"time-step\": 1804}, {\"accuracy\": 0.75, \"loss\": 0.12594883143901825, \"time-step\": 1805}, {\"accuracy\": 0.75, \"loss\": 0.1259472817182541, \"time-step\": 1806}, {\"accuracy\": 0.75, \"loss\": 0.12594662606716156, \"time-step\": 1807}, {\"accuracy\": 0.75, \"loss\": 0.12594513595104218, \"time-step\": 1808}, {\"accuracy\": 0.75, \"loss\": 0.12594455480575562, \"time-step\": 1809}, {\"accuracy\": 0.75, \"loss\": 0.12594301998615265, \"time-step\": 1810}, {\"accuracy\": 0.75, \"loss\": 0.12594246864318848, \"time-step\": 1811}, {\"accuracy\": 0.75, \"loss\": 0.12594088912010193, \"time-step\": 1812}, {\"accuracy\": 0.75, \"loss\": 0.12594035267829895, \"time-step\": 1813}, {\"accuracy\": 0.75, \"loss\": 0.12593883275985718, \"time-step\": 1814}, {\"accuracy\": 0.75, \"loss\": 0.1259383261203766, \"time-step\": 1815}, {\"accuracy\": 0.75, \"loss\": 0.12593677639961243, \"time-step\": 1816}, {\"accuracy\": 0.75, \"loss\": 0.12593628466129303, \"time-step\": 1817}, {\"accuracy\": 0.75, \"loss\": 0.1259346902370453, \"time-step\": 1818}, {\"accuracy\": 0.75, \"loss\": 0.1259341686964035, \"time-step\": 1819}, {\"accuracy\": 0.75, \"loss\": 0.12593267858028412, \"time-step\": 1820}, {\"accuracy\": 0.75, \"loss\": 0.12593212723731995, \"time-step\": 1821}, {\"accuracy\": 0.75, \"loss\": 0.12593062222003937, \"time-step\": 1822}, {\"accuracy\": 0.75, \"loss\": 0.1259300857782364, \"time-step\": 1823}, {\"accuracy\": 0.75, \"loss\": 0.1259285807609558, \"time-step\": 1824}, {\"accuracy\": 0.75, \"loss\": 0.12592813372612, \"time-step\": 1825}, {\"accuracy\": 0.75, \"loss\": 0.12592661380767822, \"time-step\": 1826}, {\"accuracy\": 0.75, \"loss\": 0.12592604756355286, \"time-step\": 1827}, {\"accuracy\": 0.75, \"loss\": 0.12592452764511108, \"time-step\": 1828}, {\"accuracy\": 0.75, \"loss\": 0.1259240359067917, \"time-step\": 1829}, {\"accuracy\": 0.75, \"loss\": 0.12592250108718872, \"time-step\": 1830}, {\"accuracy\": 0.75, \"loss\": 0.12592199444770813, \"time-step\": 1831}, {\"accuracy\": 0.75, \"loss\": 0.12592056393623352, \"time-step\": 1832}, {\"accuracy\": 0.75, \"loss\": 0.1259201467037201, \"time-step\": 1833}, {\"accuracy\": 0.75, \"loss\": 0.12591856718063354, \"time-step\": 1834}, {\"accuracy\": 0.75, \"loss\": 0.12591813504695892, \"time-step\": 1835}, {\"accuracy\": 0.75, \"loss\": 0.12591655552387238, \"time-step\": 1836}, {\"accuracy\": 0.75, \"loss\": 0.12591613829135895, \"time-step\": 1837}, {\"accuracy\": 0.75, \"loss\": 0.12591469287872314, \"time-step\": 1838}, {\"accuracy\": 0.75, \"loss\": 0.1259143054485321, \"time-step\": 1839}, {\"accuracy\": 0.75, \"loss\": 0.12591271102428436, \"time-step\": 1840}, {\"accuracy\": 0.75, \"loss\": 0.12591232359409332, \"time-step\": 1841}, {\"accuracy\": 0.75, \"loss\": 0.12591078877449036, \"time-step\": 1842}, {\"accuracy\": 0.75, \"loss\": 0.12591038644313812, \"time-step\": 1843}, {\"accuracy\": 0.75, \"loss\": 0.12590888142585754, \"time-step\": 1844}, {\"accuracy\": 0.75, \"loss\": 0.12590846419334412, \"time-step\": 1845}, {\"accuracy\": 0.75, \"loss\": 0.12590691447257996, \"time-step\": 1846}, {\"accuracy\": 0.75, \"loss\": 0.1259065866470337, \"time-step\": 1847}, {\"accuracy\": 0.75, \"loss\": 0.12590503692626953, \"time-step\": 1848}, {\"accuracy\": 0.75, \"loss\": 0.12590470910072327, \"time-step\": 1849}, {\"accuracy\": 0.75, \"loss\": 0.12590312957763672, \"time-step\": 1850}, {\"accuracy\": 0.75, \"loss\": 0.12590278685092926, \"time-step\": 1851}, {\"accuracy\": 0.75, \"loss\": 0.1259012520313263, \"time-step\": 1852}, {\"accuracy\": 0.75, \"loss\": 0.1259010136127472, \"time-step\": 1853}, {\"accuracy\": 0.75, \"loss\": 0.12589940428733826, \"time-step\": 1854}, {\"accuracy\": 0.75, \"loss\": 0.1258990466594696, \"time-step\": 1855}, {\"accuracy\": 0.75, \"loss\": 0.12589752674102783, \"time-step\": 1856}, {\"accuracy\": 0.75, \"loss\": 0.12589724361896515, \"time-step\": 1857}, {\"accuracy\": 0.75, \"loss\": 0.12589572370052338, \"time-step\": 1858}, {\"accuracy\": 0.75, \"loss\": 0.12589538097381592, \"time-step\": 1859}, {\"accuracy\": 0.75, \"loss\": 0.12589387595653534, \"time-step\": 1860}, {\"accuracy\": 0.75, \"loss\": 0.12589356303215027, \"time-step\": 1861}, {\"accuracy\": 0.75, \"loss\": 0.1258920133113861, \"time-step\": 1862}, {\"accuracy\": 0.75, \"loss\": 0.12589173018932343, \"time-step\": 1863}, {\"accuracy\": 0.75, \"loss\": 0.12589015066623688, \"time-step\": 1864}, {\"accuracy\": 0.75, \"loss\": 0.12588995695114136, \"time-step\": 1865}, {\"accuracy\": 0.75, \"loss\": 0.1258884072303772, \"time-step\": 1866}, {\"accuracy\": 0.75, \"loss\": 0.12588812410831451, \"time-step\": 1867}, {\"accuracy\": 0.75, \"loss\": 0.12588658928871155, \"time-step\": 1868}, {\"accuracy\": 0.75, \"loss\": 0.12588636577129364, \"time-step\": 1869}, {\"accuracy\": 0.75, \"loss\": 0.1258847713470459, \"time-step\": 1870}, {\"accuracy\": 0.75, \"loss\": 0.12588462233543396, \"time-step\": 1871}, {\"accuracy\": 0.75, \"loss\": 0.12588298320770264, \"time-step\": 1872}, {\"accuracy\": 0.75, \"loss\": 0.1258828043937683, \"time-step\": 1873}, {\"accuracy\": 0.75, \"loss\": 0.12588125467300415, \"time-step\": 1874}, {\"accuracy\": 0.75, \"loss\": 0.12588106095790863, \"time-step\": 1875}, {\"accuracy\": 0.75, \"loss\": 0.12587949633598328, \"time-step\": 1876}, {\"accuracy\": 0.75, \"loss\": 0.12587931752204895, \"time-step\": 1877}, {\"accuracy\": 0.75, \"loss\": 0.12587769329547882, \"time-step\": 1878}, {\"accuracy\": 0.75, \"loss\": 0.12587758898735046, \"time-step\": 1879}, {\"accuracy\": 0.75, \"loss\": 0.1258760392665863, \"time-step\": 1880}, {\"accuracy\": 0.75, \"loss\": 0.12587584555149078, \"time-step\": 1881}, {\"accuracy\": 0.75, \"loss\": 0.12587420642375946, \"time-step\": 1882}, {\"accuracy\": 0.75, \"loss\": 0.1258741319179535, \"time-step\": 1883}, {\"accuracy\": 0.75, \"loss\": 0.12587252259254456, \"time-step\": 1884}, {\"accuracy\": 0.75, \"loss\": 0.12587247788906097, \"time-step\": 1885}, {\"accuracy\": 0.75, \"loss\": 0.12587085366249084, \"time-step\": 1886}, {\"accuracy\": 0.75, \"loss\": 0.1258707344532013, \"time-step\": 1887}, {\"accuracy\": 0.75, \"loss\": 0.12586912512779236, \"time-step\": 1888}, {\"accuracy\": 0.75, \"loss\": 0.12586906552314758, \"time-step\": 1889}, {\"accuracy\": 0.75, \"loss\": 0.12586745619773865, \"time-step\": 1890}, {\"accuracy\": 0.75, \"loss\": 0.12586739659309387, \"time-step\": 1891}, {\"accuracy\": 0.75, \"loss\": 0.12586571276187897, \"time-step\": 1892}, {\"accuracy\": 0.75, \"loss\": 0.12586572766304016, \"time-step\": 1893}, {\"accuracy\": 0.75, \"loss\": 0.12586405873298645, \"time-step\": 1894}, {\"accuracy\": 0.75, \"loss\": 0.1258639693260193, \"time-step\": 1895}, {\"accuracy\": 0.75, \"loss\": 0.12586244940757751, \"time-step\": 1896}, {\"accuracy\": 0.75, \"loss\": 0.12586244940757751, \"time-step\": 1897}, {\"accuracy\": 0.75, \"loss\": 0.12586082518100739, \"time-step\": 1898}, {\"accuracy\": 0.75, \"loss\": 0.1258607804775238, \"time-step\": 1899}, {\"accuracy\": 0.75, \"loss\": 0.1258591264486313, \"time-step\": 1900}, {\"accuracy\": 0.75, \"loss\": 0.1258590817451477, \"time-step\": 1901}, {\"accuracy\": 0.75, \"loss\": 0.12585756182670593, \"time-step\": 1902}, {\"accuracy\": 0.75, \"loss\": 0.1258574277162552, \"time-step\": 1903}, {\"accuracy\": 0.75, \"loss\": 0.12585587799549103, \"time-step\": 1904}, {\"accuracy\": 0.75, \"loss\": 0.12585589289665222, \"time-step\": 1905}, {\"accuracy\": 0.75, \"loss\": 0.12585431337356567, \"time-step\": 1906}, {\"accuracy\": 0.75, \"loss\": 0.12585429847240448, \"time-step\": 1907}, {\"accuracy\": 0.75, \"loss\": 0.12585262954235077, \"time-step\": 1908}, {\"accuracy\": 0.75, \"loss\": 0.12585267424583435, \"time-step\": 1909}, {\"accuracy\": 0.75, \"loss\": 0.125851109623909, \"time-step\": 1910}, {\"accuracy\": 0.75, \"loss\": 0.1258510947227478, \"time-step\": 1911}, {\"accuracy\": 0.75, \"loss\": 0.12584948539733887, \"time-step\": 1912}, {\"accuracy\": 0.75, \"loss\": 0.12584948539733887, \"time-step\": 1913}, {\"accuracy\": 0.75, \"loss\": 0.12584789097309113, \"time-step\": 1914}, {\"accuracy\": 0.75, \"loss\": 0.1258479505777359, \"time-step\": 1915}, {\"accuracy\": 0.75, \"loss\": 0.12584629654884338, \"time-step\": 1916}, {\"accuracy\": 0.75, \"loss\": 0.12584632635116577, \"time-step\": 1917}, {\"accuracy\": 0.75, \"loss\": 0.12584473192691803, \"time-step\": 1918}, {\"accuracy\": 0.75, \"loss\": 0.1258448362350464, \"time-step\": 1919}, {\"accuracy\": 0.75, \"loss\": 0.12584325671195984, \"time-step\": 1920}, {\"accuracy\": 0.75, \"loss\": 0.12584330141544342, \"time-step\": 1921}, {\"accuracy\": 0.75, \"loss\": 0.12584172189235687, \"time-step\": 1922}, {\"accuracy\": 0.75, \"loss\": 0.12584176659584045, \"time-step\": 1923}, {\"accuracy\": 0.75, \"loss\": 0.1258401721715927, \"time-step\": 1924}, {\"accuracy\": 0.75, \"loss\": 0.12584024667739868, \"time-step\": 1925}, {\"accuracy\": 0.75, \"loss\": 0.12583862245082855, \"time-step\": 1926}, {\"accuracy\": 0.75, \"loss\": 0.12583869695663452, \"time-step\": 1927}, {\"accuracy\": 0.75, \"loss\": 0.1258370578289032, \"time-step\": 1928}, {\"accuracy\": 0.75, \"loss\": 0.12583723664283752, \"time-step\": 1929}, {\"accuracy\": 0.75, \"loss\": 0.1258355677127838, \"time-step\": 1930}, {\"accuracy\": 0.75, \"loss\": 0.12583568692207336, \"time-step\": 1931}, {\"accuracy\": 0.75, \"loss\": 0.12583407759666443, \"time-step\": 1932}, {\"accuracy\": 0.75, \"loss\": 0.12583424150943756, \"time-step\": 1933}, {\"accuracy\": 0.75, \"loss\": 0.12583255767822266, \"time-step\": 1934}, {\"accuracy\": 0.75, \"loss\": 0.1258326917886734, \"time-step\": 1935}, {\"accuracy\": 0.75, \"loss\": 0.12583105266094208, \"time-step\": 1936}, {\"accuracy\": 0.75, \"loss\": 0.1258312612771988, \"time-step\": 1937}, {\"accuracy\": 0.75, \"loss\": 0.12582960724830627, \"time-step\": 1938}, {\"accuracy\": 0.75, \"loss\": 0.1258298009634018, \"time-step\": 1939}, {\"accuracy\": 0.75, \"loss\": 0.12582813203334808, \"time-step\": 1940}, {\"accuracy\": 0.75, \"loss\": 0.12582837045192719, \"time-step\": 1941}, {\"accuracy\": 0.75, \"loss\": 0.1258266419172287, \"time-step\": 1942}, {\"accuracy\": 0.75, \"loss\": 0.1258268654346466, \"time-step\": 1943}, {\"accuracy\": 0.75, \"loss\": 0.12582524120807648, \"time-step\": 1944}, {\"accuracy\": 0.75, \"loss\": 0.12582550942897797, \"time-step\": 1945}, {\"accuracy\": 0.75, \"loss\": 0.12582381069660187, \"time-step\": 1946}, {\"accuracy\": 0.75, \"loss\": 0.12582410871982574, \"time-step\": 1947}, {\"accuracy\": 0.75, \"loss\": 0.12582233548164368, \"time-step\": 1948}, {\"accuracy\": 0.75, \"loss\": 0.12582260370254517, \"time-step\": 1949}, {\"accuracy\": 0.75, \"loss\": 0.12582087516784668, \"time-step\": 1950}, {\"accuracy\": 0.75, \"loss\": 0.12582120299339294, \"time-step\": 1951}, {\"accuracy\": 0.75, \"loss\": 0.12581951916217804, \"time-step\": 1952}, {\"accuracy\": 0.75, \"loss\": 0.12581980228424072, \"time-step\": 1953}, {\"accuracy\": 0.75, \"loss\": 0.12581799924373627, \"time-step\": 1954}, {\"accuracy\": 0.75, \"loss\": 0.12581831216812134, \"time-step\": 1955}, {\"accuracy\": 0.75, \"loss\": 0.12581659853458405, \"time-step\": 1956}, {\"accuracy\": 0.75, \"loss\": 0.12581703066825867, \"time-step\": 1957}, {\"accuracy\": 0.75, \"loss\": 0.1258152574300766, \"time-step\": 1958}, {\"accuracy\": 0.75, \"loss\": 0.12581560015678406, \"time-step\": 1959}, {\"accuracy\": 0.75, \"loss\": 0.12581385672092438, \"time-step\": 1960}, {\"accuracy\": 0.75, \"loss\": 0.12581421434879303, \"time-step\": 1961}, {\"accuracy\": 0.75, \"loss\": 0.12581250071525574, \"time-step\": 1962}, {\"accuracy\": 0.75, \"loss\": 0.1258128434419632, \"time-step\": 1963}, {\"accuracy\": 0.75, \"loss\": 0.1258111298084259, \"time-step\": 1964}, {\"accuracy\": 0.75, \"loss\": 0.12581145763397217, \"time-step\": 1965}, {\"accuracy\": 0.75, \"loss\": 0.12580980360507965, \"time-step\": 1966}, {\"accuracy\": 0.75, \"loss\": 0.1258101761341095, \"time-step\": 1967}, {\"accuracy\": 0.75, \"loss\": 0.12580837309360504, \"time-step\": 1968}, {\"accuracy\": 0.75, \"loss\": 0.12580882012844086, \"time-step\": 1969}, {\"accuracy\": 0.75, \"loss\": 0.12580710649490356, \"time-step\": 1970}, {\"accuracy\": 0.75, \"loss\": 0.1258074790239334, \"time-step\": 1971}, {\"accuracy\": 0.75, \"loss\": 0.12580576539039612, \"time-step\": 1972}, {\"accuracy\": 0.75, \"loss\": 0.12580615282058716, \"time-step\": 1973}, {\"accuracy\": 0.75, \"loss\": 0.12580439448356628, \"time-step\": 1974}, {\"accuracy\": 0.75, \"loss\": 0.1258048713207245, \"time-step\": 1975}, {\"accuracy\": 0.75, \"loss\": 0.12580306828022003, \"time-step\": 1976}, {\"accuracy\": 0.75, \"loss\": 0.12580350041389465, \"time-step\": 1977}, {\"accuracy\": 0.75, \"loss\": 0.12580180168151855, \"time-step\": 1978}, {\"accuracy\": 0.75, \"loss\": 0.1258021593093872, \"time-step\": 1979}, {\"accuracy\": 0.75, \"loss\": 0.12580032646656036, \"time-step\": 1980}, {\"accuracy\": 0.75, \"loss\": 0.1258009523153305, \"time-step\": 1981}, {\"accuracy\": 0.75, \"loss\": 0.12579916417598724, \"time-step\": 1982}, {\"accuracy\": 0.75, \"loss\": 0.12579959630966187, \"time-step\": 1983}, {\"accuracy\": 0.75, \"loss\": 0.125797837972641, \"time-step\": 1984}, {\"accuracy\": 0.75, \"loss\": 0.1257983297109604, \"time-step\": 1985}, {\"accuracy\": 0.75, \"loss\": 0.12579652667045593, \"time-step\": 1986}, {\"accuracy\": 0.75, \"loss\": 0.12579700350761414, \"time-step\": 1987}, {\"accuracy\": 0.75, \"loss\": 0.1257951855659485, \"time-step\": 1988}, {\"accuracy\": 0.75, \"loss\": 0.12579576671123505, \"time-step\": 1989}, {\"accuracy\": 0.75, \"loss\": 0.1257939636707306, \"time-step\": 1990}, {\"accuracy\": 0.75, \"loss\": 0.12579450011253357, \"time-step\": 1991}, {\"accuracy\": 0.75, \"loss\": 0.12579278647899628, \"time-step\": 1992}, {\"accuracy\": 0.75, \"loss\": 0.1257932037115097, \"time-step\": 1993}, {\"accuracy\": 0.75, \"loss\": 0.1257914900779724, \"time-step\": 1994}, {\"accuracy\": 0.75, \"loss\": 0.1257920116186142, \"time-step\": 1995}, {\"accuracy\": 0.75, \"loss\": 0.12579020857810974, \"time-step\": 1996}, {\"accuracy\": 0.75, \"loss\": 0.1257907748222351, \"time-step\": 1997}, {\"accuracy\": 0.75, \"loss\": 0.12578895688056946, \"time-step\": 1998}, {\"accuracy\": 0.75, \"loss\": 0.12578950822353363, \"time-step\": 1999}, {\"accuracy\": 0.75, \"loss\": 0.12578770518302917, \"time-step\": 2000}, {\"accuracy\": 0.75, \"loss\": 0.12578825652599335, \"time-step\": 2001}, {\"accuracy\": 0.75, \"loss\": 0.12578655779361725, \"time-step\": 2002}, {\"accuracy\": 0.75, \"loss\": 0.12578706443309784, \"time-step\": 2003}, {\"accuracy\": 0.75, \"loss\": 0.125785231590271, \"time-step\": 2004}, {\"accuracy\": 0.75, \"loss\": 0.12578590214252472, \"time-step\": 2005}, {\"accuracy\": 0.75, \"loss\": 0.1257840245962143, \"time-step\": 2006}, {\"accuracy\": 0.75, \"loss\": 0.12578459084033966, \"time-step\": 2007}, {\"accuracy\": 0.75, \"loss\": 0.12578290700912476, \"time-step\": 2008}, {\"accuracy\": 0.75, \"loss\": 0.12578347325325012, \"time-step\": 2009}, {\"accuracy\": 0.75, \"loss\": 0.12578168511390686, \"time-step\": 2010}, {\"accuracy\": 0.75, \"loss\": 0.12578228116035461, \"time-step\": 2011}, {\"accuracy\": 0.75, \"loss\": 0.12578044831752777, \"time-step\": 2012}, {\"accuracy\": 0.75, \"loss\": 0.1257810890674591, \"time-step\": 2013}, {\"accuracy\": 0.75, \"loss\": 0.1257791817188263, \"time-step\": 2014}, {\"accuracy\": 0.75, \"loss\": 0.1257799118757248, \"time-step\": 2015}, {\"accuracy\": 0.75, \"loss\": 0.12577801942825317, \"time-step\": 2016}, {\"accuracy\": 0.75, \"loss\": 0.12577871978282928, \"time-step\": 2017}, {\"accuracy\": 0.75, \"loss\": 0.12577691674232483, \"time-step\": 2018}, {\"accuracy\": 0.75, \"loss\": 0.12577757239341736, \"time-step\": 2019}, {\"accuracy\": 0.75, \"loss\": 0.12577570974826813, \"time-step\": 2020}, {\"accuracy\": 0.75, \"loss\": 0.12577641010284424, \"time-step\": 2021}, {\"accuracy\": 0.75, \"loss\": 0.1257745325565338, \"time-step\": 2022}, {\"accuracy\": 0.75, \"loss\": 0.12577524781227112, \"time-step\": 2023}, {\"accuracy\": 0.75, \"loss\": 0.1257733553647995, \"time-step\": 2024}, {\"accuracy\": 0.75, \"loss\": 0.1257741004228592, \"time-step\": 2025}, {\"accuracy\": 0.75, \"loss\": 0.1257721334695816, \"time-step\": 2026}, {\"accuracy\": 0.75, \"loss\": 0.1257728934288025, \"time-step\": 2027}, {\"accuracy\": 0.75, \"loss\": 0.12577104568481445, \"time-step\": 2028}, {\"accuracy\": 0.75, \"loss\": 0.12577185034751892, \"time-step\": 2029}, {\"accuracy\": 0.75, \"loss\": 0.1257699728012085, \"time-step\": 2030}, {\"accuracy\": 0.75, \"loss\": 0.1257707178592682, \"time-step\": 2031}, {\"accuracy\": 0.75, \"loss\": 0.1257687658071518, \"time-step\": 2032}, {\"accuracy\": 0.75, \"loss\": 0.12576960027217865, \"time-step\": 2033}, {\"accuracy\": 0.75, \"loss\": 0.12576770782470703, \"time-step\": 2034}, {\"accuracy\": 0.75, \"loss\": 0.12576845288276672, \"time-step\": 2035}, {\"accuracy\": 0.75, \"loss\": 0.12576663494110107, \"time-step\": 2036}, {\"accuracy\": 0.75, \"loss\": 0.12576736509799957, \"time-step\": 2037}, {\"accuracy\": 0.75, \"loss\": 0.1257653832435608, \"time-step\": 2038}, {\"accuracy\": 0.75, \"loss\": 0.12576626241207123, \"time-step\": 2039}, {\"accuracy\": 0.75, \"loss\": 0.12576429545879364, \"time-step\": 2040}, {\"accuracy\": 0.75, \"loss\": 0.12576521933078766, \"time-step\": 2041}, {\"accuracy\": 0.75, \"loss\": 0.1257632076740265, \"time-step\": 2042}, {\"accuracy\": 0.75, \"loss\": 0.12576404213905334, \"time-step\": 2043}, {\"accuracy\": 0.75, \"loss\": 0.12576201558113098, \"time-step\": 2044}, {\"accuracy\": 0.75, \"loss\": 0.1257629692554474, \"time-step\": 2045}, {\"accuracy\": 0.75, \"loss\": 0.12576106190681458, \"time-step\": 2046}, {\"accuracy\": 0.75, \"loss\": 0.125761941075325, \"time-step\": 2047}, {\"accuracy\": 0.75, \"loss\": 0.12575992941856384, \"time-step\": 2048}, {\"accuracy\": 0.75, \"loss\": 0.12576088309288025, \"time-step\": 2049}, {\"accuracy\": 0.75, \"loss\": 0.12575891613960266, \"time-step\": 2050}, {\"accuracy\": 0.75, \"loss\": 0.1257598102092743, \"time-step\": 2051}, {\"accuracy\": 0.75, \"loss\": 0.12575788795948029, \"time-step\": 2052}, {\"accuracy\": 0.75, \"loss\": 0.12575873732566833, \"time-step\": 2053}, {\"accuracy\": 0.75, \"loss\": 0.12575671076774597, \"time-step\": 2054}, {\"accuracy\": 0.75, \"loss\": 0.12575767934322357, \"time-step\": 2055}, {\"accuracy\": 0.75, \"loss\": 0.12575571238994598, \"time-step\": 2056}, {\"accuracy\": 0.75, \"loss\": 0.1257566213607788, \"time-step\": 2057}, {\"accuracy\": 0.75, \"loss\": 0.12575465440750122, \"time-step\": 2058}, {\"accuracy\": 0.75, \"loss\": 0.12575557827949524, \"time-step\": 2059}, {\"accuracy\": 0.75, \"loss\": 0.12575364112854004, \"time-step\": 2060}, {\"accuracy\": 0.75, \"loss\": 0.12575457990169525, \"time-step\": 2061}, {\"accuracy\": 0.75, \"loss\": 0.1257525533437729, \"time-step\": 2062}, {\"accuracy\": 0.75, \"loss\": 0.12575361132621765, \"time-step\": 2063}, {\"accuracy\": 0.75, \"loss\": 0.1257515549659729, \"time-step\": 2064}, {\"accuracy\": 0.75, \"loss\": 0.1257525384426117, \"time-step\": 2065}, {\"accuracy\": 0.75, \"loss\": 0.12575054168701172, \"time-step\": 2066}, {\"accuracy\": 0.75, \"loss\": 0.12575149536132812, \"time-step\": 2067}, {\"accuracy\": 0.75, \"loss\": 0.12574945390224457, \"time-step\": 2068}, {\"accuracy\": 0.75, \"loss\": 0.12575049698352814, \"time-step\": 2069}, {\"accuracy\": 0.75, \"loss\": 0.12574851512908936, \"time-step\": 2070}, {\"accuracy\": 0.75, \"loss\": 0.12574942409992218, \"time-step\": 2071}, {\"accuracy\": 0.75, \"loss\": 0.1257474571466446, \"time-step\": 2072}, {\"accuracy\": 0.75, \"loss\": 0.12574857473373413, \"time-step\": 2073}, {\"accuracy\": 0.75, \"loss\": 0.1257464587688446, \"time-step\": 2074}, {\"accuracy\": 0.75, \"loss\": 0.12574753165245056, \"time-step\": 2075}, {\"accuracy\": 0.75, \"loss\": 0.12574543058872223, \"time-step\": 2076}, {\"accuracy\": 0.75, \"loss\": 0.1257464736700058, \"time-step\": 2077}, {\"accuracy\": 0.75, \"loss\": 0.12574443221092224, \"time-step\": 2078}, {\"accuracy\": 0.75, \"loss\": 0.1257455199956894, \"time-step\": 2079}, {\"accuracy\": 0.75, \"loss\": 0.12574343383312225, \"time-step\": 2080}, {\"accuracy\": 0.75, \"loss\": 0.12574459612369537, \"time-step\": 2081}, {\"accuracy\": 0.75, \"loss\": 0.12574243545532227, \"time-step\": 2082}, {\"accuracy\": 0.75, \"loss\": 0.1257435828447342, \"time-step\": 2083}, {\"accuracy\": 0.75, \"loss\": 0.12574146687984467, \"time-step\": 2084}, {\"accuracy\": 0.75, \"loss\": 0.1257426142692566, \"time-step\": 2085}, {\"accuracy\": 0.75, \"loss\": 0.12574058771133423, \"time-step\": 2086}, {\"accuracy\": 0.75, \"loss\": 0.125741645693779, \"time-step\": 2087}, {\"accuracy\": 0.75, \"loss\": 0.12573955953121185, \"time-step\": 2088}, {\"accuracy\": 0.75, \"loss\": 0.12574072182178497, \"time-step\": 2089}, {\"accuracy\": 0.75, \"loss\": 0.12573863565921783, \"time-step\": 2090}, {\"accuracy\": 0.75, \"loss\": 0.12573976814746857, \"time-step\": 2091}, {\"accuracy\": 0.75, \"loss\": 0.125737726688385, \"time-step\": 2092}, {\"accuracy\": 0.75, \"loss\": 0.1257389485836029, \"time-step\": 2093}, {\"accuracy\": 0.75, \"loss\": 0.12573669850826263, \"time-step\": 2094}, {\"accuracy\": 0.75, \"loss\": 0.12573792040348053, \"time-step\": 2095}, {\"accuracy\": 0.75, \"loss\": 0.12573584914207458, \"time-step\": 2096}, {\"accuracy\": 0.75, \"loss\": 0.1257370412349701, \"time-step\": 2097}, {\"accuracy\": 0.75, \"loss\": 0.12573488056659698, \"time-step\": 2098}, {\"accuracy\": 0.75, \"loss\": 0.1257360875606537, \"time-step\": 2099}, {\"accuracy\": 0.75, \"loss\": 0.125733882188797, \"time-step\": 2100}, {\"accuracy\": 0.75, \"loss\": 0.1257350593805313, \"time-step\": 2101}, {\"accuracy\": 0.75, \"loss\": 0.1257329285144806, \"time-step\": 2102}, {\"accuracy\": 0.75, \"loss\": 0.12573426961898804, \"time-step\": 2103}, {\"accuracy\": 0.75, \"loss\": 0.12573203444480896, \"time-step\": 2104}, {\"accuracy\": 0.75, \"loss\": 0.12573334574699402, \"time-step\": 2105}, {\"accuracy\": 0.75, \"loss\": 0.1257311999797821, \"time-step\": 2106}, {\"accuracy\": 0.75, \"loss\": 0.12573248147964478, \"time-step\": 2107}, {\"accuracy\": 0.75, \"loss\": 0.1257302314043045, \"time-step\": 2108}, {\"accuracy\": 0.75, \"loss\": 0.12573154270648956, \"time-step\": 2109}, {\"accuracy\": 0.75, \"loss\": 0.1257292926311493, \"time-step\": 2110}, {\"accuracy\": 0.75, \"loss\": 0.12573064863681793, \"time-step\": 2111}, {\"accuracy\": 0.75, \"loss\": 0.12572847306728363, \"time-step\": 2112}, {\"accuracy\": 0.75, \"loss\": 0.1257297694683075, \"time-step\": 2113}, {\"accuracy\": 0.75, \"loss\": 0.12572750449180603, \"time-step\": 2114}, {\"accuracy\": 0.75, \"loss\": 0.12572892010211945, \"time-step\": 2115}, {\"accuracy\": 0.75, \"loss\": 0.125726580619812, \"time-step\": 2116}, {\"accuracy\": 0.75, \"loss\": 0.12572801113128662, \"time-step\": 2117}, {\"accuracy\": 0.75, \"loss\": 0.12572574615478516, \"time-step\": 2118}, {\"accuracy\": 0.75, \"loss\": 0.12572714686393738, \"time-step\": 2119}, {\"accuracy\": 0.75, \"loss\": 0.12572485208511353, \"time-step\": 2120}, {\"accuracy\": 0.75, \"loss\": 0.12572622299194336, \"time-step\": 2121}, {\"accuracy\": 0.75, \"loss\": 0.12572400271892548, \"time-step\": 2122}, {\"accuracy\": 0.75, \"loss\": 0.12572549283504486, \"time-step\": 2123}, {\"accuracy\": 0.75, \"loss\": 0.12572316825389862, \"time-step\": 2124}, {\"accuracy\": 0.75, \"loss\": 0.12572456896305084, \"time-step\": 2125}, {\"accuracy\": 0.75, \"loss\": 0.1257222592830658, \"time-step\": 2126}, {\"accuracy\": 0.75, \"loss\": 0.12572365999221802, \"time-step\": 2127}, {\"accuracy\": 0.75, \"loss\": 0.12572140991687775, \"time-step\": 2128}, {\"accuracy\": 0.75, \"loss\": 0.12572281062602997, \"time-step\": 2129}, {\"accuracy\": 0.75, \"loss\": 0.1257205605506897, \"time-step\": 2130}, {\"accuracy\": 0.75, \"loss\": 0.12572205066680908, \"time-step\": 2131}, {\"accuracy\": 0.75, \"loss\": 0.12571972608566284, \"time-step\": 2132}, {\"accuracy\": 0.75, \"loss\": 0.12572121620178223, \"time-step\": 2133}, {\"accuracy\": 0.75, \"loss\": 0.12571881711483002, \"time-step\": 2134}, {\"accuracy\": 0.75, \"loss\": 0.12572036683559418, \"time-step\": 2135}, {\"accuracy\": 0.75, \"loss\": 0.12571804225444794, \"time-step\": 2136}, {\"accuracy\": 0.75, \"loss\": 0.12571948766708374, \"time-step\": 2137}, {\"accuracy\": 0.75, \"loss\": 0.12571723759174347, \"time-step\": 2138}, {\"accuracy\": 0.75, \"loss\": 0.12571871280670166, \"time-step\": 2139}, {\"accuracy\": 0.75, \"loss\": 0.1257164627313614, \"time-step\": 2140}, {\"accuracy\": 0.75, \"loss\": 0.1257179081439972, \"time-step\": 2141}, {\"accuracy\": 0.75, \"loss\": 0.12571552395820618, \"time-step\": 2142}, {\"accuracy\": 0.75, \"loss\": 0.12571710348129272, \"time-step\": 2143}, {\"accuracy\": 0.75, \"loss\": 0.12571480870246887, \"time-step\": 2144}, {\"accuracy\": 0.75, \"loss\": 0.12571625411510468, \"time-step\": 2145}, {\"accuracy\": 0.75, \"loss\": 0.12571385502815247, \"time-step\": 2146}, {\"accuracy\": 0.75, \"loss\": 0.1257154792547226, \"time-step\": 2147}, {\"accuracy\": 0.75, \"loss\": 0.12571309506893158, \"time-step\": 2148}, {\"accuracy\": 0.75, \"loss\": 0.1257147192955017, \"time-step\": 2149}, {\"accuracy\": 0.75, \"loss\": 0.1257123500108719, \"time-step\": 2150}, {\"accuracy\": 0.75, \"loss\": 0.12571388483047485, \"time-step\": 2151}, {\"accuracy\": 0.75, \"loss\": 0.12571147084236145, \"time-step\": 2152}, {\"accuracy\": 0.75, \"loss\": 0.12571310997009277, \"time-step\": 2153}, {\"accuracy\": 0.75, \"loss\": 0.12571068108081818, \"time-step\": 2154}, {\"accuracy\": 0.75, \"loss\": 0.1257122904062271, \"time-step\": 2155}, {\"accuracy\": 0.75, \"loss\": 0.1257099211215973, \"time-step\": 2156}, {\"accuracy\": 0.75, \"loss\": 0.12571153044700623, \"time-step\": 2157}, {\"accuracy\": 0.75, \"loss\": 0.12570908665657043, \"time-step\": 2158}, {\"accuracy\": 0.75, \"loss\": 0.12571080029010773, \"time-step\": 2159}, {\"accuracy\": 0.75, \"loss\": 0.12570825219154358, \"time-step\": 2160}, {\"accuracy\": 0.75, \"loss\": 0.12571001052856445, \"time-step\": 2161}, {\"accuracy\": 0.75, \"loss\": 0.12570756673812866, \"time-step\": 2162}, {\"accuracy\": 0.75, \"loss\": 0.1257091611623764, \"time-step\": 2163}, {\"accuracy\": 0.75, \"loss\": 0.12570679187774658, \"time-step\": 2164}, {\"accuracy\": 0.75, \"loss\": 0.1257084608078003, \"time-step\": 2165}, {\"accuracy\": 0.75, \"loss\": 0.12570592761039734, \"time-step\": 2166}, {\"accuracy\": 0.75, \"loss\": 0.12570776045322418, \"time-step\": 2167}, {\"accuracy\": 0.75, \"loss\": 0.12570519745349884, \"time-step\": 2168}, {\"accuracy\": 0.75, \"loss\": 0.12570695579051971, \"time-step\": 2169}, {\"accuracy\": 0.75, \"loss\": 0.12570443749427795, \"time-step\": 2170}, {\"accuracy\": 0.75, \"loss\": 0.12570630013942719, \"time-step\": 2171}, {\"accuracy\": 0.75, \"loss\": 0.12570366263389587, \"time-step\": 2172}, {\"accuracy\": 0.75, \"loss\": 0.12570549547672272, \"time-step\": 2173}, {\"accuracy\": 0.75, \"loss\": 0.12570302188396454, \"time-step\": 2174}, {\"accuracy\": 0.75, \"loss\": 0.12570475041866302, \"time-step\": 2175}, {\"accuracy\": 0.75, \"loss\": 0.12570223212242126, \"time-step\": 2176}, {\"accuracy\": 0.75, \"loss\": 0.12570397555828094, \"time-step\": 2177}, {\"accuracy\": 0.75, \"loss\": 0.125701442360878, \"time-step\": 2178}, {\"accuracy\": 0.75, \"loss\": 0.12570327520370483, \"time-step\": 2179}, {\"accuracy\": 0.75, \"loss\": 0.1257006973028183, \"time-step\": 2180}, {\"accuracy\": 0.75, \"loss\": 0.1257026046514511, \"time-step\": 2181}, {\"accuracy\": 0.75, \"loss\": 0.12569989264011383, \"time-step\": 2182}, {\"accuracy\": 0.75, \"loss\": 0.12570178508758545, \"time-step\": 2183}, {\"accuracy\": 0.75, \"loss\": 0.1256992071866989, \"time-step\": 2184}, {\"accuracy\": 0.75, \"loss\": 0.12570109963417053, \"time-step\": 2185}, {\"accuracy\": 0.75, \"loss\": 0.1256985068321228, \"time-step\": 2186}, {\"accuracy\": 0.75, \"loss\": 0.1257004588842392, \"time-step\": 2187}, {\"accuracy\": 0.75, \"loss\": 0.12569788098335266, \"time-step\": 2188}, {\"accuracy\": 0.75, \"loss\": 0.1256997287273407, \"time-step\": 2189}, {\"accuracy\": 0.75, \"loss\": 0.12569715082645416, \"time-step\": 2190}, {\"accuracy\": 0.75, \"loss\": 0.12569904327392578, \"time-step\": 2191}, {\"accuracy\": 0.75, \"loss\": 0.1256963163614273, \"time-step\": 2192}, {\"accuracy\": 0.75, \"loss\": 0.12569832801818848, \"time-step\": 2193}, {\"accuracy\": 0.75, \"loss\": 0.1256956309080124, \"time-step\": 2194}, {\"accuracy\": 0.75, \"loss\": 0.12569761276245117, \"time-step\": 2195}, {\"accuracy\": 0.75, \"loss\": 0.12569496035575867, \"time-step\": 2196}, {\"accuracy\": 0.75, \"loss\": 0.12569691240787506, \"time-step\": 2197}, {\"accuracy\": 0.75, \"loss\": 0.12569420039653778, \"time-step\": 2198}, {\"accuracy\": 0.75, \"loss\": 0.12569624185562134, \"time-step\": 2199}, {\"accuracy\": 0.75, \"loss\": 0.12569357454776764, \"time-step\": 2200}, {\"accuracy\": 0.75, \"loss\": 0.1256955862045288, \"time-step\": 2201}, {\"accuracy\": 0.75, \"loss\": 0.12569285929203033, \"time-step\": 2202}, {\"accuracy\": 0.75, \"loss\": 0.1256949007511139, \"time-step\": 2203}, {\"accuracy\": 0.75, \"loss\": 0.12569215893745422, \"time-step\": 2204}, {\"accuracy\": 0.75, \"loss\": 0.1256941705942154, \"time-step\": 2205}, {\"accuracy\": 0.75, \"loss\": 0.12569144368171692, \"time-step\": 2206}, {\"accuracy\": 0.75, \"loss\": 0.12569355964660645, \"time-step\": 2207}, {\"accuracy\": 0.75, \"loss\": 0.12569083273410797, \"time-step\": 2208}, {\"accuracy\": 0.75, \"loss\": 0.12569281458854675, \"time-step\": 2209}, {\"accuracy\": 0.75, \"loss\": 0.12569019198417664, \"time-step\": 2210}, {\"accuracy\": 0.75, \"loss\": 0.1256922483444214, \"time-step\": 2211}, {\"accuracy\": 0.75, \"loss\": 0.12568944692611694, \"time-step\": 2212}, {\"accuracy\": 0.75, \"loss\": 0.1256915032863617, \"time-step\": 2213}, {\"accuracy\": 0.75, \"loss\": 0.12568877637386322, \"time-step\": 2214}, {\"accuracy\": 0.75, \"loss\": 0.12569087743759155, \"time-step\": 2215}, {\"accuracy\": 0.75, \"loss\": 0.1256880909204483, \"time-step\": 2216}, {\"accuracy\": 0.75, \"loss\": 0.1256902515888214, \"time-step\": 2217}, {\"accuracy\": 0.75, \"loss\": 0.12568746507167816, \"time-step\": 2218}, {\"accuracy\": 0.75, \"loss\": 0.12568959593772888, \"time-step\": 2219}, {\"accuracy\": 0.75, \"loss\": 0.1256868690252304, \"time-step\": 2220}, {\"accuracy\": 0.75, \"loss\": 0.12568895518779755, \"time-step\": 2221}, {\"accuracy\": 0.75, \"loss\": 0.1256861686706543, \"time-step\": 2222}, {\"accuracy\": 0.75, \"loss\": 0.12568838894367218, \"time-step\": 2223}, {\"accuracy\": 0.75, \"loss\": 0.12568554282188416, \"time-step\": 2224}, {\"accuracy\": 0.75, \"loss\": 0.12568771839141846, \"time-step\": 2225}, {\"accuracy\": 0.75, \"loss\": 0.1256849467754364, \"time-step\": 2226}, {\"accuracy\": 0.75, \"loss\": 0.1256871223449707, \"time-step\": 2227}, {\"accuracy\": 0.75, \"loss\": 0.12568426132202148, \"time-step\": 2228}, {\"accuracy\": 0.75, \"loss\": 0.12568636238574982, \"time-step\": 2229}, {\"accuracy\": 0.75, \"loss\": 0.12568366527557373, \"time-step\": 2230}, {\"accuracy\": 0.75, \"loss\": 0.1256858855485916, \"time-step\": 2231}, {\"accuracy\": 0.75, \"loss\": 0.12568299472332, \"time-step\": 2232}, {\"accuracy\": 0.75, \"loss\": 0.12568524479866028, \"time-step\": 2233}, {\"accuracy\": 0.75, \"loss\": 0.12568233907222748, \"time-step\": 2234}, {\"accuracy\": 0.75, \"loss\": 0.12568460404872894, \"time-step\": 2235}, {\"accuracy\": 0.75, \"loss\": 0.12568172812461853, \"time-step\": 2236}, {\"accuracy\": 0.75, \"loss\": 0.12568399310112, \"time-step\": 2237}, {\"accuracy\": 0.75, \"loss\": 0.1256810426712036, \"time-step\": 2238}, {\"accuracy\": 0.75, \"loss\": 0.12568333745002747, \"time-step\": 2239}, {\"accuracy\": 0.75, \"loss\": 0.12568053603172302, \"time-step\": 2240}, {\"accuracy\": 0.75, \"loss\": 0.1256827861070633, \"time-step\": 2241}, {\"accuracy\": 0.75, \"loss\": 0.1256798803806305, \"time-step\": 2242}, {\"accuracy\": 0.75, \"loss\": 0.1256822794675827, \"time-step\": 2243}, {\"accuracy\": 0.75, \"loss\": 0.12567928433418274, \"time-step\": 2244}, {\"accuracy\": 0.75, \"loss\": 0.12568160891532898, \"time-step\": 2245}, {\"accuracy\": 0.75, \"loss\": 0.12567868828773499, \"time-step\": 2246}, {\"accuracy\": 0.75, \"loss\": 0.12568099796772003, \"time-step\": 2247}, {\"accuracy\": 0.75, \"loss\": 0.12567810714244843, \"time-step\": 2248}, {\"accuracy\": 0.75, \"loss\": 0.12568043172359467, \"time-step\": 2249}, {\"accuracy\": 0.75, \"loss\": 0.1256774663925171, \"time-step\": 2250}, {\"accuracy\": 0.75, \"loss\": 0.12567982077598572, \"time-step\": 2251}, {\"accuracy\": 0.75, \"loss\": 0.12567684054374695, \"time-step\": 2252}, {\"accuracy\": 0.75, \"loss\": 0.12567922472953796, \"time-step\": 2253}, {\"accuracy\": 0.75, \"loss\": 0.12567630410194397, \"time-step\": 2254}, {\"accuracy\": 0.75, \"loss\": 0.12567871809005737, \"time-step\": 2255}, {\"accuracy\": 0.75, \"loss\": 0.12567569315433502, \"time-step\": 2256}, {\"accuracy\": 0.75, \"loss\": 0.125678151845932, \"time-step\": 2257}, {\"accuracy\": 0.75, \"loss\": 0.1256749927997589, \"time-step\": 2258}, {\"accuracy\": 0.75, \"loss\": 0.12567749619483948, \"time-step\": 2259}, {\"accuracy\": 0.75, \"loss\": 0.12567450106143951, \"time-step\": 2260}, {\"accuracy\": 0.75, \"loss\": 0.1256769299507141, \"time-step\": 2261}, {\"accuracy\": 0.75, \"loss\": 0.12567394971847534, \"time-step\": 2262}, {\"accuracy\": 0.75, \"loss\": 0.12567639350891113, \"time-step\": 2263}, {\"accuracy\": 0.75, \"loss\": 0.12567338347434998, \"time-step\": 2264}, {\"accuracy\": 0.75, \"loss\": 0.12567591667175293, \"time-step\": 2265}, {\"accuracy\": 0.75, \"loss\": 0.1256728321313858, \"time-step\": 2266}, {\"accuracy\": 0.75, \"loss\": 0.12567535042762756, \"time-step\": 2267}, {\"accuracy\": 0.75, \"loss\": 0.12567228078842163, \"time-step\": 2268}, {\"accuracy\": 0.75, \"loss\": 0.12567484378814697, \"time-step\": 2269}, {\"accuracy\": 0.75, \"loss\": 0.1256716251373291, \"time-step\": 2270}, {\"accuracy\": 0.75, \"loss\": 0.12567423284053802, \"time-step\": 2271}, {\"accuracy\": 0.75, \"loss\": 0.12567104399204254, \"time-step\": 2272}, {\"accuracy\": 0.75, \"loss\": 0.12567363679409027, \"time-step\": 2273}, {\"accuracy\": 0.75, \"loss\": 0.12567049264907837, \"time-step\": 2274}, {\"accuracy\": 0.75, \"loss\": 0.12567317485809326, \"time-step\": 2275}, {\"accuracy\": 0.75, \"loss\": 0.12566997110843658, \"time-step\": 2276}, {\"accuracy\": 0.75, \"loss\": 0.12567254900932312, \"time-step\": 2277}, {\"accuracy\": 0.75, \"loss\": 0.1256694346666336, \"time-step\": 2278}, {\"accuracy\": 0.75, \"loss\": 0.1256720870733261, \"time-step\": 2279}, {\"accuracy\": 0.75, \"loss\": 0.12566888332366943, \"time-step\": 2280}, {\"accuracy\": 0.75, \"loss\": 0.12567153573036194, \"time-step\": 2281}, {\"accuracy\": 0.75, \"loss\": 0.12566830217838287, \"time-step\": 2282}, {\"accuracy\": 0.75, \"loss\": 0.12567099928855896, \"time-step\": 2283}, {\"accuracy\": 0.75, \"loss\": 0.12566779553890228, \"time-step\": 2284}, {\"accuracy\": 0.75, \"loss\": 0.12567046284675598, \"time-step\": 2285}, {\"accuracy\": 0.75, \"loss\": 0.12566719949245453, \"time-step\": 2286}, {\"accuracy\": 0.75, \"loss\": 0.12566998600959778, \"time-step\": 2287}, {\"accuracy\": 0.75, \"loss\": 0.12566670775413513, \"time-step\": 2288}, {\"accuracy\": 0.75, \"loss\": 0.12566938996315002, \"time-step\": 2289}, {\"accuracy\": 0.75, \"loss\": 0.12566614151000977, \"time-step\": 2290}, {\"accuracy\": 0.75, \"loss\": 0.12566889822483063, \"time-step\": 2291}, {\"accuracy\": 0.75, \"loss\": 0.12566561996936798, \"time-step\": 2292}, {\"accuracy\": 0.75, \"loss\": 0.12566837668418884, \"time-step\": 2293}, {\"accuracy\": 0.75, \"loss\": 0.12566518783569336, \"time-step\": 2294}, {\"accuracy\": 0.75, \"loss\": 0.12566791474819183, \"time-step\": 2295}, {\"accuracy\": 0.75, \"loss\": 0.1256646364927292, \"time-step\": 2296}, {\"accuracy\": 0.75, \"loss\": 0.12566737830638885, \"time-step\": 2297}, {\"accuracy\": 0.75, \"loss\": 0.12566404044628143, \"time-step\": 2298}, {\"accuracy\": 0.75, \"loss\": 0.12566685676574707, \"time-step\": 2299}, {\"accuracy\": 0.75, \"loss\": 0.1256636083126068, \"time-step\": 2300}, {\"accuracy\": 0.75, \"loss\": 0.12566640973091125, \"time-step\": 2301}, {\"accuracy\": 0.75, \"loss\": 0.1256631463766098, \"time-step\": 2302}, {\"accuracy\": 0.75, \"loss\": 0.12566594779491425, \"time-step\": 2303}, {\"accuracy\": 0.75, \"loss\": 0.12566258013248444, \"time-step\": 2304}, {\"accuracy\": 0.75, \"loss\": 0.12566547095775604, \"time-step\": 2305}, {\"accuracy\": 0.75, \"loss\": 0.12566211819648743, \"time-step\": 2306}, {\"accuracy\": 0.75, \"loss\": 0.1256648600101471, \"time-step\": 2307}, {\"accuracy\": 0.75, \"loss\": 0.12566158175468445, \"time-step\": 2308}, {\"accuracy\": 0.75, \"loss\": 0.12566441297531128, \"time-step\": 2309}, {\"accuracy\": 0.75, \"loss\": 0.12566107511520386, \"time-step\": 2310}, {\"accuracy\": 0.75, \"loss\": 0.12566396594047546, \"time-step\": 2311}, {\"accuracy\": 0.75, \"loss\": 0.12566061317920685, \"time-step\": 2312}, {\"accuracy\": 0.75, \"loss\": 0.12566347420215607, \"time-step\": 2313}, {\"accuracy\": 0.75, \"loss\": 0.12566010653972626, \"time-step\": 2314}, {\"accuracy\": 0.75, \"loss\": 0.1256629228591919, \"time-step\": 2315}, {\"accuracy\": 0.75, \"loss\": 0.12565955519676208, \"time-step\": 2316}, {\"accuracy\": 0.75, \"loss\": 0.1256624311208725, \"time-step\": 2317}, {\"accuracy\": 0.75, \"loss\": 0.12565912306308746, \"time-step\": 2318}, {\"accuracy\": 0.75, \"loss\": 0.1256619542837143, \"time-step\": 2319}, {\"accuracy\": 0.75, \"loss\": 0.12565863132476807, \"time-step\": 2320}, {\"accuracy\": 0.75, \"loss\": 0.12566153705120087, \"time-step\": 2321}, {\"accuracy\": 0.75, \"loss\": 0.12565816938877106, \"time-step\": 2322}, {\"accuracy\": 0.75, \"loss\": 0.12566107511520386, \"time-step\": 2323}, {\"accuracy\": 0.75, \"loss\": 0.12565772235393524, \"time-step\": 2324}, {\"accuracy\": 0.75, \"loss\": 0.12566064298152924, \"time-step\": 2325}, {\"accuracy\": 0.75, \"loss\": 0.12565718591213226, \"time-step\": 2326}, {\"accuracy\": 0.75, \"loss\": 0.12566021084785461, \"time-step\": 2327}, {\"accuracy\": 0.75, \"loss\": 0.12565673887729645, \"time-step\": 2328}, {\"accuracy\": 0.75, \"loss\": 0.12565977871418, \"time-step\": 2329}, {\"accuracy\": 0.75, \"loss\": 0.12565626204013824, \"time-step\": 2330}, {\"accuracy\": 0.75, \"loss\": 0.12565933167934418, \"time-step\": 2331}, {\"accuracy\": 0.75, \"loss\": 0.12565578520298004, \"time-step\": 2332}, {\"accuracy\": 0.75, \"loss\": 0.1256588250398636, \"time-step\": 2333}, {\"accuracy\": 0.75, \"loss\": 0.12565533816814423, \"time-step\": 2334}, {\"accuracy\": 0.75, \"loss\": 0.12565837800502777, \"time-step\": 2335}, {\"accuracy\": 0.75, \"loss\": 0.12565478682518005, \"time-step\": 2336}, {\"accuracy\": 0.75, \"loss\": 0.12565797567367554, \"time-step\": 2337}, {\"accuracy\": 0.75, \"loss\": 0.125654399394989, \"time-step\": 2338}, {\"accuracy\": 0.75, \"loss\": 0.12565748393535614, \"time-step\": 2339}, {\"accuracy\": 0.75, \"loss\": 0.12565386295318604, \"time-step\": 2340}, {\"accuracy\": 0.75, \"loss\": 0.1256570667028427, \"time-step\": 2341}, {\"accuracy\": 0.75, \"loss\": 0.12565350532531738, \"time-step\": 2342}, {\"accuracy\": 0.75, \"loss\": 0.1256566047668457, \"time-step\": 2343}, {\"accuracy\": 0.75, \"loss\": 0.12565302848815918, \"time-step\": 2344}, {\"accuracy\": 0.75, \"loss\": 0.12565618753433228, \"time-step\": 2345}, {\"accuracy\": 0.75, \"loss\": 0.12565261125564575, \"time-step\": 2346}, {\"accuracy\": 0.75, \"loss\": 0.1256558746099472, \"time-step\": 2347}, {\"accuracy\": 0.75, \"loss\": 0.12565211951732635, \"time-step\": 2348}, {\"accuracy\": 0.75, \"loss\": 0.12565533816814423, \"time-step\": 2349}, {\"accuracy\": 0.75, \"loss\": 0.12565161287784576, \"time-step\": 2350}, {\"accuracy\": 0.75, \"loss\": 0.1256549209356308, \"time-step\": 2351}, {\"accuracy\": 0.75, \"loss\": 0.12565124034881592, \"time-step\": 2352}, {\"accuracy\": 0.75, \"loss\": 0.12565447390079498, \"time-step\": 2353}, {\"accuracy\": 0.75, \"loss\": 0.12565091252326965, \"time-step\": 2354}, {\"accuracy\": 0.75, \"loss\": 0.12565410137176514, \"time-step\": 2355}, {\"accuracy\": 0.75, \"loss\": 0.12565042078495026, \"time-step\": 2356}, {\"accuracy\": 0.75, \"loss\": 0.12565375864505768, \"time-step\": 2357}, {\"accuracy\": 0.75, \"loss\": 0.12564998865127563, \"time-step\": 2358}, {\"accuracy\": 0.75, \"loss\": 0.1256532371044159, \"time-step\": 2359}, {\"accuracy\": 0.75, \"loss\": 0.12564951181411743, \"time-step\": 2360}, {\"accuracy\": 0.75, \"loss\": 0.12565286457538605, \"time-step\": 2361}, {\"accuracy\": 0.75, \"loss\": 0.12564906477928162, \"time-step\": 2362}, {\"accuracy\": 0.75, \"loss\": 0.125652477145195, \"time-step\": 2363}, {\"accuracy\": 0.75, \"loss\": 0.12564870715141296, \"time-step\": 2364}, {\"accuracy\": 0.75, \"loss\": 0.12565205991268158, \"time-step\": 2365}, {\"accuracy\": 0.75, \"loss\": 0.12564824521541595, \"time-step\": 2366}, {\"accuracy\": 0.75, \"loss\": 0.12565162777900696, \"time-step\": 2367}, {\"accuracy\": 0.75, \"loss\": 0.12564784288406372, \"time-step\": 2368}, {\"accuracy\": 0.75, \"loss\": 0.1256512552499771, \"time-step\": 2369}, {\"accuracy\": 0.75, \"loss\": 0.1256474405527115, \"time-step\": 2370}, {\"accuracy\": 0.75, \"loss\": 0.12565085291862488, \"time-step\": 2371}, {\"accuracy\": 0.75, \"loss\": 0.12564706802368164, \"time-step\": 2372}, {\"accuracy\": 0.75, \"loss\": 0.12565040588378906, \"time-step\": 2373}, {\"accuracy\": 0.75, \"loss\": 0.1256466954946518, \"time-step\": 2374}, {\"accuracy\": 0.75, \"loss\": 0.1256500482559204, \"time-step\": 2375}, {\"accuracy\": 0.75, \"loss\": 0.12564624845981598, \"time-step\": 2376}, {\"accuracy\": 0.75, \"loss\": 0.12564966082572937, \"time-step\": 2377}, {\"accuracy\": 0.75, \"loss\": 0.12564578652381897, \"time-step\": 2378}, {\"accuracy\": 0.75, \"loss\": 0.12564930319786072, \"time-step\": 2379}, {\"accuracy\": 0.75, \"loss\": 0.12564535439014435, \"time-step\": 2380}, {\"accuracy\": 0.75, \"loss\": 0.12564893066883087, \"time-step\": 2381}, {\"accuracy\": 0.75, \"loss\": 0.12564502656459808, \"time-step\": 2382}, {\"accuracy\": 0.75, \"loss\": 0.12564849853515625, \"time-step\": 2383}, {\"accuracy\": 0.75, \"loss\": 0.12564463913440704, \"time-step\": 2384}, {\"accuracy\": 0.75, \"loss\": 0.12564809620380402, \"time-step\": 2385}, {\"accuracy\": 0.75, \"loss\": 0.1256442666053772, \"time-step\": 2386}, {\"accuracy\": 0.75, \"loss\": 0.12564778327941895, \"time-step\": 2387}, {\"accuracy\": 0.75, \"loss\": 0.1256437748670578, \"time-step\": 2388}, {\"accuracy\": 0.75, \"loss\": 0.12564735114574432, \"time-step\": 2389}, {\"accuracy\": 0.75, \"loss\": 0.12564349174499512, \"time-step\": 2390}, {\"accuracy\": 0.75, \"loss\": 0.12564703822135925, \"time-step\": 2391}, {\"accuracy\": 0.75, \"loss\": 0.12564310431480408, \"time-step\": 2392}, {\"accuracy\": 0.75, \"loss\": 0.1256466507911682, \"time-step\": 2393}, {\"accuracy\": 0.75, \"loss\": 0.12564271688461304, \"time-step\": 2394}, {\"accuracy\": 0.75, \"loss\": 0.12564635276794434, \"time-step\": 2395}, {\"accuracy\": 0.75, \"loss\": 0.1256423145532608, \"time-step\": 2396}, {\"accuracy\": 0.75, \"loss\": 0.12564599514007568, \"time-step\": 2397}, {\"accuracy\": 0.75, \"loss\": 0.12564191222190857, \"time-step\": 2398}, {\"accuracy\": 0.75, \"loss\": 0.12564559280872345, \"time-step\": 2399}, {\"accuracy\": 0.75, \"loss\": 0.1256415992975235, \"time-step\": 2400}, {\"accuracy\": 0.75, \"loss\": 0.12564519047737122, \"time-step\": 2401}, {\"accuracy\": 0.75, \"loss\": 0.12564116716384888, \"time-step\": 2402}, {\"accuracy\": 0.75, \"loss\": 0.12564495205879211, \"time-step\": 2403}, {\"accuracy\": 0.75, \"loss\": 0.12564080953598022, \"time-step\": 2404}, {\"accuracy\": 0.75, \"loss\": 0.1256444752216339, \"time-step\": 2405}, {\"accuracy\": 0.75, \"loss\": 0.1256403774023056, \"time-step\": 2406}, {\"accuracy\": 0.75, \"loss\": 0.12564407289028168, \"time-step\": 2407}, {\"accuracy\": 0.75, \"loss\": 0.12564001977443695, \"time-step\": 2408}, {\"accuracy\": 0.75, \"loss\": 0.12564383447170258, \"time-step\": 2409}, {\"accuracy\": 0.75, \"loss\": 0.12563972175121307, \"time-step\": 2410}, {\"accuracy\": 0.75, \"loss\": 0.1256435066461563, \"time-step\": 2411}, {\"accuracy\": 0.75, \"loss\": 0.12563936412334442, \"time-step\": 2412}, {\"accuracy\": 0.75, \"loss\": 0.12564310431480408, \"time-step\": 2413}, {\"accuracy\": 0.75, \"loss\": 0.1256389319896698, \"time-step\": 2414}, {\"accuracy\": 0.75, \"loss\": 0.125642791390419, \"time-step\": 2415}, {\"accuracy\": 0.75, \"loss\": 0.12563863396644592, \"time-step\": 2416}, {\"accuracy\": 0.75, \"loss\": 0.12564240396022797, \"time-step\": 2417}, {\"accuracy\": 0.75, \"loss\": 0.1256382018327713, \"time-step\": 2418}, {\"accuracy\": 0.75, \"loss\": 0.12564200162887573, \"time-step\": 2419}, {\"accuracy\": 0.75, \"loss\": 0.125637948513031, \"time-step\": 2420}, {\"accuracy\": 0.75, \"loss\": 0.12564174830913544, \"time-step\": 2421}, {\"accuracy\": 0.75, \"loss\": 0.12563753128051758, \"time-step\": 2422}, {\"accuracy\": 0.75, \"loss\": 0.12564148008823395, \"time-step\": 2423}, {\"accuracy\": 0.75, \"loss\": 0.1256372332572937, \"time-step\": 2424}, {\"accuracy\": 0.75, \"loss\": 0.12564115226268768, \"time-step\": 2425}, {\"accuracy\": 0.75, \"loss\": 0.12563689053058624, \"time-step\": 2426}, {\"accuracy\": 0.75, \"loss\": 0.12564074993133545, \"time-step\": 2427}, {\"accuracy\": 0.75, \"loss\": 0.12563656270503998, \"time-step\": 2428}, {\"accuracy\": 0.75, \"loss\": 0.12564054131507874, \"time-step\": 2429}, {\"accuracy\": 0.75, \"loss\": 0.12563621997833252, \"time-step\": 2430}, {\"accuracy\": 0.75, \"loss\": 0.1256401687860489, \"time-step\": 2431}, {\"accuracy\": 0.75, \"loss\": 0.12563586235046387, \"time-step\": 2432}, {\"accuracy\": 0.75, \"loss\": 0.12563975155353546, \"time-step\": 2433}, {\"accuracy\": 0.75, \"loss\": 0.1256355345249176, \"time-step\": 2434}, {\"accuracy\": 0.75, \"loss\": 0.12563948333263397, \"time-step\": 2435}, {\"accuracy\": 0.75, \"loss\": 0.12563522160053253, \"time-step\": 2436}, {\"accuracy\": 0.75, \"loss\": 0.12563927471637726, \"time-step\": 2437}, {\"accuracy\": 0.75, \"loss\": 0.12563487887382507, \"time-step\": 2438}, {\"accuracy\": 0.75, \"loss\": 0.12563888728618622, \"time-step\": 2439}, {\"accuracy\": 0.75, \"loss\": 0.1256345808506012, \"time-step\": 2440}, {\"accuracy\": 0.75, \"loss\": 0.12563864886760712, \"time-step\": 2441}, {\"accuracy\": 0.75, \"loss\": 0.12563416361808777, \"time-step\": 2442}, {\"accuracy\": 0.75, \"loss\": 0.12563829123973846, \"time-step\": 2443}, {\"accuracy\": 0.75, \"loss\": 0.12563392519950867, \"time-step\": 2444}, {\"accuracy\": 0.75, \"loss\": 0.1256379336118698, \"time-step\": 2445}, {\"accuracy\": 0.75, \"loss\": 0.12563349306583405, \"time-step\": 2446}, {\"accuracy\": 0.75, \"loss\": 0.12563760578632355, \"time-step\": 2447}, {\"accuracy\": 0.75, \"loss\": 0.12563323974609375, \"time-step\": 2448}, {\"accuracy\": 0.75, \"loss\": 0.12563736736774445, \"time-step\": 2449}, {\"accuracy\": 0.75, \"loss\": 0.12563291192054749, \"time-step\": 2450}, {\"accuracy\": 0.75, \"loss\": 0.12563709914684296, \"time-step\": 2451}, {\"accuracy\": 0.75, \"loss\": 0.12563259899616241, \"time-step\": 2452}, {\"accuracy\": 0.75, \"loss\": 0.12563680112361908, \"time-step\": 2453}, {\"accuracy\": 0.75, \"loss\": 0.12563231587409973, \"time-step\": 2454}, {\"accuracy\": 0.75, \"loss\": 0.12563642859458923, \"time-step\": 2455}, {\"accuracy\": 0.75, \"loss\": 0.12563203275203705, \"time-step\": 2456}, {\"accuracy\": 0.75, \"loss\": 0.12563619017601013, \"time-step\": 2457}, {\"accuracy\": 0.75, \"loss\": 0.12563170492649078, \"time-step\": 2458}, {\"accuracy\": 0.75, \"loss\": 0.12563586235046387, \"time-step\": 2459}, {\"accuracy\": 0.75, \"loss\": 0.1256314069032669, \"time-step\": 2460}, {\"accuracy\": 0.75, \"loss\": 0.12563563883304596, \"time-step\": 2461}, {\"accuracy\": 0.75, \"loss\": 0.12563113868236542, \"time-step\": 2462}, {\"accuracy\": 0.75, \"loss\": 0.1256352961063385, \"time-step\": 2463}, {\"accuracy\": 0.75, \"loss\": 0.125630721449852, \"time-step\": 2464}, {\"accuracy\": 0.75, \"loss\": 0.125635027885437, \"time-step\": 2465}, {\"accuracy\": 0.75, \"loss\": 0.1256304681301117, \"time-step\": 2466}, {\"accuracy\": 0.75, \"loss\": 0.12563471496105194, \"time-step\": 2467}, {\"accuracy\": 0.75, \"loss\": 0.125630185008049, \"time-step\": 2468}, {\"accuracy\": 0.75, \"loss\": 0.12563441693782806, \"time-step\": 2469}, {\"accuracy\": 0.75, \"loss\": 0.12562993168830872, \"time-step\": 2470}, {\"accuracy\": 0.75, \"loss\": 0.12563422322273254, \"time-step\": 2471}, {\"accuracy\": 0.75, \"loss\": 0.12562960386276245, \"time-step\": 2472}, {\"accuracy\": 0.75, \"loss\": 0.12563388049602509, \"time-step\": 2473}, {\"accuracy\": 0.75, \"loss\": 0.1256292313337326, \"time-step\": 2474}, {\"accuracy\": 0.75, \"loss\": 0.12563365697860718, \"time-step\": 2475}, {\"accuracy\": 0.75, \"loss\": 0.12562891840934753, \"time-step\": 2476}, {\"accuracy\": 0.75, \"loss\": 0.1256333440542221, \"time-step\": 2477}, {\"accuracy\": 0.75, \"loss\": 0.12562865018844604, \"time-step\": 2478}, {\"accuracy\": 0.75, \"loss\": 0.125633105635643, \"time-step\": 2479}, {\"accuracy\": 0.75, \"loss\": 0.12562838196754456, \"time-step\": 2480}, {\"accuracy\": 0.75, \"loss\": 0.1256328821182251, \"time-step\": 2481}, {\"accuracy\": 0.75, \"loss\": 0.12562806904315948, \"time-step\": 2482}, {\"accuracy\": 0.75, \"loss\": 0.12563258409500122, \"time-step\": 2483}, {\"accuracy\": 0.75, \"loss\": 0.12562784552574158, \"time-step\": 2484}, {\"accuracy\": 0.75, \"loss\": 0.12563225626945496, \"time-step\": 2485}, {\"accuracy\": 0.75, \"loss\": 0.1256275177001953, \"time-step\": 2486}, {\"accuracy\": 0.75, \"loss\": 0.12563198804855347, \"time-step\": 2487}, {\"accuracy\": 0.75, \"loss\": 0.1256272941827774, \"time-step\": 2488}, {\"accuracy\": 0.75, \"loss\": 0.12563179433345795, \"time-step\": 2489}, {\"accuracy\": 0.75, \"loss\": 0.12562699615955353, \"time-step\": 2490}, {\"accuracy\": 0.75, \"loss\": 0.12563152611255646, \"time-step\": 2491}, {\"accuracy\": 0.75, \"loss\": 0.12562674283981323, \"time-step\": 2492}, {\"accuracy\": 0.75, \"loss\": 0.12563125789165497, \"time-step\": 2493}, {\"accuracy\": 0.75, \"loss\": 0.12562642991542816, \"time-step\": 2494}, {\"accuracy\": 0.75, \"loss\": 0.12563103437423706, \"time-step\": 2495}, {\"accuracy\": 0.75, \"loss\": 0.1256261169910431, \"time-step\": 2496}, {\"accuracy\": 0.75, \"loss\": 0.12563073635101318, \"time-step\": 2497}, {\"accuracy\": 0.75, \"loss\": 0.12562596797943115, \"time-step\": 2498}, {\"accuracy\": 0.75, \"loss\": 0.12563052773475647, \"time-step\": 2499}, {\"accuracy\": 0.75, \"loss\": 0.12562568485736847, \"time-step\": 2500}, {\"accuracy\": 0.75, \"loss\": 0.1256302297115326, \"time-step\": 2501}, {\"accuracy\": 0.75, \"loss\": 0.1256253570318222, \"time-step\": 2502}, {\"accuracy\": 0.75, \"loss\": 0.12563006579875946, \"time-step\": 2503}, {\"accuracy\": 0.75, \"loss\": 0.1256251037120819, \"time-step\": 2504}, {\"accuracy\": 0.75, \"loss\": 0.12562979757785797, \"time-step\": 2505}, {\"accuracy\": 0.75, \"loss\": 0.12562483549118042, \"time-step\": 2506}, {\"accuracy\": 0.75, \"loss\": 0.12562957406044006, \"time-step\": 2507}, {\"accuracy\": 0.75, \"loss\": 0.12562452256679535, \"time-step\": 2508}, {\"accuracy\": 0.75, \"loss\": 0.1256292164325714, \"time-step\": 2509}, {\"accuracy\": 0.75, \"loss\": 0.12562435865402222, \"time-step\": 2510}, {\"accuracy\": 0.75, \"loss\": 0.1256289929151535, \"time-step\": 2511}, {\"accuracy\": 0.75, \"loss\": 0.12562410533428192, \"time-step\": 2512}, {\"accuracy\": 0.75, \"loss\": 0.1256287842988968, \"time-step\": 2513}, {\"accuracy\": 0.75, \"loss\": 0.125623881816864, \"time-step\": 2514}, {\"accuracy\": 0.75, \"loss\": 0.1256285011768341, \"time-step\": 2515}, {\"accuracy\": 0.75, \"loss\": 0.12562356889247894, \"time-step\": 2516}, {\"accuracy\": 0.75, \"loss\": 0.1256282925605774, \"time-step\": 2517}, {\"accuracy\": 0.75, \"loss\": 0.12562334537506104, \"time-step\": 2518}, {\"accuracy\": 0.75, \"loss\": 0.12562806904315948, \"time-step\": 2519}, {\"accuracy\": 0.75, \"loss\": 0.12562312185764313, \"time-step\": 2520}, {\"accuracy\": 0.75, \"loss\": 0.12562790513038635, \"time-step\": 2521}, {\"accuracy\": 0.75, \"loss\": 0.12562288343906403, \"time-step\": 2522}, {\"accuracy\": 0.75, \"loss\": 0.12562769651412964, \"time-step\": 2523}, {\"accuracy\": 0.75, \"loss\": 0.12562257051467896, \"time-step\": 2524}, {\"accuracy\": 0.75, \"loss\": 0.12562741339206696, \"time-step\": 2525}, {\"accuracy\": 0.75, \"loss\": 0.12562237679958344, \"time-step\": 2526}, {\"accuracy\": 0.75, \"loss\": 0.12562721967697144, \"time-step\": 2527}, {\"accuracy\": 0.75, \"loss\": 0.12562212347984314, \"time-step\": 2528}, {\"accuracy\": 0.75, \"loss\": 0.12562690675258636, \"time-step\": 2529}, {\"accuracy\": 0.75, \"loss\": 0.12562188506126404, \"time-step\": 2530}, {\"accuracy\": 0.75, \"loss\": 0.1256268322467804, \"time-step\": 2531}, {\"accuracy\": 0.75, \"loss\": 0.12562161684036255, \"time-step\": 2532}, {\"accuracy\": 0.75, \"loss\": 0.1256265491247177, \"time-step\": 2533}, {\"accuracy\": 0.75, \"loss\": 0.12562134861946106, \"time-step\": 2534}, {\"accuracy\": 0.75, \"loss\": 0.12562641501426697, \"time-step\": 2535}, {\"accuracy\": 0.75, \"loss\": 0.12562116980552673, \"time-step\": 2536}, {\"accuracy\": 0.75, \"loss\": 0.1256261169910431, \"time-step\": 2537}, {\"accuracy\": 0.75, \"loss\": 0.12562096118927002, \"time-step\": 2538}, {\"accuracy\": 0.75, \"loss\": 0.12562596797943115, \"time-step\": 2539}, {\"accuracy\": 0.75, \"loss\": 0.12562069296836853, \"time-step\": 2540}, {\"accuracy\": 0.75, \"loss\": 0.12562565505504608, \"time-step\": 2541}, {\"accuracy\": 0.75, \"loss\": 0.1256205141544342, \"time-step\": 2542}, {\"accuracy\": 0.75, \"loss\": 0.12562552094459534, \"time-step\": 2543}, {\"accuracy\": 0.75, \"loss\": 0.12562023103237152, \"time-step\": 2544}, {\"accuracy\": 0.75, \"loss\": 0.12562529742717743, \"time-step\": 2545}, {\"accuracy\": 0.75, \"loss\": 0.12561999261379242, \"time-step\": 2546}, {\"accuracy\": 0.75, \"loss\": 0.12562508881092072, \"time-step\": 2547}, {\"accuracy\": 0.75, \"loss\": 0.1256197690963745, \"time-step\": 2548}, {\"accuracy\": 0.75, \"loss\": 0.125624880194664, \"time-step\": 2549}, {\"accuracy\": 0.75, \"loss\": 0.12561959028244019, \"time-step\": 2550}, {\"accuracy\": 0.75, \"loss\": 0.12562459707260132, \"time-step\": 2551}, {\"accuracy\": 0.75, \"loss\": 0.12561938166618347, \"time-step\": 2552}, {\"accuracy\": 0.75, \"loss\": 0.12562443315982819, \"time-step\": 2553}, {\"accuracy\": 0.75, \"loss\": 0.12561917304992676, \"time-step\": 2554}, {\"accuracy\": 0.75, \"loss\": 0.12562429904937744, \"time-step\": 2555}, {\"accuracy\": 0.75, \"loss\": 0.12561893463134766, \"time-step\": 2556}, {\"accuracy\": 0.75, \"loss\": 0.12562407553195953, \"time-step\": 2557}, {\"accuracy\": 0.75, \"loss\": 0.12561868131160736, \"time-step\": 2558}, {\"accuracy\": 0.75, \"loss\": 0.12562379240989685, \"time-step\": 2559}, {\"accuracy\": 0.75, \"loss\": 0.12561842799186707, \"time-step\": 2560}, {\"accuracy\": 0.75, \"loss\": 0.12562362849712372, \"time-step\": 2561}, {\"accuracy\": 0.75, \"loss\": 0.12561821937561035, \"time-step\": 2562}, {\"accuracy\": 0.75, \"loss\": 0.12562350928783417, \"time-step\": 2563}, {\"accuracy\": 0.75, \"loss\": 0.12561805546283722, \"time-step\": 2564}, {\"accuracy\": 0.75, \"loss\": 0.12562325596809387, \"time-step\": 2565}, {\"accuracy\": 0.75, \"loss\": 0.1256178617477417, \"time-step\": 2566}, {\"accuracy\": 0.75, \"loss\": 0.12562303245067596, \"time-step\": 2567}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 2568}, {\"accuracy\": 0.75, \"loss\": 0.12562289834022522, \"time-step\": 2569}, {\"accuracy\": 0.75, \"loss\": 0.12561747431755066, \"time-step\": 2570}, {\"accuracy\": 0.75, \"loss\": 0.1256226897239685, \"time-step\": 2571}, {\"accuracy\": 0.75, \"loss\": 0.12561720609664917, \"time-step\": 2572}, {\"accuracy\": 0.75, \"loss\": 0.1256224811077118, \"time-step\": 2573}, {\"accuracy\": 0.75, \"loss\": 0.12561702728271484, \"time-step\": 2574}, {\"accuracy\": 0.75, \"loss\": 0.12562231719493866, \"time-step\": 2575}, {\"accuracy\": 0.75, \"loss\": 0.12561677396297455, \"time-step\": 2576}, {\"accuracy\": 0.75, \"loss\": 0.12562213838100433, \"time-step\": 2577}, {\"accuracy\": 0.75, \"loss\": 0.1256166249513626, \"time-step\": 2578}, {\"accuracy\": 0.75, \"loss\": 0.12562201917171478, \"time-step\": 2579}, {\"accuracy\": 0.75, \"loss\": 0.12561646103858948, \"time-step\": 2580}, {\"accuracy\": 0.75, \"loss\": 0.12562178075313568, \"time-step\": 2581}, {\"accuracy\": 0.75, \"loss\": 0.12561631202697754, \"time-step\": 2582}, {\"accuracy\": 0.75, \"loss\": 0.12562158703804016, \"time-step\": 2583}, {\"accuracy\": 0.75, \"loss\": 0.12561596930027008, \"time-step\": 2584}, {\"accuracy\": 0.75, \"loss\": 0.12562143802642822, \"time-step\": 2585}, {\"accuracy\": 0.75, \"loss\": 0.12561579048633575, \"time-step\": 2586}, {\"accuracy\": 0.75, \"loss\": 0.1256212294101715, \"time-step\": 2587}, {\"accuracy\": 0.75, \"loss\": 0.12561562657356262, \"time-step\": 2588}, {\"accuracy\": 0.75, \"loss\": 0.12562109529972076, \"time-step\": 2589}, {\"accuracy\": 0.75, \"loss\": 0.12561547756195068, \"time-step\": 2590}, {\"accuracy\": 0.75, \"loss\": 0.12562090158462524, \"time-step\": 2591}, {\"accuracy\": 0.75, \"loss\": 0.12561528384685516, \"time-step\": 2592}, {\"accuracy\": 0.75, \"loss\": 0.1256207525730133, \"time-step\": 2593}, {\"accuracy\": 0.75, \"loss\": 0.12561506032943726, \"time-step\": 2594}, {\"accuracy\": 0.75, \"loss\": 0.1256205439567566, \"time-step\": 2595}, {\"accuracy\": 0.75, \"loss\": 0.12561485171318054, \"time-step\": 2596}, {\"accuracy\": 0.75, \"loss\": 0.12562036514282227, \"time-step\": 2597}, {\"accuracy\": 0.75, \"loss\": 0.12561467289924622, \"time-step\": 2598}, {\"accuracy\": 0.75, \"loss\": 0.1256202906370163, \"time-step\": 2599}, {\"accuracy\": 0.75, \"loss\": 0.1256144642829895, \"time-step\": 2600}, {\"accuracy\": 0.75, \"loss\": 0.12562009692192078, \"time-step\": 2601}, {\"accuracy\": 0.75, \"loss\": 0.12561434507369995, \"time-step\": 2602}, {\"accuracy\": 0.75, \"loss\": 0.1256200075149536, \"time-step\": 2603}, {\"accuracy\": 0.75, \"loss\": 0.12561409175395966, \"time-step\": 2604}, {\"accuracy\": 0.75, \"loss\": 0.12561987340450287, \"time-step\": 2605}, {\"accuracy\": 0.75, \"loss\": 0.1256139576435089, \"time-step\": 2606}, {\"accuracy\": 0.75, \"loss\": 0.1256195455789566, \"time-step\": 2607}, {\"accuracy\": 0.75, \"loss\": 0.12561377882957458, \"time-step\": 2608}, {\"accuracy\": 0.75, \"loss\": 0.12561936676502228, \"time-step\": 2609}, {\"accuracy\": 0.75, \"loss\": 0.12561360001564026, \"time-step\": 2610}, {\"accuracy\": 0.75, \"loss\": 0.12561927735805511, \"time-step\": 2611}, {\"accuracy\": 0.75, \"loss\": 0.12561345100402832, \"time-step\": 2612}, {\"accuracy\": 0.75, \"loss\": 0.12561911344528198, \"time-step\": 2613}, {\"accuracy\": 0.75, \"loss\": 0.125613272190094, \"time-step\": 2614}, {\"accuracy\": 0.75, \"loss\": 0.12561891973018646, \"time-step\": 2615}, {\"accuracy\": 0.75, \"loss\": 0.12561307847499847, \"time-step\": 2616}, {\"accuracy\": 0.75, \"loss\": 0.1256188154220581, \"time-step\": 2617}, {\"accuracy\": 0.75, \"loss\": 0.12561282515525818, \"time-step\": 2618}, {\"accuracy\": 0.75, \"loss\": 0.12561871111392975, \"time-step\": 2619}, {\"accuracy\": 0.75, \"loss\": 0.12561272084712982, \"time-step\": 2620}, {\"accuracy\": 0.75, \"loss\": 0.1256185621023178, \"time-step\": 2621}, {\"accuracy\": 0.75, \"loss\": 0.12561258673667908, \"time-step\": 2622}, {\"accuracy\": 0.75, \"loss\": 0.1256183683872223, \"time-step\": 2623}, {\"accuracy\": 0.75, \"loss\": 0.1256123185157776, \"time-step\": 2624}, {\"accuracy\": 0.75, \"loss\": 0.12561824917793274, \"time-step\": 2625}, {\"accuracy\": 0.75, \"loss\": 0.12561219930648804, \"time-step\": 2626}, {\"accuracy\": 0.75, \"loss\": 0.1256180703639984, \"time-step\": 2627}, {\"accuracy\": 0.75, \"loss\": 0.12561199069023132, \"time-step\": 2628}, {\"accuracy\": 0.75, \"loss\": 0.1256178915500641, \"time-step\": 2629}, {\"accuracy\": 0.75, \"loss\": 0.12561190128326416, \"time-step\": 2630}, {\"accuracy\": 0.75, \"loss\": 0.12561769783496857, \"time-step\": 2631}, {\"accuracy\": 0.75, \"loss\": 0.12561172246932983, \"time-step\": 2632}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 2633}, {\"accuracy\": 0.75, \"loss\": 0.12561164796352386, \"time-step\": 2634}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 2635}, {\"accuracy\": 0.75, \"loss\": 0.12561142444610596, \"time-step\": 2636}, {\"accuracy\": 0.75, \"loss\": 0.1256173551082611, \"time-step\": 2637}, {\"accuracy\": 0.75, \"loss\": 0.1256113052368164, \"time-step\": 2638}, {\"accuracy\": 0.75, \"loss\": 0.12561719119548798, \"time-step\": 2639}, {\"accuracy\": 0.75, \"loss\": 0.1256110966205597, \"time-step\": 2640}, {\"accuracy\": 0.75, \"loss\": 0.125617116689682, \"time-step\": 2641}, {\"accuracy\": 0.75, \"loss\": 0.12561088800430298, \"time-step\": 2642}, {\"accuracy\": 0.75, \"loss\": 0.12561693787574768, \"time-step\": 2643}, {\"accuracy\": 0.75, \"loss\": 0.12561079859733582, \"time-step\": 2644}, {\"accuracy\": 0.75, \"loss\": 0.12561681866645813, \"time-step\": 2645}, {\"accuracy\": 0.75, \"loss\": 0.12561073899269104, \"time-step\": 2646}, {\"accuracy\": 0.75, \"loss\": 0.125616654753685, \"time-step\": 2647}, {\"accuracy\": 0.75, \"loss\": 0.12561048567295074, \"time-step\": 2648}, {\"accuracy\": 0.75, \"loss\": 0.12561658024787903, \"time-step\": 2649}, {\"accuracy\": 0.75, \"loss\": 0.12561041116714478, \"time-step\": 2650}, {\"accuracy\": 0.75, \"loss\": 0.12561647593975067, \"time-step\": 2651}, {\"accuracy\": 0.75, \"loss\": 0.12561023235321045, \"time-step\": 2652}, {\"accuracy\": 0.75, \"loss\": 0.12561634182929993, \"time-step\": 2653}, {\"accuracy\": 0.75, \"loss\": 0.12561005353927612, \"time-step\": 2654}, {\"accuracy\": 0.75, \"loss\": 0.1256161779165268, \"time-step\": 2655}, {\"accuracy\": 0.75, \"loss\": 0.12560991942882538, \"time-step\": 2656}, {\"accuracy\": 0.75, \"loss\": 0.12561599910259247, \"time-step\": 2657}, {\"accuracy\": 0.75, \"loss\": 0.12560975551605225, \"time-step\": 2658}, {\"accuracy\": 0.75, \"loss\": 0.1256159096956253, \"time-step\": 2659}, {\"accuracy\": 0.75, \"loss\": 0.1256096214056015, \"time-step\": 2660}, {\"accuracy\": 0.75, \"loss\": 0.12561577558517456, \"time-step\": 2661}, {\"accuracy\": 0.75, \"loss\": 0.12560947239398956, \"time-step\": 2662}, {\"accuracy\": 0.75, \"loss\": 0.1256157010793686, \"time-step\": 2663}, {\"accuracy\": 0.75, \"loss\": 0.12560933828353882, \"time-step\": 2664}, {\"accuracy\": 0.75, \"loss\": 0.12561552226543427, \"time-step\": 2665}, {\"accuracy\": 0.75, \"loss\": 0.1256091147661209, \"time-step\": 2666}, {\"accuracy\": 0.75, \"loss\": 0.12561535835266113, \"time-step\": 2667}, {\"accuracy\": 0.75, \"loss\": 0.12560908496379852, \"time-step\": 2668}, {\"accuracy\": 0.75, \"loss\": 0.12561534345149994, \"time-step\": 2669}, {\"accuracy\": 0.75, \"loss\": 0.12560893595218658, \"time-step\": 2670}, {\"accuracy\": 0.75, \"loss\": 0.1256151646375656, \"time-step\": 2671}, {\"accuracy\": 0.75, \"loss\": 0.12560875713825226, \"time-step\": 2672}, {\"accuracy\": 0.75, \"loss\": 0.12561509013175964, \"time-step\": 2673}, {\"accuracy\": 0.75, \"loss\": 0.12560872733592987, \"time-step\": 2674}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 2675}, {\"accuracy\": 0.75, \"loss\": 0.12560848891735077, \"time-step\": 2676}, {\"accuracy\": 0.75, \"loss\": 0.12561482191085815, \"time-step\": 2677}, {\"accuracy\": 0.75, \"loss\": 0.12560836970806122, \"time-step\": 2678}, {\"accuracy\": 0.75, \"loss\": 0.12561465799808502, \"time-step\": 2679}, {\"accuracy\": 0.75, \"loss\": 0.12560820579528809, \"time-step\": 2680}, {\"accuracy\": 0.75, \"loss\": 0.12561465799808502, \"time-step\": 2681}, {\"accuracy\": 0.75, \"loss\": 0.12560805678367615, \"time-step\": 2682}, {\"accuracy\": 0.75, \"loss\": 0.12561458349227905, \"time-step\": 2683}, {\"accuracy\": 0.75, \"loss\": 0.12560798227787018, \"time-step\": 2684}, {\"accuracy\": 0.75, \"loss\": 0.12561438977718353, \"time-step\": 2685}, {\"accuracy\": 0.75, \"loss\": 0.1256079077720642, \"time-step\": 2686}, {\"accuracy\": 0.75, \"loss\": 0.12561434507369995, \"time-step\": 2687}, {\"accuracy\": 0.75, \"loss\": 0.1256077140569687, \"time-step\": 2688}, {\"accuracy\": 0.75, \"loss\": 0.12561416625976562, \"time-step\": 2689}, {\"accuracy\": 0.75, \"loss\": 0.12560759484767914, \"time-step\": 2690}, {\"accuracy\": 0.75, \"loss\": 0.12561409175395966, \"time-step\": 2691}, {\"accuracy\": 0.75, \"loss\": 0.1256074458360672, \"time-step\": 2692}, {\"accuracy\": 0.75, \"loss\": 0.12561392784118652, \"time-step\": 2693}, {\"accuracy\": 0.75, \"loss\": 0.12560734152793884, \"time-step\": 2694}, {\"accuracy\": 0.75, \"loss\": 0.12561385333538055, \"time-step\": 2695}, {\"accuracy\": 0.75, \"loss\": 0.1256072223186493, \"time-step\": 2696}, {\"accuracy\": 0.75, \"loss\": 0.12561377882957458, \"time-step\": 2697}, {\"accuracy\": 0.75, \"loss\": 0.12560707330703735, \"time-step\": 2698}, {\"accuracy\": 0.75, \"loss\": 0.12561367452144623, \"time-step\": 2699}, {\"accuracy\": 0.75, \"loss\": 0.12560692429542542, \"time-step\": 2700}, {\"accuracy\": 0.75, \"loss\": 0.1256134957075119, \"time-step\": 2701}, {\"accuracy\": 0.75, \"loss\": 0.12560690939426422, \"time-step\": 2702}, {\"accuracy\": 0.75, \"loss\": 0.12561346590518951, \"time-step\": 2703}, {\"accuracy\": 0.75, \"loss\": 0.1256067305803299, \"time-step\": 2704}, {\"accuracy\": 0.75, \"loss\": 0.12561339139938354, \"time-step\": 2705}, {\"accuracy\": 0.75, \"loss\": 0.12560662627220154, \"time-step\": 2706}, {\"accuracy\": 0.75, \"loss\": 0.12561333179473877, \"time-step\": 2707}, {\"accuracy\": 0.75, \"loss\": 0.12560643255710602, \"time-step\": 2708}, {\"accuracy\": 0.75, \"loss\": 0.12561321258544922, \"time-step\": 2709}, {\"accuracy\": 0.75, \"loss\": 0.12560635805130005, \"time-step\": 2710}, {\"accuracy\": 0.75, \"loss\": 0.1256130337715149, \"time-step\": 2711}, {\"accuracy\": 0.75, \"loss\": 0.12560629844665527, \"time-step\": 2712}, {\"accuracy\": 0.75, \"loss\": 0.1256130039691925, \"time-step\": 2713}, {\"accuracy\": 0.75, \"loss\": 0.12560610473155975, \"time-step\": 2714}, {\"accuracy\": 0.75, \"loss\": 0.12561288475990295, \"time-step\": 2715}, {\"accuracy\": 0.75, \"loss\": 0.12560604512691498, \"time-step\": 2716}, {\"accuracy\": 0.75, \"loss\": 0.1256127953529358, \"time-step\": 2717}, {\"accuracy\": 0.75, \"loss\": 0.12560591101646423, \"time-step\": 2718}, {\"accuracy\": 0.75, \"loss\": 0.12561272084712982, \"time-step\": 2719}, {\"accuracy\": 0.75, \"loss\": 0.1256057620048523, \"time-step\": 2720}, {\"accuracy\": 0.75, \"loss\": 0.12561266124248505, \"time-step\": 2721}, {\"accuracy\": 0.75, \"loss\": 0.1256057471036911, \"time-step\": 2722}, {\"accuracy\": 0.75, \"loss\": 0.1256125271320343, \"time-step\": 2723}, {\"accuracy\": 0.75, \"loss\": 0.12560556828975677, \"time-step\": 2724}, {\"accuracy\": 0.75, \"loss\": 0.12561248242855072, \"time-step\": 2725}, {\"accuracy\": 0.75, \"loss\": 0.12560546398162842, \"time-step\": 2726}, {\"accuracy\": 0.75, \"loss\": 0.12561237812042236, \"time-step\": 2727}, {\"accuracy\": 0.75, \"loss\": 0.12560540437698364, \"time-step\": 2728}, {\"accuracy\": 0.75, \"loss\": 0.1256123185157776, \"time-step\": 2729}, {\"accuracy\": 0.75, \"loss\": 0.12560522556304932, \"time-step\": 2730}, {\"accuracy\": 0.75, \"loss\": 0.12561210989952087, \"time-step\": 2731}, {\"accuracy\": 0.75, \"loss\": 0.12560513615608215, \"time-step\": 2732}, {\"accuracy\": 0.75, \"loss\": 0.12561215460300446, \"time-step\": 2733}, {\"accuracy\": 0.75, \"loss\": 0.12560507655143738, \"time-step\": 2734}, {\"accuracy\": 0.75, \"loss\": 0.1256120204925537, \"time-step\": 2735}, {\"accuracy\": 0.75, \"loss\": 0.12560494244098663, \"time-step\": 2736}, {\"accuracy\": 0.75, \"loss\": 0.12561196088790894, \"time-step\": 2737}, {\"accuracy\": 0.75, \"loss\": 0.12560488283634186, \"time-step\": 2738}, {\"accuracy\": 0.75, \"loss\": 0.12561187148094177, \"time-step\": 2739}, {\"accuracy\": 0.75, \"loss\": 0.1256047636270523, \"time-step\": 2740}, {\"accuracy\": 0.75, \"loss\": 0.12561172246932983, \"time-step\": 2741}, {\"accuracy\": 0.75, \"loss\": 0.12560471892356873, \"time-step\": 2742}, {\"accuracy\": 0.75, \"loss\": 0.12561172246932983, \"time-step\": 2743}, {\"accuracy\": 0.75, \"loss\": 0.12560461461544037, \"time-step\": 2744}, {\"accuracy\": 0.75, \"loss\": 0.12561164796352386, \"time-step\": 2745}, {\"accuracy\": 0.75, \"loss\": 0.12560445070266724, \"time-step\": 2746}, {\"accuracy\": 0.75, \"loss\": 0.1256115436553955, \"time-step\": 2747}, {\"accuracy\": 0.75, \"loss\": 0.12560439109802246, \"time-step\": 2748}, {\"accuracy\": 0.75, \"loss\": 0.12561145424842834, \"time-step\": 2749}, {\"accuracy\": 0.75, \"loss\": 0.12560424208641052, \"time-step\": 2750}, {\"accuracy\": 0.75, \"loss\": 0.12561139464378357, \"time-step\": 2751}, {\"accuracy\": 0.75, \"loss\": 0.12560416758060455, \"time-step\": 2752}, {\"accuracy\": 0.75, \"loss\": 0.1256113201379776, \"time-step\": 2753}, {\"accuracy\": 0.75, \"loss\": 0.12560412287712097, \"time-step\": 2754}, {\"accuracy\": 0.75, \"loss\": 0.12561121582984924, \"time-step\": 2755}, {\"accuracy\": 0.75, \"loss\": 0.12560400366783142, \"time-step\": 2756}, {\"accuracy\": 0.75, \"loss\": 0.1256110966205597, \"time-step\": 2757}, {\"accuracy\": 0.75, \"loss\": 0.1256038248538971, \"time-step\": 2758}, {\"accuracy\": 0.75, \"loss\": 0.12561118602752686, \"time-step\": 2759}, {\"accuracy\": 0.75, \"loss\": 0.1256038099527359, \"time-step\": 2760}, {\"accuracy\": 0.75, \"loss\": 0.12561103701591492, \"time-step\": 2761}, {\"accuracy\": 0.75, \"loss\": 0.12560372054576874, \"time-step\": 2762}, {\"accuracy\": 0.75, \"loss\": 0.12561091780662537, \"time-step\": 2763}, {\"accuracy\": 0.75, \"loss\": 0.12560364603996277, \"time-step\": 2764}, {\"accuracy\": 0.75, \"loss\": 0.12561096251010895, \"time-step\": 2765}, {\"accuracy\": 0.75, \"loss\": 0.12560349702835083, \"time-step\": 2766}, {\"accuracy\": 0.75, \"loss\": 0.1256108433008194, \"time-step\": 2767}, {\"accuracy\": 0.75, \"loss\": 0.12560349702835083, \"time-step\": 2768}, {\"accuracy\": 0.75, \"loss\": 0.12561070919036865, \"time-step\": 2769}, {\"accuracy\": 0.75, \"loss\": 0.12560336291790009, \"time-step\": 2770}, {\"accuracy\": 0.75, \"loss\": 0.12561073899269104, \"time-step\": 2771}, {\"accuracy\": 0.75, \"loss\": 0.12560322880744934, \"time-step\": 2772}, {\"accuracy\": 0.75, \"loss\": 0.12561066448688507, \"time-step\": 2773}, {\"accuracy\": 0.75, \"loss\": 0.12560321390628815, \"time-step\": 2774}, {\"accuracy\": 0.75, \"loss\": 0.12561051547527313, \"time-step\": 2775}, {\"accuracy\": 0.75, \"loss\": 0.12560319900512695, \"time-step\": 2776}, {\"accuracy\": 0.75, \"loss\": 0.12561048567295074, \"time-step\": 2777}, {\"accuracy\": 0.75, \"loss\": 0.12560303509235382, \"time-step\": 2778}, {\"accuracy\": 0.75, \"loss\": 0.12561050057411194, \"time-step\": 2779}, {\"accuracy\": 0.75, \"loss\": 0.12560304999351501, \"time-step\": 2780}, {\"accuracy\": 0.75, \"loss\": 0.1256103813648224, \"time-step\": 2781}, {\"accuracy\": 0.75, \"loss\": 0.12560291588306427, \"time-step\": 2782}, {\"accuracy\": 0.75, \"loss\": 0.12561029195785522, \"time-step\": 2783}, {\"accuracy\": 0.75, \"loss\": 0.12560278177261353, \"time-step\": 2784}, {\"accuracy\": 0.75, \"loss\": 0.12561030685901642, \"time-step\": 2785}, {\"accuracy\": 0.75, \"loss\": 0.12560275197029114, \"time-step\": 2786}, {\"accuracy\": 0.75, \"loss\": 0.12561015784740448, \"time-step\": 2787}, {\"accuracy\": 0.75, \"loss\": 0.1256026327610016, \"time-step\": 2788}, {\"accuracy\": 0.75, \"loss\": 0.12561018764972687, \"time-step\": 2789}, {\"accuracy\": 0.75, \"loss\": 0.1256026029586792, \"time-step\": 2790}, {\"accuracy\": 0.75, \"loss\": 0.12561003863811493, \"time-step\": 2791}, {\"accuracy\": 0.75, \"loss\": 0.12560249865055084, \"time-step\": 2792}, {\"accuracy\": 0.75, \"loss\": 0.12561005353927612, \"time-step\": 2793}, {\"accuracy\": 0.75, \"loss\": 0.12560239434242249, \"time-step\": 2794}, {\"accuracy\": 0.75, \"loss\": 0.1256098747253418, \"time-step\": 2795}, {\"accuracy\": 0.75, \"loss\": 0.12560230493545532, \"time-step\": 2796}, {\"accuracy\": 0.75, \"loss\": 0.1256098747253418, \"time-step\": 2797}, {\"accuracy\": 0.75, \"loss\": 0.12560226023197174, \"time-step\": 2798}, {\"accuracy\": 0.75, \"loss\": 0.12560983002185822, \"time-step\": 2799}, {\"accuracy\": 0.75, \"loss\": 0.12560215592384338, \"time-step\": 2800}, {\"accuracy\": 0.75, \"loss\": 0.12560978531837463, \"time-step\": 2801}, {\"accuracy\": 0.75, \"loss\": 0.12560215592384338, \"time-step\": 2802}, {\"accuracy\": 0.75, \"loss\": 0.12560978531837463, \"time-step\": 2803}, {\"accuracy\": 0.75, \"loss\": 0.1256021410226822, \"time-step\": 2804}, {\"accuracy\": 0.75, \"loss\": 0.12560969591140747, \"time-step\": 2805}, {\"accuracy\": 0.75, \"loss\": 0.12560196220874786, \"time-step\": 2806}, {\"accuracy\": 0.75, \"loss\": 0.1256096214056015, \"time-step\": 2807}, {\"accuracy\": 0.75, \"loss\": 0.1256019026041031, \"time-step\": 2808}, {\"accuracy\": 0.75, \"loss\": 0.12560951709747314, \"time-step\": 2809}, {\"accuracy\": 0.75, \"loss\": 0.1256018429994583, \"time-step\": 2810}, {\"accuracy\": 0.75, \"loss\": 0.12560957670211792, \"time-step\": 2811}, {\"accuracy\": 0.75, \"loss\": 0.12560181319713593, \"time-step\": 2812}, {\"accuracy\": 0.75, \"loss\": 0.12560945749282837, \"time-step\": 2813}, {\"accuracy\": 0.75, \"loss\": 0.12560176849365234, \"time-step\": 2814}, {\"accuracy\": 0.75, \"loss\": 0.12560947239398956, \"time-step\": 2815}, {\"accuracy\": 0.75, \"loss\": 0.1256016492843628, \"time-step\": 2816}, {\"accuracy\": 0.75, \"loss\": 0.12560948729515076, \"time-step\": 2817}, {\"accuracy\": 0.75, \"loss\": 0.1256016045808792, \"time-step\": 2818}, {\"accuracy\": 0.75, \"loss\": 0.12560942769050598, \"time-step\": 2819}, {\"accuracy\": 0.75, \"loss\": 0.12560150027275085, \"time-step\": 2820}, {\"accuracy\": 0.75, \"loss\": 0.12560929358005524, \"time-step\": 2821}, {\"accuracy\": 0.75, \"loss\": 0.12560147047042847, \"time-step\": 2822}, {\"accuracy\": 0.75, \"loss\": 0.12560930848121643, \"time-step\": 2823}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 2824}, {\"accuracy\": 0.75, \"loss\": 0.12560918927192688, \"time-step\": 2825}, {\"accuracy\": 0.75, \"loss\": 0.1256013810634613, \"time-step\": 2826}, {\"accuracy\": 0.75, \"loss\": 0.12560918927192688, \"time-step\": 2827}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 2828}, {\"accuracy\": 0.75, \"loss\": 0.12560921907424927, \"time-step\": 2829}, {\"accuracy\": 0.75, \"loss\": 0.12560118734836578, \"time-step\": 2830}, {\"accuracy\": 0.75, \"loss\": 0.1256091296672821, \"time-step\": 2831}, {\"accuracy\": 0.75, \"loss\": 0.1256011575460434, \"time-step\": 2832}, {\"accuracy\": 0.75, \"loss\": 0.12560909986495972, \"time-step\": 2833}, {\"accuracy\": 0.75, \"loss\": 0.12560106813907623, \"time-step\": 2834}, {\"accuracy\": 0.75, \"loss\": 0.12560898065567017, \"time-step\": 2835}, {\"accuracy\": 0.75, \"loss\": 0.12560105323791504, \"time-step\": 2836}, {\"accuracy\": 0.75, \"loss\": 0.12560904026031494, \"time-step\": 2837}, {\"accuracy\": 0.75, \"loss\": 0.12560096383094788, \"time-step\": 2838}, {\"accuracy\": 0.75, \"loss\": 0.12560895085334778, \"time-step\": 2839}, {\"accuracy\": 0.75, \"loss\": 0.1256008744239807, \"time-step\": 2840}, {\"accuracy\": 0.75, \"loss\": 0.12560895085334778, \"time-step\": 2841}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 2842}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 2843}, {\"accuracy\": 0.75, \"loss\": 0.12560071051120758, \"time-step\": 2844}, {\"accuracy\": 0.75, \"loss\": 0.1256089061498642, \"time-step\": 2845}, {\"accuracy\": 0.75, \"loss\": 0.12560074031352997, \"time-step\": 2846}, {\"accuracy\": 0.75, \"loss\": 0.12560880184173584, \"time-step\": 2847}, {\"accuracy\": 0.75, \"loss\": 0.125600665807724, \"time-step\": 2848}, {\"accuracy\": 0.75, \"loss\": 0.12560875713825226, \"time-step\": 2849}, {\"accuracy\": 0.75, \"loss\": 0.1256006360054016, \"time-step\": 2850}, {\"accuracy\": 0.75, \"loss\": 0.12560869753360748, \"time-step\": 2851}, {\"accuracy\": 0.75, \"loss\": 0.12560057640075684, \"time-step\": 2852}, {\"accuracy\": 0.75, \"loss\": 0.12560869753360748, \"time-step\": 2853}, {\"accuracy\": 0.75, \"loss\": 0.12560057640075684, \"time-step\": 2854}, {\"accuracy\": 0.75, \"loss\": 0.1256086677312851, \"time-step\": 2855}, {\"accuracy\": 0.75, \"loss\": 0.12560047209262848, \"time-step\": 2856}, {\"accuracy\": 0.75, \"loss\": 0.12560872733592987, \"time-step\": 2857}, {\"accuracy\": 0.75, \"loss\": 0.1256003975868225, \"time-step\": 2858}, {\"accuracy\": 0.75, \"loss\": 0.12560860812664032, \"time-step\": 2859}, {\"accuracy\": 0.75, \"loss\": 0.12560033798217773, \"time-step\": 2860}, {\"accuracy\": 0.75, \"loss\": 0.12560859322547913, \"time-step\": 2861}, {\"accuracy\": 0.75, \"loss\": 0.12560027837753296, \"time-step\": 2862}, {\"accuracy\": 0.75, \"loss\": 0.12560856342315674, \"time-step\": 2863}, {\"accuracy\": 0.75, \"loss\": 0.12560027837753296, \"time-step\": 2864}, {\"accuracy\": 0.75, \"loss\": 0.12560854852199554, \"time-step\": 2865}, {\"accuracy\": 0.75, \"loss\": 0.12560024857521057, \"time-step\": 2866}, {\"accuracy\": 0.75, \"loss\": 0.12560848891735077, \"time-step\": 2867}, {\"accuracy\": 0.75, \"loss\": 0.1256001889705658, \"time-step\": 2868}, {\"accuracy\": 0.75, \"loss\": 0.12560850381851196, \"time-step\": 2869}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 2870}, {\"accuracy\": 0.75, \"loss\": 0.1256084144115448, \"time-step\": 2871}, {\"accuracy\": 0.75, \"loss\": 0.12560006976127625, \"time-step\": 2872}, {\"accuracy\": 0.75, \"loss\": 0.1256083846092224, \"time-step\": 2873}, {\"accuracy\": 0.75, \"loss\": 0.12559998035430908, \"time-step\": 2874}, {\"accuracy\": 0.75, \"loss\": 0.1256083846092224, \"time-step\": 2875}, {\"accuracy\": 0.75, \"loss\": 0.12560003995895386, \"time-step\": 2876}, {\"accuracy\": 0.75, \"loss\": 0.1256084442138672, \"time-step\": 2877}, {\"accuracy\": 0.75, \"loss\": 0.1255999505519867, \"time-step\": 2878}, {\"accuracy\": 0.75, \"loss\": 0.12560829520225525, \"time-step\": 2879}, {\"accuracy\": 0.75, \"loss\": 0.12559989094734192, \"time-step\": 2880}, {\"accuracy\": 0.75, \"loss\": 0.12560831010341644, \"time-step\": 2881}, {\"accuracy\": 0.75, \"loss\": 0.12559983134269714, \"time-step\": 2882}, {\"accuracy\": 0.75, \"loss\": 0.12560822069644928, \"time-step\": 2883}, {\"accuracy\": 0.75, \"loss\": 0.12559977173805237, \"time-step\": 2884}, {\"accuracy\": 0.75, \"loss\": 0.12560825049877167, \"time-step\": 2885}, {\"accuracy\": 0.75, \"loss\": 0.12559977173805237, \"time-step\": 2886}, {\"accuracy\": 0.75, \"loss\": 0.12560820579528809, \"time-step\": 2887}, {\"accuracy\": 0.75, \"loss\": 0.1255997270345688, \"time-step\": 2888}, {\"accuracy\": 0.75, \"loss\": 0.12560831010341644, \"time-step\": 2889}, {\"accuracy\": 0.75, \"loss\": 0.12559954822063446, \"time-step\": 2890}, {\"accuracy\": 0.75, \"loss\": 0.12560813128948212, \"time-step\": 2891}, {\"accuracy\": 0.75, \"loss\": 0.12559960782527924, \"time-step\": 2892}, {\"accuracy\": 0.75, \"loss\": 0.12560813128948212, \"time-step\": 2893}, {\"accuracy\": 0.75, \"loss\": 0.12559960782527924, \"time-step\": 2894}, {\"accuracy\": 0.75, \"loss\": 0.12560813128948212, \"time-step\": 2895}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 2896}, {\"accuracy\": 0.75, \"loss\": 0.12560808658599854, \"time-step\": 2897}, {\"accuracy\": 0.75, \"loss\": 0.12559953331947327, \"time-step\": 2898}, {\"accuracy\": 0.75, \"loss\": 0.1256081461906433, \"time-step\": 2899}, {\"accuracy\": 0.75, \"loss\": 0.12559948861598969, \"time-step\": 2900}, {\"accuracy\": 0.75, \"loss\": 0.1256081461906433, \"time-step\": 2901}, {\"accuracy\": 0.75, \"loss\": 0.12559941411018372, \"time-step\": 2902}, {\"accuracy\": 0.75, \"loss\": 0.12560804188251495, \"time-step\": 2903}, {\"accuracy\": 0.75, \"loss\": 0.12559938430786133, \"time-step\": 2904}, {\"accuracy\": 0.75, \"loss\": 0.12560810148715973, \"time-step\": 2905}, {\"accuracy\": 0.75, \"loss\": 0.12559936940670013, \"time-step\": 2906}, {\"accuracy\": 0.75, \"loss\": 0.12560807168483734, \"time-step\": 2907}, {\"accuracy\": 0.75, \"loss\": 0.12559927999973297, \"time-step\": 2908}, {\"accuracy\": 0.75, \"loss\": 0.12560804188251495, \"time-step\": 2909}, {\"accuracy\": 0.75, \"loss\": 0.12559925019741058, \"time-step\": 2910}, {\"accuracy\": 0.75, \"loss\": 0.12560799717903137, \"time-step\": 2911}, {\"accuracy\": 0.75, \"loss\": 0.12559925019741058, \"time-step\": 2912}, {\"accuracy\": 0.75, \"loss\": 0.12560807168483734, \"time-step\": 2913}, {\"accuracy\": 0.75, \"loss\": 0.12559917569160461, \"time-step\": 2914}, {\"accuracy\": 0.75, \"loss\": 0.12560796737670898, \"time-step\": 2915}, {\"accuracy\": 0.75, \"loss\": 0.12559916079044342, \"time-step\": 2916}, {\"accuracy\": 0.75, \"loss\": 0.1256079375743866, \"time-step\": 2917}, {\"accuracy\": 0.75, \"loss\": 0.125599205493927, \"time-step\": 2918}, {\"accuracy\": 0.75, \"loss\": 0.1256079524755478, \"time-step\": 2919}, {\"accuracy\": 0.75, \"loss\": 0.12559910118579865, \"time-step\": 2920}, {\"accuracy\": 0.75, \"loss\": 0.1256079226732254, \"time-step\": 2921}, {\"accuracy\": 0.75, \"loss\": 0.12559908628463745, \"time-step\": 2922}, {\"accuracy\": 0.75, \"loss\": 0.1256079375743866, \"time-step\": 2923}, {\"accuracy\": 0.75, \"loss\": 0.12559908628463745, \"time-step\": 2924}, {\"accuracy\": 0.75, \"loss\": 0.12560796737670898, \"time-step\": 2925}, {\"accuracy\": 0.75, \"loss\": 0.1255989819765091, \"time-step\": 2926}, {\"accuracy\": 0.75, \"loss\": 0.12560796737670898, \"time-step\": 2927}, {\"accuracy\": 0.75, \"loss\": 0.12559890747070312, \"time-step\": 2928}, {\"accuracy\": 0.75, \"loss\": 0.1256079077720642, \"time-step\": 2929}, {\"accuracy\": 0.75, \"loss\": 0.12559883296489716, \"time-step\": 2930}, {\"accuracy\": 0.75, \"loss\": 0.1256079524755478, \"time-step\": 2931}, {\"accuracy\": 0.75, \"loss\": 0.12559883296489716, \"time-step\": 2932}, {\"accuracy\": 0.75, \"loss\": 0.12560787796974182, \"time-step\": 2933}, {\"accuracy\": 0.75, \"loss\": 0.12559890747070312, \"time-step\": 2934}, {\"accuracy\": 0.75, \"loss\": 0.12560786306858063, \"time-step\": 2935}, {\"accuracy\": 0.75, \"loss\": 0.12559884786605835, \"time-step\": 2936}, {\"accuracy\": 0.75, \"loss\": 0.12560789287090302, \"time-step\": 2937}, {\"accuracy\": 0.75, \"loss\": 0.12559880316257477, \"time-step\": 2938}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2939}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 2940}, {\"accuracy\": 0.75, \"loss\": 0.12560787796974182, \"time-step\": 2941}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 2942}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2943}, {\"accuracy\": 0.75, \"loss\": 0.1255987286567688, \"time-step\": 2944}, {\"accuracy\": 0.75, \"loss\": 0.12560783326625824, \"time-step\": 2945}, {\"accuracy\": 0.75, \"loss\": 0.12559868395328522, \"time-step\": 2946}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2947}, {\"accuracy\": 0.75, \"loss\": 0.12559860944747925, \"time-step\": 2948}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 2949}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 2950}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2951}, {\"accuracy\": 0.75, \"loss\": 0.12559860944747925, \"time-step\": 2952}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2953}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 2954}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2955}, {\"accuracy\": 0.75, \"loss\": 0.12559853494167328, \"time-step\": 2956}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2957}, {\"accuracy\": 0.75, \"loss\": 0.12559856474399567, \"time-step\": 2958}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2959}, {\"accuracy\": 0.75, \"loss\": 0.1255984753370285, \"time-step\": 2960}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2961}, {\"accuracy\": 0.75, \"loss\": 0.12559854984283447, \"time-step\": 2962}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2963}, {\"accuracy\": 0.75, \"loss\": 0.1255984604358673, \"time-step\": 2964}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2965}, {\"accuracy\": 0.75, \"loss\": 0.1255984902381897, \"time-step\": 2966}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2967}, {\"accuracy\": 0.75, \"loss\": 0.1255984753370285, \"time-step\": 2968}, {\"accuracy\": 0.75, \"loss\": 0.12560775876045227, \"time-step\": 2969}, {\"accuracy\": 0.75, \"loss\": 0.12559843063354492, \"time-step\": 2970}, {\"accuracy\": 0.75, \"loss\": 0.12560783326625824, \"time-step\": 2971}, {\"accuracy\": 0.75, \"loss\": 0.12559843063354492, \"time-step\": 2972}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2973}, {\"accuracy\": 0.75, \"loss\": 0.12559838593006134, \"time-step\": 2974}, {\"accuracy\": 0.75, \"loss\": 0.12560778856277466, \"time-step\": 2975}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 2976}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 2977}, {\"accuracy\": 0.75, \"loss\": 0.12559835612773895, \"time-step\": 2978}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 2979}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 2980}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2981}, {\"accuracy\": 0.75, \"loss\": 0.12559829652309418, \"time-step\": 2982}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2983}, {\"accuracy\": 0.75, \"loss\": 0.12559828162193298, \"time-step\": 2984}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2985}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 2986}, {\"accuracy\": 0.75, \"loss\": 0.12560783326625824, \"time-step\": 2987}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 2988}, {\"accuracy\": 0.75, \"loss\": 0.12560778856277466, \"time-step\": 2989}, {\"accuracy\": 0.75, \"loss\": 0.12559819221496582, \"time-step\": 2990}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 2991}, {\"accuracy\": 0.75, \"loss\": 0.1255982369184494, \"time-step\": 2992}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 2993}, {\"accuracy\": 0.75, \"loss\": 0.12559820711612701, \"time-step\": 2994}, {\"accuracy\": 0.75, \"loss\": 0.12560775876045227, \"time-step\": 2995}, {\"accuracy\": 0.75, \"loss\": 0.12559820711612701, \"time-step\": 2996}, {\"accuracy\": 0.75, \"loss\": 0.12560778856277466, \"time-step\": 2997}, {\"accuracy\": 0.75, \"loss\": 0.1255982369184494, \"time-step\": 2998}, {\"accuracy\": 0.75, \"loss\": 0.12560777366161346, \"time-step\": 2999}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3000}, {\"accuracy\": 0.75, \"loss\": 0.12560780346393585, \"time-step\": 3001}, {\"accuracy\": 0.75, \"loss\": 0.12559816241264343, \"time-step\": 3002}, {\"accuracy\": 0.75, \"loss\": 0.12560783326625824, \"time-step\": 3003}, {\"accuracy\": 0.75, \"loss\": 0.12559813261032104, \"time-step\": 3004}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 3005}, {\"accuracy\": 0.75, \"loss\": 0.12559816241264343, \"time-step\": 3006}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 3007}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3008}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 3009}, {\"accuracy\": 0.75, \"loss\": 0.12559813261032104, \"time-step\": 3010}, {\"accuracy\": 0.75, \"loss\": 0.1256079077720642, \"time-step\": 3011}, {\"accuracy\": 0.75, \"loss\": 0.12559811770915985, \"time-step\": 3012}, {\"accuracy\": 0.75, \"loss\": 0.1256079077720642, \"time-step\": 3013}, {\"accuracy\": 0.75, \"loss\": 0.12559811770915985, \"time-step\": 3014}, {\"accuracy\": 0.75, \"loss\": 0.12560781836509705, \"time-step\": 3015}, {\"accuracy\": 0.75, \"loss\": 0.12559807300567627, \"time-step\": 3016}, {\"accuracy\": 0.75, \"loss\": 0.1256079077720642, \"time-step\": 3017}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3018}, {\"accuracy\": 0.75, \"loss\": 0.12560783326625824, \"time-step\": 3019}, {\"accuracy\": 0.75, \"loss\": 0.1255979984998703, \"time-step\": 3020}, {\"accuracy\": 0.75, \"loss\": 0.12560787796974182, \"time-step\": 3021}, {\"accuracy\": 0.75, \"loss\": 0.1255980283021927, \"time-step\": 3022}, {\"accuracy\": 0.75, \"loss\": 0.1256079375743866, \"time-step\": 3023}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3024}, {\"accuracy\": 0.75, \"loss\": 0.12560789287090302, \"time-step\": 3025}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3026}, {\"accuracy\": 0.75, \"loss\": 0.12560799717903137, \"time-step\": 3027}, {\"accuracy\": 0.75, \"loss\": 0.12559807300567627, \"time-step\": 3028}, {\"accuracy\": 0.75, \"loss\": 0.1256079524755478, \"time-step\": 3029}, {\"accuracy\": 0.75, \"loss\": 0.1255979686975479, \"time-step\": 3030}, {\"accuracy\": 0.75, \"loss\": 0.1256079524755478, \"time-step\": 3031}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3032}, {\"accuracy\": 0.75, \"loss\": 0.12560796737670898, \"time-step\": 3033}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3034}, {\"accuracy\": 0.75, \"loss\": 0.12560796737670898, \"time-step\": 3035}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3036}, {\"accuracy\": 0.75, \"loss\": 0.12560810148715973, \"time-step\": 3037}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3038}, {\"accuracy\": 0.75, \"loss\": 0.12560802698135376, \"time-step\": 3039}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3040}, {\"accuracy\": 0.75, \"loss\": 0.12560802698135376, \"time-step\": 3041}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3042}, {\"accuracy\": 0.75, \"loss\": 0.12560804188251495, \"time-step\": 3043}, {\"accuracy\": 0.75, \"loss\": 0.12559787929058075, \"time-step\": 3044}, {\"accuracy\": 0.75, \"loss\": 0.12560805678367615, \"time-step\": 3045}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3046}, {\"accuracy\": 0.75, \"loss\": 0.12560808658599854, \"time-step\": 3047}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3048}, {\"accuracy\": 0.75, \"loss\": 0.12560805678367615, \"time-step\": 3049}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3050}, {\"accuracy\": 0.75, \"loss\": 0.12560813128948212, \"time-step\": 3051}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3052}, {\"accuracy\": 0.75, \"loss\": 0.1256081461906433, \"time-step\": 3053}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3054}, {\"accuracy\": 0.75, \"loss\": 0.1256081610918045, \"time-step\": 3055}, {\"accuracy\": 0.75, \"loss\": 0.12559795379638672, \"time-step\": 3056}, {\"accuracy\": 0.75, \"loss\": 0.1256081759929657, \"time-step\": 3057}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3058}, {\"accuracy\": 0.75, \"loss\": 0.1256081461906433, \"time-step\": 3059}, {\"accuracy\": 0.75, \"loss\": 0.1255979984998703, \"time-step\": 3060}, {\"accuracy\": 0.75, \"loss\": 0.12560823559761047, \"time-step\": 3061}, {\"accuracy\": 0.75, \"loss\": 0.12559795379638672, \"time-step\": 3062}, {\"accuracy\": 0.75, \"loss\": 0.1256081759929657, \"time-step\": 3063}, {\"accuracy\": 0.75, \"loss\": 0.12559790909290314, \"time-step\": 3064}, {\"accuracy\": 0.75, \"loss\": 0.12560823559761047, \"time-step\": 3065}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 3066}, {\"accuracy\": 0.75, \"loss\": 0.12560822069644928, \"time-step\": 3067}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3068}, {\"accuracy\": 0.75, \"loss\": 0.12560832500457764, \"time-step\": 3069}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3070}, {\"accuracy\": 0.75, \"loss\": 0.12560825049877167, \"time-step\": 3071}, {\"accuracy\": 0.75, \"loss\": 0.12559784948825836, \"time-step\": 3072}, {\"accuracy\": 0.75, \"loss\": 0.12560828030109406, \"time-step\": 3073}, {\"accuracy\": 0.75, \"loss\": 0.12559790909290314, \"time-step\": 3074}, {\"accuracy\": 0.75, \"loss\": 0.12560832500457764, \"time-step\": 3075}, {\"accuracy\": 0.75, \"loss\": 0.12559784948825836, \"time-step\": 3076}, {\"accuracy\": 0.75, \"loss\": 0.12560833990573883, \"time-step\": 3077}, {\"accuracy\": 0.75, \"loss\": 0.12559783458709717, \"time-step\": 3078}, {\"accuracy\": 0.75, \"loss\": 0.12560832500457764, \"time-step\": 3079}, {\"accuracy\": 0.75, \"loss\": 0.12559783458709717, \"time-step\": 3080}, {\"accuracy\": 0.75, \"loss\": 0.12560828030109406, \"time-step\": 3081}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3082}, {\"accuracy\": 0.75, \"loss\": 0.12560836970806122, \"time-step\": 3083}, {\"accuracy\": 0.75, \"loss\": 0.12559790909290314, \"time-step\": 3084}, {\"accuracy\": 0.75, \"loss\": 0.1256084144115448, \"time-step\": 3085}, {\"accuracy\": 0.75, \"loss\": 0.12559787929058075, \"time-step\": 3086}, {\"accuracy\": 0.75, \"loss\": 0.1256083995103836, \"time-step\": 3087}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3088}, {\"accuracy\": 0.75, \"loss\": 0.125608429312706, \"time-step\": 3089}, {\"accuracy\": 0.75, \"loss\": 0.12559784948825836, \"time-step\": 3090}, {\"accuracy\": 0.75, \"loss\": 0.12560847401618958, \"time-step\": 3091}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3092}, {\"accuracy\": 0.75, \"loss\": 0.12560847401618958, \"time-step\": 3093}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 3094}, {\"accuracy\": 0.75, \"loss\": 0.12560845911502838, \"time-step\": 3095}, {\"accuracy\": 0.75, \"loss\": 0.12559783458709717, \"time-step\": 3096}, {\"accuracy\": 0.75, \"loss\": 0.12560857832431793, \"time-step\": 3097}, {\"accuracy\": 0.75, \"loss\": 0.12559784948825836, \"time-step\": 3098}, {\"accuracy\": 0.75, \"loss\": 0.12560853362083435, \"time-step\": 3099}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3100}, {\"accuracy\": 0.75, \"loss\": 0.12560860812664032, \"time-step\": 3101}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3102}, {\"accuracy\": 0.75, \"loss\": 0.12560851871967316, \"time-step\": 3103}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3104}, {\"accuracy\": 0.75, \"loss\": 0.1256086677312851, \"time-step\": 3105}, {\"accuracy\": 0.75, \"loss\": 0.12559787929058075, \"time-step\": 3106}, {\"accuracy\": 0.75, \"loss\": 0.1256086230278015, \"time-step\": 3107}, {\"accuracy\": 0.75, \"loss\": 0.12559787929058075, \"time-step\": 3108}, {\"accuracy\": 0.75, \"loss\": 0.12560871243476868, \"time-step\": 3109}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 3110}, {\"accuracy\": 0.75, \"loss\": 0.12560869753360748, \"time-step\": 3111}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3112}, {\"accuracy\": 0.75, \"loss\": 0.1256086826324463, \"time-step\": 3113}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 3114}, {\"accuracy\": 0.75, \"loss\": 0.12560872733592987, \"time-step\": 3115}, {\"accuracy\": 0.75, \"loss\": 0.12559787929058075, \"time-step\": 3116}, {\"accuracy\": 0.75, \"loss\": 0.12560878694057465, \"time-step\": 3117}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3118}, {\"accuracy\": 0.75, \"loss\": 0.1256086677312851, \"time-step\": 3119}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3120}, {\"accuracy\": 0.75, \"loss\": 0.12560881674289703, \"time-step\": 3121}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3122}, {\"accuracy\": 0.75, \"loss\": 0.12560880184173584, \"time-step\": 3123}, {\"accuracy\": 0.75, \"loss\": 0.12559789419174194, \"time-step\": 3124}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 3125}, {\"accuracy\": 0.75, \"loss\": 0.12559795379638672, \"time-step\": 3126}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 3127}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3128}, {\"accuracy\": 0.75, \"loss\": 0.1256088763475418, \"time-step\": 3129}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3130}, {\"accuracy\": 0.75, \"loss\": 0.12560893595218658, \"time-step\": 3131}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3132}, {\"accuracy\": 0.75, \"loss\": 0.12560898065567017, \"time-step\": 3133}, {\"accuracy\": 0.75, \"loss\": 0.1255979686975479, \"time-step\": 3134}, {\"accuracy\": 0.75, \"loss\": 0.12560898065567017, \"time-step\": 3135}, {\"accuracy\": 0.75, \"loss\": 0.1255979686975479, \"time-step\": 3136}, {\"accuracy\": 0.75, \"loss\": 0.1256089210510254, \"time-step\": 3137}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 3138}, {\"accuracy\": 0.75, \"loss\": 0.12560901045799255, \"time-step\": 3139}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3140}, {\"accuracy\": 0.75, \"loss\": 0.12560899555683136, \"time-step\": 3141}, {\"accuracy\": 0.75, \"loss\": 0.12559793889522552, \"time-step\": 3142}, {\"accuracy\": 0.75, \"loss\": 0.12560908496379852, \"time-step\": 3143}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3144}, {\"accuracy\": 0.75, \"loss\": 0.1256091445684433, \"time-step\": 3145}, {\"accuracy\": 0.75, \"loss\": 0.12559795379638672, \"time-step\": 3146}, {\"accuracy\": 0.75, \"loss\": 0.12560908496379852, \"time-step\": 3147}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3148}, {\"accuracy\": 0.75, \"loss\": 0.12560924887657166, \"time-step\": 3149}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3150}, {\"accuracy\": 0.75, \"loss\": 0.12560918927192688, \"time-step\": 3151}, {\"accuracy\": 0.75, \"loss\": 0.12559808790683746, \"time-step\": 3152}, {\"accuracy\": 0.75, \"loss\": 0.12560921907424927, \"time-step\": 3153}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3154}, {\"accuracy\": 0.75, \"loss\": 0.12560920417308807, \"time-step\": 3155}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3156}, {\"accuracy\": 0.75, \"loss\": 0.12560920417308807, \"time-step\": 3157}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3158}, {\"accuracy\": 0.75, \"loss\": 0.12560921907424927, \"time-step\": 3159}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 3160}, {\"accuracy\": 0.75, \"loss\": 0.12560930848121643, \"time-step\": 3161}, {\"accuracy\": 0.75, \"loss\": 0.12559807300567627, \"time-step\": 3162}, {\"accuracy\": 0.75, \"loss\": 0.12560927867889404, \"time-step\": 3163}, {\"accuracy\": 0.75, \"loss\": 0.12559811770915985, \"time-step\": 3164}, {\"accuracy\": 0.75, \"loss\": 0.1256093978881836, \"time-step\": 3165}, {\"accuracy\": 0.75, \"loss\": 0.1255980134010315, \"time-step\": 3166}, {\"accuracy\": 0.75, \"loss\": 0.1256094127893448, \"time-step\": 3167}, {\"accuracy\": 0.75, \"loss\": 0.12559804320335388, \"time-step\": 3168}, {\"accuracy\": 0.75, \"loss\": 0.1256093978881836, \"time-step\": 3169}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3170}, {\"accuracy\": 0.75, \"loss\": 0.12560944259166718, \"time-step\": 3171}, {\"accuracy\": 0.75, \"loss\": 0.12559808790683746, \"time-step\": 3172}, {\"accuracy\": 0.75, \"loss\": 0.12560948729515076, \"time-step\": 3173}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3174}, {\"accuracy\": 0.75, \"loss\": 0.12560942769050598, \"time-step\": 3175}, {\"accuracy\": 0.75, \"loss\": 0.12559814751148224, \"time-step\": 3176}, {\"accuracy\": 0.75, \"loss\": 0.12560951709747314, \"time-step\": 3177}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 3178}, {\"accuracy\": 0.75, \"loss\": 0.12560954689979553, \"time-step\": 3179}, {\"accuracy\": 0.75, \"loss\": 0.12559811770915985, \"time-step\": 3180}, {\"accuracy\": 0.75, \"loss\": 0.12560957670211792, \"time-step\": 3181}, {\"accuracy\": 0.75, \"loss\": 0.12559811770915985, \"time-step\": 3182}, {\"accuracy\": 0.75, \"loss\": 0.1256096065044403, \"time-step\": 3183}, {\"accuracy\": 0.75, \"loss\": 0.12559819221496582, \"time-step\": 3184}, {\"accuracy\": 0.75, \"loss\": 0.12560956180095673, \"time-step\": 3185}, {\"accuracy\": 0.75, \"loss\": 0.12559820711612701, \"time-step\": 3186}, {\"accuracy\": 0.75, \"loss\": 0.12560978531837463, \"time-step\": 3187}, {\"accuracy\": 0.75, \"loss\": 0.12559816241264343, \"time-step\": 3188}, {\"accuracy\": 0.75, \"loss\": 0.12560966610908508, \"time-step\": 3189}, {\"accuracy\": 0.75, \"loss\": 0.12559819221496582, \"time-step\": 3190}, {\"accuracy\": 0.75, \"loss\": 0.12560974061489105, \"time-step\": 3191}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 3192}, {\"accuracy\": 0.75, \"loss\": 0.12560980021953583, \"time-step\": 3193}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 3194}, {\"accuracy\": 0.75, \"loss\": 0.1256098747253418, \"time-step\": 3195}, {\"accuracy\": 0.75, \"loss\": 0.1255982220172882, \"time-step\": 3196}, {\"accuracy\": 0.75, \"loss\": 0.12560981512069702, \"time-step\": 3197}, {\"accuracy\": 0.75, \"loss\": 0.12559829652309418, \"time-step\": 3198}, {\"accuracy\": 0.75, \"loss\": 0.12560983002185822, \"time-step\": 3199}, {\"accuracy\": 0.75, \"loss\": 0.1255982667207718, \"time-step\": 3200}, {\"accuracy\": 0.75, \"loss\": 0.12560991942882538, \"time-step\": 3201}, {\"accuracy\": 0.75, \"loss\": 0.12559831142425537, \"time-step\": 3202}, {\"accuracy\": 0.75, \"loss\": 0.12560993432998657, \"time-step\": 3203}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 3204}, {\"accuracy\": 0.75, \"loss\": 0.12560997903347015, \"time-step\": 3205}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 3206}, {\"accuracy\": 0.75, \"loss\": 0.12560997903347015, \"time-step\": 3207}, {\"accuracy\": 0.75, \"loss\": 0.12559843063354492, \"time-step\": 3208}, {\"accuracy\": 0.75, \"loss\": 0.12560999393463135, \"time-step\": 3209}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 3210}, {\"accuracy\": 0.75, \"loss\": 0.12561000883579254, \"time-step\": 3211}, {\"accuracy\": 0.75, \"loss\": 0.12559840083122253, \"time-step\": 3212}, {\"accuracy\": 0.75, \"loss\": 0.1256101131439209, \"time-step\": 3213}, {\"accuracy\": 0.75, \"loss\": 0.12559838593006134, \"time-step\": 3214}, {\"accuracy\": 0.75, \"loss\": 0.1256100982427597, \"time-step\": 3215}, {\"accuracy\": 0.75, \"loss\": 0.12559838593006134, \"time-step\": 3216}, {\"accuracy\": 0.75, \"loss\": 0.1256101280450821, \"time-step\": 3217}, {\"accuracy\": 0.75, \"loss\": 0.12559840083122253, \"time-step\": 3218}, {\"accuracy\": 0.75, \"loss\": 0.12561015784740448, \"time-step\": 3219}, {\"accuracy\": 0.75, \"loss\": 0.12559838593006134, \"time-step\": 3220}, {\"accuracy\": 0.75, \"loss\": 0.12561026215553284, \"time-step\": 3221}, {\"accuracy\": 0.75, \"loss\": 0.12559835612773895, \"time-step\": 3222}, {\"accuracy\": 0.75, \"loss\": 0.12561015784740448, \"time-step\": 3223}, {\"accuracy\": 0.75, \"loss\": 0.12559841573238373, \"time-step\": 3224}, {\"accuracy\": 0.75, \"loss\": 0.12561029195785522, \"time-step\": 3225}, {\"accuracy\": 0.75, \"loss\": 0.12559837102890015, \"time-step\": 3226}, {\"accuracy\": 0.75, \"loss\": 0.1256103217601776, \"time-step\": 3227}, {\"accuracy\": 0.75, \"loss\": 0.12559852004051208, \"time-step\": 3228}, {\"accuracy\": 0.75, \"loss\": 0.1256103515625, \"time-step\": 3229}, {\"accuracy\": 0.75, \"loss\": 0.1255984902381897, \"time-step\": 3230}, {\"accuracy\": 0.75, \"loss\": 0.1256103813648224, \"time-step\": 3231}, {\"accuracy\": 0.75, \"loss\": 0.12559841573238373, \"time-step\": 3232}, {\"accuracy\": 0.75, \"loss\": 0.12561044096946716, \"time-step\": 3233}, {\"accuracy\": 0.75, \"loss\": 0.1255984604358673, \"time-step\": 3234}, {\"accuracy\": 0.75, \"loss\": 0.12561045587062836, \"time-step\": 3235}, {\"accuracy\": 0.75, \"loss\": 0.1255984753370285, \"time-step\": 3236}, {\"accuracy\": 0.75, \"loss\": 0.12561047077178955, \"time-step\": 3237}, {\"accuracy\": 0.75, \"loss\": 0.12559854984283447, \"time-step\": 3238}, {\"accuracy\": 0.75, \"loss\": 0.12561050057411194, \"time-step\": 3239}, {\"accuracy\": 0.75, \"loss\": 0.12559852004051208, \"time-step\": 3240}, {\"accuracy\": 0.75, \"loss\": 0.12561053037643433, \"time-step\": 3241}, {\"accuracy\": 0.75, \"loss\": 0.1255984902381897, \"time-step\": 3242}, {\"accuracy\": 0.75, \"loss\": 0.1256105601787567, \"time-step\": 3243}, {\"accuracy\": 0.75, \"loss\": 0.1255984604358673, \"time-step\": 3244}, {\"accuracy\": 0.75, \"loss\": 0.12561053037643433, \"time-step\": 3245}, {\"accuracy\": 0.75, \"loss\": 0.1255985051393509, \"time-step\": 3246}, {\"accuracy\": 0.75, \"loss\": 0.1256105601787567, \"time-step\": 3247}, {\"accuracy\": 0.75, \"loss\": 0.12559853494167328, \"time-step\": 3248}, {\"accuracy\": 0.75, \"loss\": 0.12561067938804626, \"time-step\": 3249}, {\"accuracy\": 0.75, \"loss\": 0.12559853494167328, \"time-step\": 3250}, {\"accuracy\": 0.75, \"loss\": 0.12561064958572388, \"time-step\": 3251}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 3252}, {\"accuracy\": 0.75, \"loss\": 0.12561070919036865, \"time-step\": 3253}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 3254}, {\"accuracy\": 0.75, \"loss\": 0.12561076879501343, \"time-step\": 3255}, {\"accuracy\": 0.75, \"loss\": 0.12559859454631805, \"time-step\": 3256}, {\"accuracy\": 0.75, \"loss\": 0.12561087310314178, \"time-step\": 3257}, {\"accuracy\": 0.75, \"loss\": 0.12559857964515686, \"time-step\": 3258}, {\"accuracy\": 0.75, \"loss\": 0.125610813498497, \"time-step\": 3259}, {\"accuracy\": 0.75, \"loss\": 0.12559868395328522, \"time-step\": 3260}, {\"accuracy\": 0.75, \"loss\": 0.1256108582019806, \"time-step\": 3261}, {\"accuracy\": 0.75, \"loss\": 0.12559862434864044, \"time-step\": 3262}, {\"accuracy\": 0.75, \"loss\": 0.12561087310314178, \"time-step\": 3263}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 3264}, {\"accuracy\": 0.75, \"loss\": 0.1256108433008194, \"time-step\": 3265}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 3266}, {\"accuracy\": 0.75, \"loss\": 0.12561100721359253, \"time-step\": 3267}, {\"accuracy\": 0.75, \"loss\": 0.12559863924980164, \"time-step\": 3268}, {\"accuracy\": 0.75, \"loss\": 0.12561097741127014, \"time-step\": 3269}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 3270}, {\"accuracy\": 0.75, \"loss\": 0.12561100721359253, \"time-step\": 3271}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 3272}, {\"accuracy\": 0.75, \"loss\": 0.12561112642288208, \"time-step\": 3273}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 3274}, {\"accuracy\": 0.75, \"loss\": 0.1256110966205597, \"time-step\": 3275}, {\"accuracy\": 0.75, \"loss\": 0.1255987584590912, \"time-step\": 3276}, {\"accuracy\": 0.75, \"loss\": 0.12561114132404327, \"time-step\": 3277}, {\"accuracy\": 0.75, \"loss\": 0.1255987137556076, \"time-step\": 3278}, {\"accuracy\": 0.75, \"loss\": 0.12561112642288208, \"time-step\": 3279}, {\"accuracy\": 0.75, \"loss\": 0.12559880316257477, \"time-step\": 3280}, {\"accuracy\": 0.75, \"loss\": 0.12561114132404327, \"time-step\": 3281}, {\"accuracy\": 0.75, \"loss\": 0.12559880316257477, \"time-step\": 3282}, {\"accuracy\": 0.75, \"loss\": 0.12561120092868805, \"time-step\": 3283}, {\"accuracy\": 0.75, \"loss\": 0.12559877336025238, \"time-step\": 3284}, {\"accuracy\": 0.75, \"loss\": 0.12561126053333282, \"time-step\": 3285}, {\"accuracy\": 0.75, \"loss\": 0.12559883296489716, \"time-step\": 3286}, {\"accuracy\": 0.75, \"loss\": 0.1256113201379776, \"time-step\": 3287}, {\"accuracy\": 0.75, \"loss\": 0.12559877336025238, \"time-step\": 3288}, {\"accuracy\": 0.75, \"loss\": 0.12561127543449402, \"time-step\": 3289}, {\"accuracy\": 0.75, \"loss\": 0.12559886276721954, \"time-step\": 3290}, {\"accuracy\": 0.75, \"loss\": 0.1256113350391388, \"time-step\": 3291}, {\"accuracy\": 0.75, \"loss\": 0.12559892237186432, \"time-step\": 3292}, {\"accuracy\": 0.75, \"loss\": 0.12561139464378357, \"time-step\": 3293}, {\"accuracy\": 0.75, \"loss\": 0.12559890747070312, \"time-step\": 3294}, {\"accuracy\": 0.75, \"loss\": 0.12561143934726715, \"time-step\": 3295}, {\"accuracy\": 0.75, \"loss\": 0.12559892237186432, \"time-step\": 3296}, {\"accuracy\": 0.75, \"loss\": 0.12561143934726715, \"time-step\": 3297}, {\"accuracy\": 0.75, \"loss\": 0.1255989372730255, \"time-step\": 3298}, {\"accuracy\": 0.75, \"loss\": 0.12561152875423431, \"time-step\": 3299}, {\"accuracy\": 0.75, \"loss\": 0.12559890747070312, \"time-step\": 3300}, {\"accuracy\": 0.75, \"loss\": 0.12561149895191193, \"time-step\": 3301}, {\"accuracy\": 0.75, \"loss\": 0.1255989819765091, \"time-step\": 3302}, {\"accuracy\": 0.75, \"loss\": 0.12561152875423431, \"time-step\": 3303}, {\"accuracy\": 0.75, \"loss\": 0.1255989670753479, \"time-step\": 3304}, {\"accuracy\": 0.75, \"loss\": 0.1256115883588791, \"time-step\": 3305}, {\"accuracy\": 0.75, \"loss\": 0.1255989819765091, \"time-step\": 3306}, {\"accuracy\": 0.75, \"loss\": 0.12561160326004028, \"time-step\": 3307}, {\"accuracy\": 0.75, \"loss\": 0.12559901177883148, \"time-step\": 3308}, {\"accuracy\": 0.75, \"loss\": 0.12561166286468506, \"time-step\": 3309}, {\"accuracy\": 0.75, \"loss\": 0.1255989670753479, \"time-step\": 3310}, {\"accuracy\": 0.75, \"loss\": 0.12561160326004028, \"time-step\": 3311}, {\"accuracy\": 0.75, \"loss\": 0.12559908628463745, \"time-step\": 3312}, {\"accuracy\": 0.75, \"loss\": 0.12561166286468506, \"time-step\": 3313}, {\"accuracy\": 0.75, \"loss\": 0.12559913098812103, \"time-step\": 3314}, {\"accuracy\": 0.75, \"loss\": 0.1256117820739746, \"time-step\": 3315}, {\"accuracy\": 0.75, \"loss\": 0.12559904158115387, \"time-step\": 3316}, {\"accuracy\": 0.75, \"loss\": 0.1256118267774582, \"time-step\": 3317}, {\"accuracy\": 0.75, \"loss\": 0.12559913098812103, \"time-step\": 3318}, {\"accuracy\": 0.75, \"loss\": 0.1256118267774582, \"time-step\": 3319}, {\"accuracy\": 0.75, \"loss\": 0.12559908628463745, \"time-step\": 3320}, {\"accuracy\": 0.75, \"loss\": 0.1256117969751358, \"time-step\": 3321}, {\"accuracy\": 0.75, \"loss\": 0.125599205493927, \"time-step\": 3322}, {\"accuracy\": 0.75, \"loss\": 0.12561197578907013, \"time-step\": 3323}, {\"accuracy\": 0.75, \"loss\": 0.12559910118579865, \"time-step\": 3324}, {\"accuracy\": 0.75, \"loss\": 0.12561190128326416, \"time-step\": 3325}, {\"accuracy\": 0.75, \"loss\": 0.1255992203950882, \"time-step\": 3326}, {\"accuracy\": 0.75, \"loss\": 0.12561191618442535, \"time-step\": 3327}, {\"accuracy\": 0.75, \"loss\": 0.12559908628463745, \"time-step\": 3328}, {\"accuracy\": 0.75, \"loss\": 0.12561200559139252, \"time-step\": 3329}, {\"accuracy\": 0.75, \"loss\": 0.12559914588928223, \"time-step\": 3330}, {\"accuracy\": 0.75, \"loss\": 0.12561200559139252, \"time-step\": 3331}, {\"accuracy\": 0.75, \"loss\": 0.12559925019741058, \"time-step\": 3332}, {\"accuracy\": 0.75, \"loss\": 0.12561215460300446, \"time-step\": 3333}, {\"accuracy\": 0.75, \"loss\": 0.1255992352962494, \"time-step\": 3334}, {\"accuracy\": 0.75, \"loss\": 0.12561210989952087, \"time-step\": 3335}, {\"accuracy\": 0.75, \"loss\": 0.12559929490089417, \"time-step\": 3336}, {\"accuracy\": 0.75, \"loss\": 0.12561212480068207, \"time-step\": 3337}, {\"accuracy\": 0.75, \"loss\": 0.125599205493927, \"time-step\": 3338}, {\"accuracy\": 0.75, \"loss\": 0.12561216950416565, \"time-step\": 3339}, {\"accuracy\": 0.75, \"loss\": 0.12559927999973297, \"time-step\": 3340}, {\"accuracy\": 0.75, \"loss\": 0.12561212480068207, \"time-step\": 3341}, {\"accuracy\": 0.75, \"loss\": 0.12559930980205536, \"time-step\": 3342}, {\"accuracy\": 0.75, \"loss\": 0.125612273812294, \"time-step\": 3343}, {\"accuracy\": 0.75, \"loss\": 0.1255992352962494, \"time-step\": 3344}, {\"accuracy\": 0.75, \"loss\": 0.12561222910881042, \"time-step\": 3345}, {\"accuracy\": 0.75, \"loss\": 0.12559932470321655, \"time-step\": 3346}, {\"accuracy\": 0.75, \"loss\": 0.12561234831809998, \"time-step\": 3347}, {\"accuracy\": 0.75, \"loss\": 0.1255992352962494, \"time-step\": 3348}, {\"accuracy\": 0.75, \"loss\": 0.12561219930648804, \"time-step\": 3349}, {\"accuracy\": 0.75, \"loss\": 0.12559935450553894, \"time-step\": 3350}, {\"accuracy\": 0.75, \"loss\": 0.1256123185157776, \"time-step\": 3351}, {\"accuracy\": 0.75, \"loss\": 0.12559938430786133, \"time-step\": 3352}, {\"accuracy\": 0.75, \"loss\": 0.12561239302158356, \"time-step\": 3353}, {\"accuracy\": 0.75, \"loss\": 0.1255994439125061, \"time-step\": 3354}, {\"accuracy\": 0.75, \"loss\": 0.12561243772506714, \"time-step\": 3355}, {\"accuracy\": 0.75, \"loss\": 0.12559941411018372, \"time-step\": 3356}, {\"accuracy\": 0.75, \"loss\": 0.12561248242855072, \"time-step\": 3357}, {\"accuracy\": 0.75, \"loss\": 0.1255994588136673, \"time-step\": 3358}, {\"accuracy\": 0.75, \"loss\": 0.12561248242855072, \"time-step\": 3359}, {\"accuracy\": 0.75, \"loss\": 0.12559939920902252, \"time-step\": 3360}, {\"accuracy\": 0.75, \"loss\": 0.12561258673667908, \"time-step\": 3361}, {\"accuracy\": 0.75, \"loss\": 0.1255994439125061, \"time-step\": 3362}, {\"accuracy\": 0.75, \"loss\": 0.1256125420331955, \"time-step\": 3363}, {\"accuracy\": 0.75, \"loss\": 0.12559957802295685, \"time-step\": 3364}, {\"accuracy\": 0.75, \"loss\": 0.12561260163784027, \"time-step\": 3365}, {\"accuracy\": 0.75, \"loss\": 0.12559950351715088, \"time-step\": 3366}, {\"accuracy\": 0.75, \"loss\": 0.12561263144016266, \"time-step\": 3367}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 3368}, {\"accuracy\": 0.75, \"loss\": 0.12561273574829102, \"time-step\": 3369}, {\"accuracy\": 0.75, \"loss\": 0.12559951841831207, \"time-step\": 3370}, {\"accuracy\": 0.75, \"loss\": 0.12561272084712982, \"time-step\": 3371}, {\"accuracy\": 0.75, \"loss\": 0.12559954822063446, \"time-step\": 3372}, {\"accuracy\": 0.75, \"loss\": 0.1256127804517746, \"time-step\": 3373}, {\"accuracy\": 0.75, \"loss\": 0.12559959292411804, \"time-step\": 3374}, {\"accuracy\": 0.75, \"loss\": 0.12561285495758057, \"time-step\": 3375}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 3376}, {\"accuracy\": 0.75, \"loss\": 0.12561284005641937, \"time-step\": 3377}, {\"accuracy\": 0.75, \"loss\": 0.12559962272644043, \"time-step\": 3378}, {\"accuracy\": 0.75, \"loss\": 0.1256127804517746, \"time-step\": 3379}, {\"accuracy\": 0.75, \"loss\": 0.12559963762760162, \"time-step\": 3380}, {\"accuracy\": 0.75, \"loss\": 0.12561294436454773, \"time-step\": 3381}, {\"accuracy\": 0.75, \"loss\": 0.12559957802295685, \"time-step\": 3382}, {\"accuracy\": 0.75, \"loss\": 0.12561291456222534, \"time-step\": 3383}, {\"accuracy\": 0.75, \"loss\": 0.12559963762760162, \"time-step\": 3384}, {\"accuracy\": 0.75, \"loss\": 0.12561286985874176, \"time-step\": 3385}, {\"accuracy\": 0.75, \"loss\": 0.125599667429924, \"time-step\": 3386}, {\"accuracy\": 0.75, \"loss\": 0.12561295926570892, \"time-step\": 3387}, {\"accuracy\": 0.75, \"loss\": 0.12559963762760162, \"time-step\": 3388}, {\"accuracy\": 0.75, \"loss\": 0.1256129890680313, \"time-step\": 3389}, {\"accuracy\": 0.75, \"loss\": 0.125599667429924, \"time-step\": 3390}, {\"accuracy\": 0.75, \"loss\": 0.1256130337715149, \"time-step\": 3391}, {\"accuracy\": 0.75, \"loss\": 0.1255997121334076, \"time-step\": 3392}, {\"accuracy\": 0.75, \"loss\": 0.12561306357383728, \"time-step\": 3393}, {\"accuracy\": 0.75, \"loss\": 0.12559975683689117, \"time-step\": 3394}, {\"accuracy\": 0.75, \"loss\": 0.12561318278312683, \"time-step\": 3395}, {\"accuracy\": 0.75, \"loss\": 0.12559975683689117, \"time-step\": 3396}, {\"accuracy\": 0.75, \"loss\": 0.12561315298080444, \"time-step\": 3397}, {\"accuracy\": 0.75, \"loss\": 0.12559977173805237, \"time-step\": 3398}, {\"accuracy\": 0.75, \"loss\": 0.12561312317848206, \"time-step\": 3399}, {\"accuracy\": 0.75, \"loss\": 0.12559977173805237, \"time-step\": 3400}, {\"accuracy\": 0.75, \"loss\": 0.12561315298080444, \"time-step\": 3401}, {\"accuracy\": 0.75, \"loss\": 0.1255999058485031, \"time-step\": 3402}, {\"accuracy\": 0.75, \"loss\": 0.12561321258544922, \"time-step\": 3403}, {\"accuracy\": 0.75, \"loss\": 0.12559983134269714, \"time-step\": 3404}, {\"accuracy\": 0.75, \"loss\": 0.1256132870912552, \"time-step\": 3405}, {\"accuracy\": 0.75, \"loss\": 0.12559980154037476, \"time-step\": 3406}, {\"accuracy\": 0.75, \"loss\": 0.12561331689357758, \"time-step\": 3407}, {\"accuracy\": 0.75, \"loss\": 0.12559980154037476, \"time-step\": 3408}, {\"accuracy\": 0.75, \"loss\": 0.12561334669589996, \"time-step\": 3409}, {\"accuracy\": 0.75, \"loss\": 0.12559980154037476, \"time-step\": 3410}, {\"accuracy\": 0.75, \"loss\": 0.12561330199241638, \"time-step\": 3411}, {\"accuracy\": 0.75, \"loss\": 0.12559987604618073, \"time-step\": 3412}, {\"accuracy\": 0.75, \"loss\": 0.12561333179473877, \"time-step\": 3413}, {\"accuracy\": 0.75, \"loss\": 0.12559986114501953, \"time-step\": 3414}, {\"accuracy\": 0.75, \"loss\": 0.12561339139938354, \"time-step\": 3415}, {\"accuracy\": 0.75, \"loss\": 0.12559986114501953, \"time-step\": 3416}, {\"accuracy\": 0.75, \"loss\": 0.1256134808063507, \"time-step\": 3417}, {\"accuracy\": 0.75, \"loss\": 0.1255999356508255, \"time-step\": 3418}, {\"accuracy\": 0.75, \"loss\": 0.12561354041099548, \"time-step\": 3419}, {\"accuracy\": 0.75, \"loss\": 0.12559999525547028, \"time-step\": 3420}, {\"accuracy\": 0.75, \"loss\": 0.12561357021331787, \"time-step\": 3421}, {\"accuracy\": 0.75, \"loss\": 0.1255999654531479, \"time-step\": 3422}, {\"accuracy\": 0.75, \"loss\": 0.12561364471912384, \"time-step\": 3423}, {\"accuracy\": 0.75, \"loss\": 0.1255999207496643, \"time-step\": 3424}, {\"accuracy\": 0.75, \"loss\": 0.12561357021331787, \"time-step\": 3425}, {\"accuracy\": 0.75, \"loss\": 0.1255999654531479, \"time-step\": 3426}, {\"accuracy\": 0.75, \"loss\": 0.12561358511447906, \"time-step\": 3427}, {\"accuracy\": 0.75, \"loss\": 0.12559998035430908, \"time-step\": 3428}, {\"accuracy\": 0.75, \"loss\": 0.1256137490272522, \"time-step\": 3429}, {\"accuracy\": 0.75, \"loss\": 0.12559999525547028, \"time-step\": 3430}, {\"accuracy\": 0.75, \"loss\": 0.12561365962028503, \"time-step\": 3431}, {\"accuracy\": 0.75, \"loss\": 0.12559998035430908, \"time-step\": 3432}, {\"accuracy\": 0.75, \"loss\": 0.12561365962028503, \"time-step\": 3433}, {\"accuracy\": 0.75, \"loss\": 0.1255999654531479, \"time-step\": 3434}, {\"accuracy\": 0.75, \"loss\": 0.12561379373073578, \"time-step\": 3435}, {\"accuracy\": 0.75, \"loss\": 0.12560006976127625, \"time-step\": 3436}, {\"accuracy\": 0.75, \"loss\": 0.12561389803886414, \"time-step\": 3437}, {\"accuracy\": 0.75, \"loss\": 0.12560003995895386, \"time-step\": 3438}, {\"accuracy\": 0.75, \"loss\": 0.12561379373073578, \"time-step\": 3439}, {\"accuracy\": 0.75, \"loss\": 0.12560003995895386, \"time-step\": 3440}, {\"accuracy\": 0.75, \"loss\": 0.12561386823654175, \"time-step\": 3441}, {\"accuracy\": 0.75, \"loss\": 0.12560005486011505, \"time-step\": 3442}, {\"accuracy\": 0.75, \"loss\": 0.12561382353305817, \"time-step\": 3443}, {\"accuracy\": 0.75, \"loss\": 0.12560011446475983, \"time-step\": 3444}, {\"accuracy\": 0.75, \"loss\": 0.12561391294002533, \"time-step\": 3445}, {\"accuracy\": 0.75, \"loss\": 0.12560011446475983, \"time-step\": 3446}, {\"accuracy\": 0.75, \"loss\": 0.1256139576435089, \"time-step\": 3447}, {\"accuracy\": 0.75, \"loss\": 0.12560011446475983, \"time-step\": 3448}, {\"accuracy\": 0.75, \"loss\": 0.1256139725446701, \"time-step\": 3449}, {\"accuracy\": 0.75, \"loss\": 0.1256001889705658, \"time-step\": 3450}, {\"accuracy\": 0.75, \"loss\": 0.12561403214931488, \"time-step\": 3451}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 3452}, {\"accuracy\": 0.75, \"loss\": 0.1256139874458313, \"time-step\": 3453}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 3454}, {\"accuracy\": 0.75, \"loss\": 0.12561403214931488, \"time-step\": 3455}, {\"accuracy\": 0.75, \"loss\": 0.125600203871727, \"time-step\": 3456}, {\"accuracy\": 0.75, \"loss\": 0.12561409175395966, \"time-step\": 3457}, {\"accuracy\": 0.75, \"loss\": 0.125600203871727, \"time-step\": 3458}, {\"accuracy\": 0.75, \"loss\": 0.12561416625976562, \"time-step\": 3459}, {\"accuracy\": 0.75, \"loss\": 0.125600203871727, \"time-step\": 3460}, {\"accuracy\": 0.75, \"loss\": 0.12561413645744324, \"time-step\": 3461}, {\"accuracy\": 0.75, \"loss\": 0.12560027837753296, \"time-step\": 3462}, {\"accuracy\": 0.75, \"loss\": 0.12561410665512085, \"time-step\": 3463}, {\"accuracy\": 0.75, \"loss\": 0.12560023367404938, \"time-step\": 3464}, {\"accuracy\": 0.75, \"loss\": 0.1256142407655716, \"time-step\": 3465}, {\"accuracy\": 0.75, \"loss\": 0.12560027837753296, \"time-step\": 3466}, {\"accuracy\": 0.75, \"loss\": 0.1256142556667328, \"time-step\": 3467}, {\"accuracy\": 0.75, \"loss\": 0.12560026347637177, \"time-step\": 3468}, {\"accuracy\": 0.75, \"loss\": 0.12561430037021637, \"time-step\": 3469}, {\"accuracy\": 0.75, \"loss\": 0.12560035288333893, \"time-step\": 3470}, {\"accuracy\": 0.75, \"loss\": 0.12561431527137756, \"time-step\": 3471}, {\"accuracy\": 0.75, \"loss\": 0.12560024857521057, \"time-step\": 3472}, {\"accuracy\": 0.75, \"loss\": 0.12561441957950592, \"time-step\": 3473}, {\"accuracy\": 0.75, \"loss\": 0.12560036778450012, \"time-step\": 3474}, {\"accuracy\": 0.75, \"loss\": 0.12561438977718353, \"time-step\": 3475}, {\"accuracy\": 0.75, \"loss\": 0.12560036778450012, \"time-step\": 3476}, {\"accuracy\": 0.75, \"loss\": 0.12561440467834473, \"time-step\": 3477}, {\"accuracy\": 0.75, \"loss\": 0.12560036778450012, \"time-step\": 3478}, {\"accuracy\": 0.75, \"loss\": 0.12561441957950592, \"time-step\": 3479}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3480}, {\"accuracy\": 0.75, \"loss\": 0.1256144493818283, \"time-step\": 3481}, {\"accuracy\": 0.75, \"loss\": 0.1256003975868225, \"time-step\": 3482}, {\"accuracy\": 0.75, \"loss\": 0.1256144940853119, \"time-step\": 3483}, {\"accuracy\": 0.75, \"loss\": 0.12560038268566132, \"time-step\": 3484}, {\"accuracy\": 0.75, \"loss\": 0.1256144940853119, \"time-step\": 3485}, {\"accuracy\": 0.75, \"loss\": 0.12560038268566132, \"time-step\": 3486}, {\"accuracy\": 0.75, \"loss\": 0.12561458349227905, \"time-step\": 3487}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3488}, {\"accuracy\": 0.75, \"loss\": 0.12561453878879547, \"time-step\": 3489}, {\"accuracy\": 0.75, \"loss\": 0.1256004124879837, \"time-step\": 3490}, {\"accuracy\": 0.75, \"loss\": 0.12561461329460144, \"time-step\": 3491}, {\"accuracy\": 0.75, \"loss\": 0.12560051679611206, \"time-step\": 3492}, {\"accuracy\": 0.75, \"loss\": 0.12561461329460144, \"time-step\": 3493}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3494}, {\"accuracy\": 0.75, \"loss\": 0.12561462819576263, \"time-step\": 3495}, {\"accuracy\": 0.75, \"loss\": 0.12560054659843445, \"time-step\": 3496}, {\"accuracy\": 0.75, \"loss\": 0.12561467289924622, \"time-step\": 3497}, {\"accuracy\": 0.75, \"loss\": 0.12560060620307922, \"time-step\": 3498}, {\"accuracy\": 0.75, \"loss\": 0.12561477720737457, \"time-step\": 3499}, {\"accuracy\": 0.75, \"loss\": 0.12560056149959564, \"time-step\": 3500}, {\"accuracy\": 0.75, \"loss\": 0.125614732503891, \"time-step\": 3501}, {\"accuracy\": 0.75, \"loss\": 0.12560050189495087, \"time-step\": 3502}, {\"accuracy\": 0.75, \"loss\": 0.12561477720737457, \"time-step\": 3503}, {\"accuracy\": 0.75, \"loss\": 0.12560056149959564, \"time-step\": 3504}, {\"accuracy\": 0.75, \"loss\": 0.12561480700969696, \"time-step\": 3505}, {\"accuracy\": 0.75, \"loss\": 0.12560053169727325, \"time-step\": 3506}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 3507}, {\"accuracy\": 0.75, \"loss\": 0.12560056149959564, \"time-step\": 3508}, {\"accuracy\": 0.75, \"loss\": 0.12561483681201935, \"time-step\": 3509}, {\"accuracy\": 0.75, \"loss\": 0.12560057640075684, \"time-step\": 3510}, {\"accuracy\": 0.75, \"loss\": 0.1256149411201477, \"time-step\": 3511}, {\"accuracy\": 0.75, \"loss\": 0.12560053169727325, \"time-step\": 3512}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 3513}, {\"accuracy\": 0.75, \"loss\": 0.1256006360054016, \"time-step\": 3514}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 3515}, {\"accuracy\": 0.75, \"loss\": 0.12560060620307922, \"time-step\": 3516}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 3517}, {\"accuracy\": 0.75, \"loss\": 0.12560062110424042, \"time-step\": 3518}, {\"accuracy\": 0.75, \"loss\": 0.12561500072479248, \"time-step\": 3519}, {\"accuracy\": 0.75, \"loss\": 0.1256006509065628, \"time-step\": 3520}, {\"accuracy\": 0.75, \"loss\": 0.12561503052711487, \"time-step\": 3521}, {\"accuracy\": 0.75, \"loss\": 0.12560071051120758, \"time-step\": 3522}, {\"accuracy\": 0.75, \"loss\": 0.12561506032943726, \"time-step\": 3523}, {\"accuracy\": 0.75, \"loss\": 0.125600665807724, \"time-step\": 3524}, {\"accuracy\": 0.75, \"loss\": 0.125615194439888, \"time-step\": 3525}, {\"accuracy\": 0.75, \"loss\": 0.1256006956100464, \"time-step\": 3526}, {\"accuracy\": 0.75, \"loss\": 0.1256151795387268, \"time-step\": 3527}, {\"accuracy\": 0.75, \"loss\": 0.1256006509065628, \"time-step\": 3528}, {\"accuracy\": 0.75, \"loss\": 0.12561513483524323, \"time-step\": 3529}, {\"accuracy\": 0.75, \"loss\": 0.12560071051120758, \"time-step\": 3530}, {\"accuracy\": 0.75, \"loss\": 0.12561523914337158, \"time-step\": 3531}, {\"accuracy\": 0.75, \"loss\": 0.125600665807724, \"time-step\": 3532}, {\"accuracy\": 0.75, \"loss\": 0.12561523914337158, \"time-step\": 3533}, {\"accuracy\": 0.75, \"loss\": 0.12560059130191803, \"time-step\": 3534}, {\"accuracy\": 0.75, \"loss\": 0.1256152242422104, \"time-step\": 3535}, {\"accuracy\": 0.75, \"loss\": 0.12560074031352997, \"time-step\": 3536}, {\"accuracy\": 0.75, \"loss\": 0.12561526894569397, \"time-step\": 3537}, {\"accuracy\": 0.75, \"loss\": 0.12560072541236877, \"time-step\": 3538}, {\"accuracy\": 0.75, \"loss\": 0.12561531364917755, \"time-step\": 3539}, {\"accuracy\": 0.75, \"loss\": 0.12560071051120758, \"time-step\": 3540}, {\"accuracy\": 0.75, \"loss\": 0.12561532855033875, \"time-step\": 3541}, {\"accuracy\": 0.75, \"loss\": 0.12560077011585236, \"time-step\": 3542}, {\"accuracy\": 0.75, \"loss\": 0.12561531364917755, \"time-step\": 3543}, {\"accuracy\": 0.75, \"loss\": 0.1256008744239807, \"time-step\": 3544}, {\"accuracy\": 0.75, \"loss\": 0.12561540305614471, \"time-step\": 3545}, {\"accuracy\": 0.75, \"loss\": 0.12560078501701355, \"time-step\": 3546}, {\"accuracy\": 0.75, \"loss\": 0.12561547756195068, \"time-step\": 3547}, {\"accuracy\": 0.75, \"loss\": 0.12560074031352997, \"time-step\": 3548}, {\"accuracy\": 0.75, \"loss\": 0.1256154328584671, \"time-step\": 3549}, {\"accuracy\": 0.75, \"loss\": 0.12560079991817474, \"time-step\": 3550}, {\"accuracy\": 0.75, \"loss\": 0.12561547756195068, \"time-step\": 3551}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3552}, {\"accuracy\": 0.75, \"loss\": 0.12561549246311188, \"time-step\": 3553}, {\"accuracy\": 0.75, \"loss\": 0.12560085952281952, \"time-step\": 3554}, {\"accuracy\": 0.75, \"loss\": 0.12561547756195068, \"time-step\": 3555}, {\"accuracy\": 0.75, \"loss\": 0.12560078501701355, \"time-step\": 3556}, {\"accuracy\": 0.75, \"loss\": 0.12561547756195068, \"time-step\": 3557}, {\"accuracy\": 0.75, \"loss\": 0.1256008744239807, \"time-step\": 3558}, {\"accuracy\": 0.75, \"loss\": 0.12561556696891785, \"time-step\": 3559}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3560}, {\"accuracy\": 0.75, \"loss\": 0.1256156712770462, \"time-step\": 3561}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3562}, {\"accuracy\": 0.75, \"loss\": 0.12561561167240143, \"time-step\": 3563}, {\"accuracy\": 0.75, \"loss\": 0.12560094892978668, \"time-step\": 3564}, {\"accuracy\": 0.75, \"loss\": 0.1256157010793686, \"time-step\": 3565}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3566}, {\"accuracy\": 0.75, \"loss\": 0.1256156861782074, \"time-step\": 3567}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3568}, {\"accuracy\": 0.75, \"loss\": 0.1256156861782074, \"time-step\": 3569}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3570}, {\"accuracy\": 0.75, \"loss\": 0.1256156861782074, \"time-step\": 3571}, {\"accuracy\": 0.75, \"loss\": 0.1256009191274643, \"time-step\": 3572}, {\"accuracy\": 0.75, \"loss\": 0.12561571598052979, \"time-step\": 3573}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3574}, {\"accuracy\": 0.75, \"loss\": 0.12561573088169098, \"time-step\": 3575}, {\"accuracy\": 0.75, \"loss\": 0.12560099363327026, \"time-step\": 3576}, {\"accuracy\": 0.75, \"loss\": 0.12561576068401337, \"time-step\": 3577}, {\"accuracy\": 0.75, \"loss\": 0.12560099363327026, \"time-step\": 3578}, {\"accuracy\": 0.75, \"loss\": 0.12561580538749695, \"time-step\": 3579}, {\"accuracy\": 0.75, \"loss\": 0.12560102343559265, \"time-step\": 3580}, {\"accuracy\": 0.75, \"loss\": 0.1256158947944641, \"time-step\": 3581}, {\"accuracy\": 0.75, \"loss\": 0.12560094892978668, \"time-step\": 3582}, {\"accuracy\": 0.75, \"loss\": 0.1256159394979477, \"time-step\": 3583}, {\"accuracy\": 0.75, \"loss\": 0.12560096383094788, \"time-step\": 3584}, {\"accuracy\": 0.75, \"loss\": 0.1256158947944641, \"time-step\": 3585}, {\"accuracy\": 0.75, \"loss\": 0.12560096383094788, \"time-step\": 3586}, {\"accuracy\": 0.75, \"loss\": 0.1256158947944641, \"time-step\": 3587}, {\"accuracy\": 0.75, \"loss\": 0.12560109794139862, \"time-step\": 3588}, {\"accuracy\": 0.75, \"loss\": 0.1256159245967865, \"time-step\": 3589}, {\"accuracy\": 0.75, \"loss\": 0.12560103833675385, \"time-step\": 3590}, {\"accuracy\": 0.75, \"loss\": 0.1256159096956253, \"time-step\": 3591}, {\"accuracy\": 0.75, \"loss\": 0.12560108304023743, \"time-step\": 3592}, {\"accuracy\": 0.75, \"loss\": 0.12561599910259247, \"time-step\": 3593}, {\"accuracy\": 0.75, \"loss\": 0.12560106813907623, \"time-step\": 3594}, {\"accuracy\": 0.75, \"loss\": 0.12561599910259247, \"time-step\": 3595}, {\"accuracy\": 0.75, \"loss\": 0.12560102343559265, \"time-step\": 3596}, {\"accuracy\": 0.75, \"loss\": 0.12561596930027008, \"time-step\": 3597}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3598}, {\"accuracy\": 0.75, \"loss\": 0.12561607360839844, \"time-step\": 3599}, {\"accuracy\": 0.75, \"loss\": 0.12560106813907623, \"time-step\": 3600}, {\"accuracy\": 0.75, \"loss\": 0.12561602890491486, \"time-step\": 3601}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3602}, {\"accuracy\": 0.75, \"loss\": 0.1256161332130432, \"time-step\": 3603}, {\"accuracy\": 0.75, \"loss\": 0.12560109794139862, \"time-step\": 3604}, {\"accuracy\": 0.75, \"loss\": 0.12561610341072083, \"time-step\": 3605}, {\"accuracy\": 0.75, \"loss\": 0.125601127743721, \"time-step\": 3606}, {\"accuracy\": 0.75, \"loss\": 0.1256161332130432, \"time-step\": 3607}, {\"accuracy\": 0.75, \"loss\": 0.1256011426448822, \"time-step\": 3608}, {\"accuracy\": 0.75, \"loss\": 0.12561620771884918, \"time-step\": 3609}, {\"accuracy\": 0.75, \"loss\": 0.12560102343559265, \"time-step\": 3610}, {\"accuracy\": 0.75, \"loss\": 0.12561625242233276, \"time-step\": 3611}, {\"accuracy\": 0.75, \"loss\": 0.12560105323791504, \"time-step\": 3612}, {\"accuracy\": 0.75, \"loss\": 0.12561628222465515, \"time-step\": 3613}, {\"accuracy\": 0.75, \"loss\": 0.1256011426448822, \"time-step\": 3614}, {\"accuracy\": 0.75, \"loss\": 0.12561623752117157, \"time-step\": 3615}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3616}, {\"accuracy\": 0.75, \"loss\": 0.12561622262001038, \"time-step\": 3617}, {\"accuracy\": 0.75, \"loss\": 0.12560118734836578, \"time-step\": 3618}, {\"accuracy\": 0.75, \"loss\": 0.12561620771884918, \"time-step\": 3619}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3620}, {\"accuracy\": 0.75, \"loss\": 0.12561628222465515, \"time-step\": 3621}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3622}, {\"accuracy\": 0.75, \"loss\": 0.12561629712581635, \"time-step\": 3623}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3624}, {\"accuracy\": 0.75, \"loss\": 0.12561637163162231, \"time-step\": 3625}, {\"accuracy\": 0.75, \"loss\": 0.1256011426448822, \"time-step\": 3626}, {\"accuracy\": 0.75, \"loss\": 0.12561635673046112, \"time-step\": 3627}, {\"accuracy\": 0.75, \"loss\": 0.12560123205184937, \"time-step\": 3628}, {\"accuracy\": 0.75, \"loss\": 0.12561635673046112, \"time-step\": 3629}, {\"accuracy\": 0.75, \"loss\": 0.12560108304023743, \"time-step\": 3630}, {\"accuracy\": 0.75, \"loss\": 0.12561637163162231, \"time-step\": 3631}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3632}, {\"accuracy\": 0.75, \"loss\": 0.1256164312362671, \"time-step\": 3633}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3634}, {\"accuracy\": 0.75, \"loss\": 0.12561656534671783, \"time-step\": 3635}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3636}, {\"accuracy\": 0.75, \"loss\": 0.1256163865327835, \"time-step\": 3637}, {\"accuracy\": 0.75, \"loss\": 0.12560118734836578, \"time-step\": 3638}, {\"accuracy\": 0.75, \"loss\": 0.12561650574207306, \"time-step\": 3639}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3640}, {\"accuracy\": 0.75, \"loss\": 0.12561655044555664, \"time-step\": 3641}, {\"accuracy\": 0.75, \"loss\": 0.12560123205184937, \"time-step\": 3642}, {\"accuracy\": 0.75, \"loss\": 0.12561655044555664, \"time-step\": 3643}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3644}, {\"accuracy\": 0.75, \"loss\": 0.12561658024787903, \"time-step\": 3645}, {\"accuracy\": 0.75, \"loss\": 0.12560124695301056, \"time-step\": 3646}, {\"accuracy\": 0.75, \"loss\": 0.12561659514904022, \"time-step\": 3647}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3648}, {\"accuracy\": 0.75, \"loss\": 0.12561650574207306, \"time-step\": 3649}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3650}, {\"accuracy\": 0.75, \"loss\": 0.1256166398525238, \"time-step\": 3651}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3652}, {\"accuracy\": 0.75, \"loss\": 0.12561658024787903, \"time-step\": 3653}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3654}, {\"accuracy\": 0.75, \"loss\": 0.1256166398525238, \"time-step\": 3655}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3656}, {\"accuracy\": 0.75, \"loss\": 0.1256166398525238, \"time-step\": 3657}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3658}, {\"accuracy\": 0.75, \"loss\": 0.12561671435832977, \"time-step\": 3659}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3660}, {\"accuracy\": 0.75, \"loss\": 0.12561669945716858, \"time-step\": 3661}, {\"accuracy\": 0.75, \"loss\": 0.12560124695301056, \"time-step\": 3662}, {\"accuracy\": 0.75, \"loss\": 0.12561669945716858, \"time-step\": 3663}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3664}, {\"accuracy\": 0.75, \"loss\": 0.12561677396297455, \"time-step\": 3665}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3666}, {\"accuracy\": 0.75, \"loss\": 0.12561680376529694, \"time-step\": 3667}, {\"accuracy\": 0.75, \"loss\": 0.12560124695301056, \"time-step\": 3668}, {\"accuracy\": 0.75, \"loss\": 0.12561677396297455, \"time-step\": 3669}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3670}, {\"accuracy\": 0.75, \"loss\": 0.12561674416065216, \"time-step\": 3671}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3672}, {\"accuracy\": 0.75, \"loss\": 0.12561674416065216, \"time-step\": 3673}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3674}, {\"accuracy\": 0.75, \"loss\": 0.1256169229745865, \"time-step\": 3675}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3676}, {\"accuracy\": 0.75, \"loss\": 0.1256168782711029, \"time-step\": 3677}, {\"accuracy\": 0.75, \"loss\": 0.12560133635997772, \"time-step\": 3678}, {\"accuracy\": 0.75, \"loss\": 0.12561681866645813, \"time-step\": 3679}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3680}, {\"accuracy\": 0.75, \"loss\": 0.1256168931722641, \"time-step\": 3681}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3682}, {\"accuracy\": 0.75, \"loss\": 0.1256169229745865, \"time-step\": 3683}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3684}, {\"accuracy\": 0.75, \"loss\": 0.12561693787574768, \"time-step\": 3685}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3686}, {\"accuracy\": 0.75, \"loss\": 0.1256168782711029, \"time-step\": 3687}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3688}, {\"accuracy\": 0.75, \"loss\": 0.12561693787574768, \"time-step\": 3689}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3690}, {\"accuracy\": 0.75, \"loss\": 0.12561693787574768, \"time-step\": 3691}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3692}, {\"accuracy\": 0.75, \"loss\": 0.1256169229745865, \"time-step\": 3693}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3694}, {\"accuracy\": 0.75, \"loss\": 0.12561693787574768, \"time-step\": 3695}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3696}, {\"accuracy\": 0.75, \"loss\": 0.12561705708503723, \"time-step\": 3697}, {\"accuracy\": 0.75, \"loss\": 0.12560129165649414, \"time-step\": 3698}, {\"accuracy\": 0.75, \"loss\": 0.12561705708503723, \"time-step\": 3699}, {\"accuracy\": 0.75, \"loss\": 0.1256013661623001, \"time-step\": 3700}, {\"accuracy\": 0.75, \"loss\": 0.12561705708503723, \"time-step\": 3701}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 3702}, {\"accuracy\": 0.75, \"loss\": 0.12561705708503723, \"time-step\": 3703}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3704}, {\"accuracy\": 0.75, \"loss\": 0.1256171017885208, \"time-step\": 3705}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3706}, {\"accuracy\": 0.75, \"loss\": 0.12561701238155365, \"time-step\": 3707}, {\"accuracy\": 0.75, \"loss\": 0.1256013661623001, \"time-step\": 3708}, {\"accuracy\": 0.75, \"loss\": 0.1256171017885208, \"time-step\": 3709}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3710}, {\"accuracy\": 0.75, \"loss\": 0.12561708688735962, \"time-step\": 3711}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3712}, {\"accuracy\": 0.75, \"loss\": 0.1256171315908432, \"time-step\": 3713}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3714}, {\"accuracy\": 0.75, \"loss\": 0.12561708688735962, \"time-step\": 3715}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3716}, {\"accuracy\": 0.75, \"loss\": 0.1256171315908432, \"time-step\": 3717}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3718}, {\"accuracy\": 0.75, \"loss\": 0.125617116689682, \"time-step\": 3719}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3720}, {\"accuracy\": 0.75, \"loss\": 0.125617116689682, \"time-step\": 3721}, {\"accuracy\": 0.75, \"loss\": 0.12560133635997772, \"time-step\": 3722}, {\"accuracy\": 0.75, \"loss\": 0.12561719119548798, \"time-step\": 3723}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3724}, {\"accuracy\": 0.75, \"loss\": 0.12561723589897156, \"time-step\": 3725}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3726}, {\"accuracy\": 0.75, \"loss\": 0.12561717629432678, \"time-step\": 3727}, {\"accuracy\": 0.75, \"loss\": 0.1256013959646225, \"time-step\": 3728}, {\"accuracy\": 0.75, \"loss\": 0.1256171464920044, \"time-step\": 3729}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3730}, {\"accuracy\": 0.75, \"loss\": 0.12561720609664917, \"time-step\": 3731}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3732}, {\"accuracy\": 0.75, \"loss\": 0.12561725080013275, \"time-step\": 3733}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 3734}, {\"accuracy\": 0.75, \"loss\": 0.12561720609664917, \"time-step\": 3735}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3736}, {\"accuracy\": 0.75, \"loss\": 0.12561723589897156, \"time-step\": 3737}, {\"accuracy\": 0.75, \"loss\": 0.1256013810634613, \"time-step\": 3738}, {\"accuracy\": 0.75, \"loss\": 0.12561723589897156, \"time-step\": 3739}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3740}, {\"accuracy\": 0.75, \"loss\": 0.12561728060245514, \"time-step\": 3741}, {\"accuracy\": 0.75, \"loss\": 0.1256013661623001, \"time-step\": 3742}, {\"accuracy\": 0.75, \"loss\": 0.12561728060245514, \"time-step\": 3743}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3744}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3745}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 3746}, {\"accuracy\": 0.75, \"loss\": 0.12561728060245514, \"time-step\": 3747}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3748}, {\"accuracy\": 0.75, \"loss\": 0.12561742961406708, \"time-step\": 3749}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3750}, {\"accuracy\": 0.75, \"loss\": 0.12561741471290588, \"time-step\": 3751}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3752}, {\"accuracy\": 0.75, \"loss\": 0.12561734020709991, \"time-step\": 3753}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 3754}, {\"accuracy\": 0.75, \"loss\": 0.12561741471290588, \"time-step\": 3755}, {\"accuracy\": 0.75, \"loss\": 0.12560135126113892, \"time-step\": 3756}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3757}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3758}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3759}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3760}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3761}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3762}, {\"accuracy\": 0.75, \"loss\": 0.1256173700094223, \"time-step\": 3763}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3764}, {\"accuracy\": 0.75, \"loss\": 0.1256173551082611, \"time-step\": 3765}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3766}, {\"accuracy\": 0.75, \"loss\": 0.12561747431755066, \"time-step\": 3767}, {\"accuracy\": 0.75, \"loss\": 0.12560133635997772, \"time-step\": 3768}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3769}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3770}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3771}, {\"accuracy\": 0.75, \"loss\": 0.12560130655765533, \"time-step\": 3772}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3773}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3774}, {\"accuracy\": 0.75, \"loss\": 0.12561745941638947, \"time-step\": 3775}, {\"accuracy\": 0.75, \"loss\": 0.12560132145881653, \"time-step\": 3776}, {\"accuracy\": 0.75, \"loss\": 0.12561747431755066, \"time-step\": 3777}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3778}, {\"accuracy\": 0.75, \"loss\": 0.12561751902103424, \"time-step\": 3779}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3780}, {\"accuracy\": 0.75, \"loss\": 0.12561754882335663, \"time-step\": 3781}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3782}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 3783}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3784}, {\"accuracy\": 0.75, \"loss\": 0.12561747431755066, \"time-step\": 3785}, {\"accuracy\": 0.75, \"loss\": 0.12560124695301056, \"time-step\": 3786}, {\"accuracy\": 0.75, \"loss\": 0.12561753392219543, \"time-step\": 3787}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3788}, {\"accuracy\": 0.75, \"loss\": 0.12561756372451782, \"time-step\": 3789}, {\"accuracy\": 0.75, \"loss\": 0.12560124695301056, \"time-step\": 3790}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3791}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3792}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3793}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3794}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3795}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3796}, {\"accuracy\": 0.75, \"loss\": 0.12561771273612976, \"time-step\": 3797}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3798}, {\"accuracy\": 0.75, \"loss\": 0.12561756372451782, \"time-step\": 3799}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3800}, {\"accuracy\": 0.75, \"loss\": 0.12561753392219543, \"time-step\": 3801}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3802}, {\"accuracy\": 0.75, \"loss\": 0.12561753392219543, \"time-step\": 3803}, {\"accuracy\": 0.75, \"loss\": 0.12560120224952698, \"time-step\": 3804}, {\"accuracy\": 0.75, \"loss\": 0.12561753392219543, \"time-step\": 3805}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3806}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3807}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3808}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3809}, {\"accuracy\": 0.75, \"loss\": 0.12560121715068817, \"time-step\": 3810}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3811}, {\"accuracy\": 0.75, \"loss\": 0.12560127675533295, \"time-step\": 3812}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3813}, {\"accuracy\": 0.75, \"loss\": 0.12560126185417175, \"time-step\": 3814}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3815}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3816}, {\"accuracy\": 0.75, \"loss\": 0.12561754882335663, \"time-step\": 3817}, {\"accuracy\": 0.75, \"loss\": 0.12560108304023743, \"time-step\": 3818}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3819}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3820}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3821}, {\"accuracy\": 0.75, \"loss\": 0.1256011575460434, \"time-step\": 3822}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3823}, {\"accuracy\": 0.75, \"loss\": 0.12560108304023743, \"time-step\": 3824}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3825}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3826}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3827}, {\"accuracy\": 0.75, \"loss\": 0.12560109794139862, \"time-step\": 3828}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3829}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3830}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3831}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 3832}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3833}, {\"accuracy\": 0.75, \"loss\": 0.12560118734836578, \"time-step\": 3834}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3835}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3836}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3837}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3838}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3839}, {\"accuracy\": 0.75, \"loss\": 0.12560111284255981, \"time-step\": 3840}, {\"accuracy\": 0.75, \"loss\": 0.12561771273612976, \"time-step\": 3841}, {\"accuracy\": 0.75, \"loss\": 0.12560105323791504, \"time-step\": 3842}, {\"accuracy\": 0.75, \"loss\": 0.12561769783496857, \"time-step\": 3843}, {\"accuracy\": 0.75, \"loss\": 0.12560108304023743, \"time-step\": 3844}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3845}, {\"accuracy\": 0.75, \"loss\": 0.12560103833675385, \"time-step\": 3846}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3847}, {\"accuracy\": 0.75, \"loss\": 0.12560105323791504, \"time-step\": 3848}, {\"accuracy\": 0.75, \"loss\": 0.12561772763729095, \"time-step\": 3849}, {\"accuracy\": 0.75, \"loss\": 0.12560105323791504, \"time-step\": 3850}, {\"accuracy\": 0.75, \"loss\": 0.12561769783496857, \"time-step\": 3851}, {\"accuracy\": 0.75, \"loss\": 0.12560100853443146, \"time-step\": 3852}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3853}, {\"accuracy\": 0.75, \"loss\": 0.12560097873210907, \"time-step\": 3854}, {\"accuracy\": 0.75, \"loss\": 0.12561768293380737, \"time-step\": 3855}, {\"accuracy\": 0.75, \"loss\": 0.1256009340286255, \"time-step\": 3856}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3857}, {\"accuracy\": 0.75, \"loss\": 0.12560100853443146, \"time-step\": 3858}, {\"accuracy\": 0.75, \"loss\": 0.12561771273612976, \"time-step\": 3859}, {\"accuracy\": 0.75, \"loss\": 0.12560097873210907, \"time-step\": 3860}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3861}, {\"accuracy\": 0.75, \"loss\": 0.12560100853443146, \"time-step\": 3862}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3863}, {\"accuracy\": 0.75, \"loss\": 0.12560094892978668, \"time-step\": 3864}, {\"accuracy\": 0.75, \"loss\": 0.12561771273612976, \"time-step\": 3865}, {\"accuracy\": 0.75, \"loss\": 0.12560096383094788, \"time-step\": 3866}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3867}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3868}, {\"accuracy\": 0.75, \"loss\": 0.12561768293380737, \"time-step\": 3869}, {\"accuracy\": 0.75, \"loss\": 0.1256009191274643, \"time-step\": 3870}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3871}, {\"accuracy\": 0.75, \"loss\": 0.1256009340286255, \"time-step\": 3872}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3873}, {\"accuracy\": 0.75, \"loss\": 0.1256009340286255, \"time-step\": 3874}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3875}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3876}, {\"accuracy\": 0.75, \"loss\": 0.12561774253845215, \"time-step\": 3877}, {\"accuracy\": 0.75, \"loss\": 0.1256008744239807, \"time-step\": 3878}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3879}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 3880}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3881}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3882}, {\"accuracy\": 0.75, \"loss\": 0.12561775743961334, \"time-step\": 3883}, {\"accuracy\": 0.75, \"loss\": 0.12560081481933594, \"time-step\": 3884}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3885}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3886}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3887}, {\"accuracy\": 0.75, \"loss\": 0.12560081481933594, \"time-step\": 3888}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3889}, {\"accuracy\": 0.75, \"loss\": 0.12560084462165833, \"time-step\": 3890}, {\"accuracy\": 0.75, \"loss\": 0.12561768293380737, \"time-step\": 3891}, {\"accuracy\": 0.75, \"loss\": 0.12560082972049713, \"time-step\": 3892}, {\"accuracy\": 0.75, \"loss\": 0.12561768293380737, \"time-step\": 3893}, {\"accuracy\": 0.75, \"loss\": 0.12560072541236877, \"time-step\": 3894}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3895}, {\"accuracy\": 0.75, \"loss\": 0.12560081481933594, \"time-step\": 3896}, {\"accuracy\": 0.75, \"loss\": 0.12561765313148499, \"time-step\": 3897}, {\"accuracy\": 0.75, \"loss\": 0.12560081481933594, \"time-step\": 3898}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3899}, {\"accuracy\": 0.75, \"loss\": 0.12560074031352997, \"time-step\": 3900}, {\"accuracy\": 0.75, \"loss\": 0.12561771273612976, \"time-step\": 3901}, {\"accuracy\": 0.75, \"loss\": 0.1256006956100464, \"time-step\": 3902}, {\"accuracy\": 0.75, \"loss\": 0.12561768293380737, \"time-step\": 3903}, {\"accuracy\": 0.75, \"loss\": 0.12560078501701355, \"time-step\": 3904}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3905}, {\"accuracy\": 0.75, \"loss\": 0.1256006956100464, \"time-step\": 3906}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3907}, {\"accuracy\": 0.75, \"loss\": 0.12560072541236877, \"time-step\": 3908}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3909}, {\"accuracy\": 0.75, \"loss\": 0.1256006509065628, \"time-step\": 3910}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3911}, {\"accuracy\": 0.75, \"loss\": 0.1256006956100464, \"time-step\": 3912}, {\"accuracy\": 0.75, \"loss\": 0.12561757862567902, \"time-step\": 3913}, {\"accuracy\": 0.75, \"loss\": 0.1256006360054016, \"time-step\": 3914}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3915}, {\"accuracy\": 0.75, \"loss\": 0.12560062110424042, \"time-step\": 3916}, {\"accuracy\": 0.75, \"loss\": 0.12561766803264618, \"time-step\": 3917}, {\"accuracy\": 0.75, \"loss\": 0.1256006360054016, \"time-step\": 3918}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3919}, {\"accuracy\": 0.75, \"loss\": 0.12560057640075684, \"time-step\": 3920}, {\"accuracy\": 0.75, \"loss\": 0.12561757862567902, \"time-step\": 3921}, {\"accuracy\": 0.75, \"loss\": 0.12560059130191803, \"time-step\": 3922}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3923}, {\"accuracy\": 0.75, \"loss\": 0.12560056149959564, \"time-step\": 3924}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3925}, {\"accuracy\": 0.75, \"loss\": 0.12560056149959564, \"time-step\": 3926}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3927}, {\"accuracy\": 0.75, \"loss\": 0.12560050189495087, \"time-step\": 3928}, {\"accuracy\": 0.75, \"loss\": 0.1256176233291626, \"time-step\": 3929}, {\"accuracy\": 0.75, \"loss\": 0.12560057640075684, \"time-step\": 3930}, {\"accuracy\": 0.75, \"loss\": 0.12561757862567902, \"time-step\": 3931}, {\"accuracy\": 0.75, \"loss\": 0.12560051679611206, \"time-step\": 3932}, {\"accuracy\": 0.75, \"loss\": 0.1256176382303238, \"time-step\": 3933}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3934}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3935}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3936}, {\"accuracy\": 0.75, \"loss\": 0.12561750411987305, \"time-step\": 3937}, {\"accuracy\": 0.75, \"loss\": 0.1256004422903061, \"time-step\": 3938}, {\"accuracy\": 0.75, \"loss\": 0.12561757862567902, \"time-step\": 3939}, {\"accuracy\": 0.75, \"loss\": 0.1256003975868225, \"time-step\": 3940}, {\"accuracy\": 0.75, \"loss\": 0.12561756372451782, \"time-step\": 3941}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3942}, {\"accuracy\": 0.75, \"loss\": 0.12561751902103424, \"time-step\": 3943}, {\"accuracy\": 0.75, \"loss\": 0.1256004422903061, \"time-step\": 3944}, {\"accuracy\": 0.75, \"loss\": 0.1256176084280014, \"time-step\": 3945}, {\"accuracy\": 0.75, \"loss\": 0.12560036778450012, \"time-step\": 3946}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3947}, {\"accuracy\": 0.75, \"loss\": 0.1256004273891449, \"time-step\": 3948}, {\"accuracy\": 0.75, \"loss\": 0.12561751902103424, \"time-step\": 3949}, {\"accuracy\": 0.75, \"loss\": 0.12560030817985535, \"time-step\": 3950}, {\"accuracy\": 0.75, \"loss\": 0.12561754882335663, \"time-step\": 3951}, {\"accuracy\": 0.75, \"loss\": 0.12560038268566132, \"time-step\": 3952}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 3953}, {\"accuracy\": 0.75, \"loss\": 0.12560032308101654, \"time-step\": 3954}, {\"accuracy\": 0.75, \"loss\": 0.1256175935268402, \"time-step\": 3955}, {\"accuracy\": 0.75, \"loss\": 0.12560024857521057, \"time-step\": 3956}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 3957}, {\"accuracy\": 0.75, \"loss\": 0.12560023367404938, \"time-step\": 3958}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 3959}, {\"accuracy\": 0.75, \"loss\": 0.12560024857521057, \"time-step\": 3960}, {\"accuracy\": 0.75, \"loss\": 0.12561742961406708, \"time-step\": 3961}, {\"accuracy\": 0.75, \"loss\": 0.1256001889705658, \"time-step\": 3962}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3963}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 3964}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3965}, {\"accuracy\": 0.75, \"loss\": 0.12560011446475983, \"time-step\": 3966}, {\"accuracy\": 0.75, \"loss\": 0.12561747431755066, \"time-step\": 3967}, {\"accuracy\": 0.75, \"loss\": 0.1256001889705658, \"time-step\": 3968}, {\"accuracy\": 0.75, \"loss\": 0.1256173551082611, \"time-step\": 3969}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 3970}, {\"accuracy\": 0.75, \"loss\": 0.12561748921871185, \"time-step\": 3971}, {\"accuracy\": 0.75, \"loss\": 0.12560014426708221, \"time-step\": 3972}, {\"accuracy\": 0.75, \"loss\": 0.12561734020709991, \"time-step\": 3973}, {\"accuracy\": 0.75, \"loss\": 0.12560006976127625, \"time-step\": 3974}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3975}, {\"accuracy\": 0.75, \"loss\": 0.12560009956359863, \"time-step\": 3976}, {\"accuracy\": 0.75, \"loss\": 0.12561744451522827, \"time-step\": 3977}, {\"accuracy\": 0.75, \"loss\": 0.12560005486011505, \"time-step\": 3978}, {\"accuracy\": 0.75, \"loss\": 0.12561731040477753, \"time-step\": 3979}, {\"accuracy\": 0.75, \"loss\": 0.12560003995895386, \"time-step\": 3980}, {\"accuracy\": 0.75, \"loss\": 0.1256173998117447, \"time-step\": 3981}, {\"accuracy\": 0.75, \"loss\": 0.12560002505779266, \"time-step\": 3982}, {\"accuracy\": 0.75, \"loss\": 0.1256173551082611, \"time-step\": 3983}, {\"accuracy\": 0.75, \"loss\": 0.12559998035430908, \"time-step\": 3984}, {\"accuracy\": 0.75, \"loss\": 0.12561732530593872, \"time-step\": 3985}, {\"accuracy\": 0.75, \"loss\": 0.12560003995895386, \"time-step\": 3986}, {\"accuracy\": 0.75, \"loss\": 0.12561732530593872, \"time-step\": 3987}, {\"accuracy\": 0.75, \"loss\": 0.1255999505519867, \"time-step\": 3988}, {\"accuracy\": 0.75, \"loss\": 0.12561734020709991, \"time-step\": 3989}, {\"accuracy\": 0.75, \"loss\": 0.1255999654531479, \"time-step\": 3990}, {\"accuracy\": 0.75, \"loss\": 0.12561731040477753, \"time-step\": 3991}, {\"accuracy\": 0.75, \"loss\": 0.1255999207496643, \"time-step\": 3992}, {\"accuracy\": 0.75, \"loss\": 0.12561734020709991, \"time-step\": 3993}, {\"accuracy\": 0.75, \"loss\": 0.1255999207496643, \"time-step\": 3994}, {\"accuracy\": 0.75, \"loss\": 0.12561729550361633, \"time-step\": 3995}, {\"accuracy\": 0.75, \"loss\": 0.12559986114501953, \"time-step\": 3996}, {\"accuracy\": 0.75, \"loss\": 0.12561725080013275, \"time-step\": 3997}, {\"accuracy\": 0.75, \"loss\": 0.12559986114501953, \"time-step\": 3998}, {\"accuracy\": 0.75, \"loss\": 0.12561728060245514, \"time-step\": 3999}, {\"accuracy\": 0.75, \"loss\": 0.12559986114501953, \"time-step\": 4000}, {\"accuracy\": 0.75, \"loss\": 0.12561719119548798, \"time-step\": 4001}, {\"accuracy\": 0.75, \"loss\": 0.12559981644153595, \"time-step\": 4002}, {\"accuracy\": 0.75, \"loss\": 0.12561720609664917, \"time-step\": 4003}, {\"accuracy\": 0.75, \"loss\": 0.12559981644153595, \"time-step\": 4004}, {\"accuracy\": 0.75, \"loss\": 0.12561723589897156, \"time-step\": 4005}, {\"accuracy\": 0.75, \"loss\": 0.12559978663921356, \"time-step\": 4006}, {\"accuracy\": 0.75, \"loss\": 0.12561726570129395, \"time-step\": 4007}, {\"accuracy\": 0.75, \"loss\": 0.1255997270345688, \"time-step\": 4008}, {\"accuracy\": 0.75, \"loss\": 0.12561722099781036, \"time-step\": 4009}, {\"accuracy\": 0.75, \"loss\": 0.1255996972322464, \"time-step\": 4010}, {\"accuracy\": 0.75, \"loss\": 0.12561723589897156, \"time-step\": 4011}, {\"accuracy\": 0.75, \"loss\": 0.1255996972322464, \"time-step\": 4012}, {\"accuracy\": 0.75, \"loss\": 0.12561705708503723, \"time-step\": 4013}, {\"accuracy\": 0.75, \"loss\": 0.1255996823310852, \"time-step\": 4014}, {\"accuracy\": 0.75, \"loss\": 0.12561708688735962, \"time-step\": 4015}, {\"accuracy\": 0.75, \"loss\": 0.12559965252876282, \"time-step\": 4016}, {\"accuracy\": 0.75, \"loss\": 0.1256171613931656, \"time-step\": 4017}, {\"accuracy\": 0.75, \"loss\": 0.12559962272644043, \"time-step\": 4018}, {\"accuracy\": 0.75, \"loss\": 0.12561717629432678, \"time-step\": 4019}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 4020}, {\"accuracy\": 0.75, \"loss\": 0.1256171613931656, \"time-step\": 4021}, {\"accuracy\": 0.75, \"loss\": 0.12559957802295685, \"time-step\": 4022}, {\"accuracy\": 0.75, \"loss\": 0.1256171017885208, \"time-step\": 4023}, {\"accuracy\": 0.75, \"loss\": 0.12559960782527924, \"time-step\": 4024}, {\"accuracy\": 0.75, \"loss\": 0.1256171315908432, \"time-step\": 4025}, {\"accuracy\": 0.75, \"loss\": 0.12559960782527924, \"time-step\": 4026}, {\"accuracy\": 0.75, \"loss\": 0.1256171017885208, \"time-step\": 4027}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 4028}, {\"accuracy\": 0.75, \"loss\": 0.12561708688735962, \"time-step\": 4029}, {\"accuracy\": 0.75, \"loss\": 0.12559953331947327, \"time-step\": 4030}, {\"accuracy\": 0.75, \"loss\": 0.12561707198619843, \"time-step\": 4031}, {\"accuracy\": 0.75, \"loss\": 0.12559941411018372, \"time-step\": 4032}, {\"accuracy\": 0.75, \"loss\": 0.12561702728271484, \"time-step\": 4033}, {\"accuracy\": 0.75, \"loss\": 0.1255994737148285, \"time-step\": 4034}, {\"accuracy\": 0.75, \"loss\": 0.12561704218387604, \"time-step\": 4035}, {\"accuracy\": 0.75, \"loss\": 0.12559948861598969, \"time-step\": 4036}, {\"accuracy\": 0.75, \"loss\": 0.12561702728271484, \"time-step\": 4037}, {\"accuracy\": 0.75, \"loss\": 0.12559935450553894, \"time-step\": 4038}, {\"accuracy\": 0.75, \"loss\": 0.12561701238155365, \"time-step\": 4039}, {\"accuracy\": 0.75, \"loss\": 0.12559935450553894, \"time-step\": 4040}, {\"accuracy\": 0.75, \"loss\": 0.12561698257923126, \"time-step\": 4041}, {\"accuracy\": 0.75, \"loss\": 0.12559933960437775, \"time-step\": 4042}, {\"accuracy\": 0.75, \"loss\": 0.12561696767807007, \"time-step\": 4043}, {\"accuracy\": 0.75, \"loss\": 0.12559932470321655, \"time-step\": 4044}, {\"accuracy\": 0.75, \"loss\": 0.12561695277690887, \"time-step\": 4045}, {\"accuracy\": 0.75, \"loss\": 0.1255992203950882, \"time-step\": 4046}, {\"accuracy\": 0.75, \"loss\": 0.1256168633699417, \"time-step\": 4047}, {\"accuracy\": 0.75, \"loss\": 0.12559932470321655, \"time-step\": 4048}, {\"accuracy\": 0.75, \"loss\": 0.1256168931722641, \"time-step\": 4049}, {\"accuracy\": 0.75, \"loss\": 0.1255992352962494, \"time-step\": 4050}, {\"accuracy\": 0.75, \"loss\": 0.1256168931722641, \"time-step\": 4051}, {\"accuracy\": 0.75, \"loss\": 0.1255992203950882, \"time-step\": 4052}, {\"accuracy\": 0.75, \"loss\": 0.1256168633699417, \"time-step\": 4053}, {\"accuracy\": 0.75, \"loss\": 0.1255991905927658, \"time-step\": 4054}, {\"accuracy\": 0.75, \"loss\": 0.1256168782711029, \"time-step\": 4055}, {\"accuracy\": 0.75, \"loss\": 0.1255991905927658, \"time-step\": 4056}, {\"accuracy\": 0.75, \"loss\": 0.12561681866645813, \"time-step\": 4057}, {\"accuracy\": 0.75, \"loss\": 0.12559913098812103, \"time-step\": 4058}, {\"accuracy\": 0.75, \"loss\": 0.12561683356761932, \"time-step\": 4059}, {\"accuracy\": 0.75, \"loss\": 0.12559916079044342, \"time-step\": 4060}, {\"accuracy\": 0.75, \"loss\": 0.12561678886413574, \"time-step\": 4061}, {\"accuracy\": 0.75, \"loss\": 0.12559904158115387, \"time-step\": 4062}, {\"accuracy\": 0.75, \"loss\": 0.12561675906181335, \"time-step\": 4063}, {\"accuracy\": 0.75, \"loss\": 0.12559901177883148, \"time-step\": 4064}, {\"accuracy\": 0.75, \"loss\": 0.12561675906181335, \"time-step\": 4065}, {\"accuracy\": 0.75, \"loss\": 0.12559911608695984, \"time-step\": 4066}, {\"accuracy\": 0.75, \"loss\": 0.12561674416065216, \"time-step\": 4067}, {\"accuracy\": 0.75, \"loss\": 0.12559901177883148, \"time-step\": 4068}, {\"accuracy\": 0.75, \"loss\": 0.12561671435832977, \"time-step\": 4069}, {\"accuracy\": 0.75, \"loss\": 0.1255989521741867, \"time-step\": 4070}, {\"accuracy\": 0.75, \"loss\": 0.1256166398525238, \"time-step\": 4071}, {\"accuracy\": 0.75, \"loss\": 0.12559890747070312, \"time-step\": 4072}, {\"accuracy\": 0.75, \"loss\": 0.12561661005020142, \"time-step\": 4073}, {\"accuracy\": 0.75, \"loss\": 0.12559886276721954, \"time-step\": 4074}, {\"accuracy\": 0.75, \"loss\": 0.12561655044555664, \"time-step\": 4075}, {\"accuracy\": 0.75, \"loss\": 0.12559884786605835, \"time-step\": 4076}, {\"accuracy\": 0.75, \"loss\": 0.125616654753685, \"time-step\": 4077}, {\"accuracy\": 0.75, \"loss\": 0.12559886276721954, \"time-step\": 4078}, {\"accuracy\": 0.75, \"loss\": 0.12561668455600739, \"time-step\": 4079}, {\"accuracy\": 0.75, \"loss\": 0.12559883296489716, \"time-step\": 4080}, {\"accuracy\": 0.75, \"loss\": 0.12561659514904022, \"time-step\": 4081}, {\"accuracy\": 0.75, \"loss\": 0.12559874355793, \"time-step\": 4082}, {\"accuracy\": 0.75, \"loss\": 0.12561659514904022, \"time-step\": 4083}, {\"accuracy\": 0.75, \"loss\": 0.12559877336025238, \"time-step\": 4084}, {\"accuracy\": 0.75, \"loss\": 0.12561661005020142, \"time-step\": 4085}, {\"accuracy\": 0.75, \"loss\": 0.1255987137556076, \"time-step\": 4086}, {\"accuracy\": 0.75, \"loss\": 0.12561655044555664, \"time-step\": 4087}, {\"accuracy\": 0.75, \"loss\": 0.12559862434864044, \"time-step\": 4088}, {\"accuracy\": 0.75, \"loss\": 0.12561650574207306, \"time-step\": 4089}, {\"accuracy\": 0.75, \"loss\": 0.12559862434864044, \"time-step\": 4090}, {\"accuracy\": 0.75, \"loss\": 0.12561649084091187, \"time-step\": 4091}, {\"accuracy\": 0.75, \"loss\": 0.12559866905212402, \"time-step\": 4092}, {\"accuracy\": 0.75, \"loss\": 0.12561644613742828, \"time-step\": 4093}, {\"accuracy\": 0.75, \"loss\": 0.12559860944747925, \"time-step\": 4094}, {\"accuracy\": 0.75, \"loss\": 0.12561635673046112, \"time-step\": 4095}, {\"accuracy\": 0.75, \"loss\": 0.12559852004051208, \"time-step\": 4096}, {\"accuracy\": 0.75, \"loss\": 0.1256163865327835, \"time-step\": 4097}, {\"accuracy\": 0.75, \"loss\": 0.12559862434864044, \"time-step\": 4098}, {\"accuracy\": 0.75, \"loss\": 0.12561637163162231, \"time-step\": 4099}, {\"accuracy\": 0.75, \"loss\": 0.1255984753370285, \"time-step\": 4100}, {\"accuracy\": 0.75, \"loss\": 0.1256163865327835, \"time-step\": 4101}, {\"accuracy\": 0.75, \"loss\": 0.1255984604358673, \"time-step\": 4102}, {\"accuracy\": 0.75, \"loss\": 0.12561628222465515, \"time-step\": 4103}, {\"accuracy\": 0.75, \"loss\": 0.1255985051393509, \"time-step\": 4104}, {\"accuracy\": 0.75, \"loss\": 0.12561632692813873, \"time-step\": 4105}, {\"accuracy\": 0.75, \"loss\": 0.12559844553470612, \"time-step\": 4106}, {\"accuracy\": 0.75, \"loss\": 0.12561626732349396, \"time-step\": 4107}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 4108}, {\"accuracy\": 0.75, \"loss\": 0.12561628222465515, \"time-step\": 4109}, {\"accuracy\": 0.75, \"loss\": 0.12559838593006134, \"time-step\": 4110}, {\"accuracy\": 0.75, \"loss\": 0.12561629712581635, \"time-step\": 4111}, {\"accuracy\": 0.75, \"loss\": 0.12559835612773895, \"time-step\": 4112}, {\"accuracy\": 0.75, \"loss\": 0.12561626732349396, \"time-step\": 4113}, {\"accuracy\": 0.75, \"loss\": 0.12559828162193298, \"time-step\": 4114}, {\"accuracy\": 0.75, \"loss\": 0.12561628222465515, \"time-step\": 4115}, {\"accuracy\": 0.75, \"loss\": 0.1255982518196106, \"time-step\": 4116}, {\"accuracy\": 0.75, \"loss\": 0.12561620771884918, \"time-step\": 4117}, {\"accuracy\": 0.75, \"loss\": 0.12559820711612701, \"time-step\": 4118}, {\"accuracy\": 0.75, \"loss\": 0.1256161779165268, \"time-step\": 4119}, {\"accuracy\": 0.75, \"loss\": 0.12559819221496582, \"time-step\": 4120}, {\"accuracy\": 0.75, \"loss\": 0.1256161779165268, \"time-step\": 4121}, {\"accuracy\": 0.75, \"loss\": 0.12559813261032104, \"time-step\": 4122}, {\"accuracy\": 0.75, \"loss\": 0.1256161630153656, \"time-step\": 4123}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 4124}, {\"accuracy\": 0.75, \"loss\": 0.1256161332130432, \"time-step\": 4125}, {\"accuracy\": 0.75, \"loss\": 0.12559805810451508, \"time-step\": 4126}, {\"accuracy\": 0.75, \"loss\": 0.12561596930027008, \"time-step\": 4127}, {\"accuracy\": 0.75, \"loss\": 0.12559810280799866, \"time-step\": 4128}, {\"accuracy\": 0.75, \"loss\": 0.12561604380607605, \"time-step\": 4129}, {\"accuracy\": 0.75, \"loss\": 0.12559807300567627, \"time-step\": 4130}, {\"accuracy\": 0.75, \"loss\": 0.12561596930027008, \"time-step\": 4131}, {\"accuracy\": 0.75, \"loss\": 0.1255979686975479, \"time-step\": 4132}, {\"accuracy\": 0.75, \"loss\": 0.1256159096956253, \"time-step\": 4133}, {\"accuracy\": 0.75, \"loss\": 0.1255979835987091, \"time-step\": 4134}, {\"accuracy\": 0.75, \"loss\": 0.1256159394979477, \"time-step\": 4135}, {\"accuracy\": 0.75, \"loss\": 0.12559792399406433, \"time-step\": 4136}, {\"accuracy\": 0.75, \"loss\": 0.1256159543991089, \"time-step\": 4137}, {\"accuracy\": 0.75, \"loss\": 0.12559783458709717, \"time-step\": 4138}, {\"accuracy\": 0.75, \"loss\": 0.12561587989330292, \"time-step\": 4139}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 4140}, {\"accuracy\": 0.75, \"loss\": 0.12561586499214172, \"time-step\": 4141}, {\"accuracy\": 0.75, \"loss\": 0.12559786438941956, \"time-step\": 4142}, {\"accuracy\": 0.75, \"loss\": 0.12561583518981934, \"time-step\": 4143}, {\"accuracy\": 0.75, \"loss\": 0.1255977302789688, \"time-step\": 4144}, {\"accuracy\": 0.75, \"loss\": 0.12561583518981934, \"time-step\": 4145}, {\"accuracy\": 0.75, \"loss\": 0.12559780478477478, \"time-step\": 4146}, {\"accuracy\": 0.75, \"loss\": 0.12561579048633575, \"time-step\": 4147}, {\"accuracy\": 0.75, \"loss\": 0.1255977302789688, \"time-step\": 4148}, {\"accuracy\": 0.75, \"loss\": 0.12561583518981934, \"time-step\": 4149}, {\"accuracy\": 0.75, \"loss\": 0.12559771537780762, \"time-step\": 4150}, {\"accuracy\": 0.75, \"loss\": 0.12561574578285217, \"time-step\": 4151}, {\"accuracy\": 0.75, \"loss\": 0.12559765577316284, \"time-step\": 4152}, {\"accuracy\": 0.75, \"loss\": 0.12561574578285217, \"time-step\": 4153}, {\"accuracy\": 0.75, \"loss\": 0.12559768557548523, \"time-step\": 4154}, {\"accuracy\": 0.75, \"loss\": 0.12561573088169098, \"time-step\": 4155}, {\"accuracy\": 0.75, \"loss\": 0.12559756636619568, \"time-step\": 4156}, {\"accuracy\": 0.75, \"loss\": 0.1256156712770462, \"time-step\": 4157}, {\"accuracy\": 0.75, \"loss\": 0.12559758126735687, \"time-step\": 4158}, {\"accuracy\": 0.75, \"loss\": 0.1256156712770462, \"time-step\": 4159}, {\"accuracy\": 0.75, \"loss\": 0.1255975067615509, \"time-step\": 4160}, {\"accuracy\": 0.75, \"loss\": 0.12561564147472382, \"time-step\": 4161}, {\"accuracy\": 0.75, \"loss\": 0.1255974918603897, \"time-step\": 4162}, {\"accuracy\": 0.75, \"loss\": 0.12561558187007904, \"time-step\": 4163}, {\"accuracy\": 0.75, \"loss\": 0.12559734284877777, \"time-step\": 4164}, {\"accuracy\": 0.75, \"loss\": 0.12561550736427307, \"time-step\": 4165}, {\"accuracy\": 0.75, \"loss\": 0.12559740245342255, \"time-step\": 4166}, {\"accuracy\": 0.75, \"loss\": 0.12561553716659546, \"time-step\": 4167}, {\"accuracy\": 0.75, \"loss\": 0.12559731304645538, \"time-step\": 4168}, {\"accuracy\": 0.75, \"loss\": 0.12561550736427307, \"time-step\": 4169}, {\"accuracy\": 0.75, \"loss\": 0.12559732794761658, \"time-step\": 4170}, {\"accuracy\": 0.75, \"loss\": 0.12561549246311188, \"time-step\": 4171}, {\"accuracy\": 0.75, \"loss\": 0.12559732794761658, \"time-step\": 4172}, {\"accuracy\": 0.75, \"loss\": 0.1256154477596283, \"time-step\": 4173}, {\"accuracy\": 0.75, \"loss\": 0.12559731304645538, \"time-step\": 4174}, {\"accuracy\": 0.75, \"loss\": 0.12561538815498352, \"time-step\": 4175}, {\"accuracy\": 0.75, \"loss\": 0.1255972683429718, \"time-step\": 4176}, {\"accuracy\": 0.75, \"loss\": 0.1256154179573059, \"time-step\": 4177}, {\"accuracy\": 0.75, \"loss\": 0.1255972534418106, \"time-step\": 4178}, {\"accuracy\": 0.75, \"loss\": 0.1256154328584671, \"time-step\": 4179}, {\"accuracy\": 0.75, \"loss\": 0.12559717893600464, \"time-step\": 4180}, {\"accuracy\": 0.75, \"loss\": 0.12561529874801636, \"time-step\": 4181}, {\"accuracy\": 0.75, \"loss\": 0.12559708952903748, \"time-step\": 4182}, {\"accuracy\": 0.75, \"loss\": 0.12561532855033875, \"time-step\": 4183}, {\"accuracy\": 0.75, \"loss\": 0.12559708952903748, \"time-step\": 4184}, {\"accuracy\": 0.75, \"loss\": 0.1256152093410492, \"time-step\": 4185}, {\"accuracy\": 0.75, \"loss\": 0.12559695541858673, \"time-step\": 4186}, {\"accuracy\": 0.75, \"loss\": 0.12561525404453278, \"time-step\": 4187}, {\"accuracy\": 0.75, \"loss\": 0.12559695541858673, \"time-step\": 4188}, {\"accuracy\": 0.75, \"loss\": 0.1256151795387268, \"time-step\": 4189}, {\"accuracy\": 0.75, \"loss\": 0.12559698522090912, \"time-step\": 4190}, {\"accuracy\": 0.75, \"loss\": 0.12561510503292084, \"time-step\": 4191}, {\"accuracy\": 0.75, \"loss\": 0.12559689581394196, \"time-step\": 4192}, {\"accuracy\": 0.75, \"loss\": 0.12561510503292084, \"time-step\": 4193}, {\"accuracy\": 0.75, \"loss\": 0.12559689581394196, \"time-step\": 4194}, {\"accuracy\": 0.75, \"loss\": 0.125615194439888, \"time-step\": 4195}, {\"accuracy\": 0.75, \"loss\": 0.1255968064069748, \"time-step\": 4196}, {\"accuracy\": 0.75, \"loss\": 0.12561506032943726, \"time-step\": 4197}, {\"accuracy\": 0.75, \"loss\": 0.1255967915058136, \"time-step\": 4198}, {\"accuracy\": 0.75, \"loss\": 0.12561507523059845, \"time-step\": 4199}, {\"accuracy\": 0.75, \"loss\": 0.1255967766046524, \"time-step\": 4200}, {\"accuracy\": 0.75, \"loss\": 0.12561507523059845, \"time-step\": 4201}, {\"accuracy\": 0.75, \"loss\": 0.12559674680233002, \"time-step\": 4202}, {\"accuracy\": 0.75, \"loss\": 0.1256149858236313, \"time-step\": 4203}, {\"accuracy\": 0.75, \"loss\": 0.12559667229652405, \"time-step\": 4204}, {\"accuracy\": 0.75, \"loss\": 0.12561491131782532, \"time-step\": 4205}, {\"accuracy\": 0.75, \"loss\": 0.12559667229652405, \"time-step\": 4206}, {\"accuracy\": 0.75, \"loss\": 0.1256149560213089, \"time-step\": 4207}, {\"accuracy\": 0.75, \"loss\": 0.1255965679883957, \"time-step\": 4208}, {\"accuracy\": 0.75, \"loss\": 0.12561489641666412, \"time-step\": 4209}, {\"accuracy\": 0.75, \"loss\": 0.12559661269187927, \"time-step\": 4210}, {\"accuracy\": 0.75, \"loss\": 0.12561479210853577, \"time-step\": 4211}, {\"accuracy\": 0.75, \"loss\": 0.1255965232849121, \"time-step\": 4212}, {\"accuracy\": 0.75, \"loss\": 0.1256149262189865, \"time-step\": 4213}, {\"accuracy\": 0.75, \"loss\": 0.12559649348258972, \"time-step\": 4214}, {\"accuracy\": 0.75, \"loss\": 0.12561474740505219, \"time-step\": 4215}, {\"accuracy\": 0.75, \"loss\": 0.1255965530872345, \"time-step\": 4216}, {\"accuracy\": 0.75, \"loss\": 0.1256147176027298, \"time-step\": 4217}, {\"accuracy\": 0.75, \"loss\": 0.12559643387794495, \"time-step\": 4218}, {\"accuracy\": 0.75, \"loss\": 0.1256147027015686, \"time-step\": 4219}, {\"accuracy\": 0.75, \"loss\": 0.12559640407562256, \"time-step\": 4220}, {\"accuracy\": 0.75, \"loss\": 0.1256147176027298, \"time-step\": 4221}, {\"accuracy\": 0.75, \"loss\": 0.12559643387794495, \"time-step\": 4222}, {\"accuracy\": 0.75, \"loss\": 0.12561467289924622, \"time-step\": 4223}, {\"accuracy\": 0.75, \"loss\": 0.125596284866333, \"time-step\": 4224}, {\"accuracy\": 0.75, \"loss\": 0.12561464309692383, \"time-step\": 4225}, {\"accuracy\": 0.75, \"loss\": 0.1255962997674942, \"time-step\": 4226}, {\"accuracy\": 0.75, \"loss\": 0.12561462819576263, \"time-step\": 4227}, {\"accuracy\": 0.75, \"loss\": 0.12559625506401062, \"time-step\": 4228}, {\"accuracy\": 0.75, \"loss\": 0.12561455368995667, \"time-step\": 4229}, {\"accuracy\": 0.75, \"loss\": 0.12559615075588226, \"time-step\": 4230}, {\"accuracy\": 0.75, \"loss\": 0.12561453878879547, \"time-step\": 4231}, {\"accuracy\": 0.75, \"loss\": 0.12559615075588226, \"time-step\": 4232}, {\"accuracy\": 0.75, \"loss\": 0.12561450898647308, \"time-step\": 4233}, {\"accuracy\": 0.75, \"loss\": 0.1255960911512375, \"time-step\": 4234}, {\"accuracy\": 0.75, \"loss\": 0.1256144493818283, \"time-step\": 4235}, {\"accuracy\": 0.75, \"loss\": 0.12559612095355988, \"time-step\": 4236}, {\"accuracy\": 0.75, \"loss\": 0.1256144493818283, \"time-step\": 4237}, {\"accuracy\": 0.75, \"loss\": 0.12559601664543152, \"time-step\": 4238}, {\"accuracy\": 0.75, \"loss\": 0.12561440467834473, \"time-step\": 4239}, {\"accuracy\": 0.75, \"loss\": 0.12559595704078674, \"time-step\": 4240}, {\"accuracy\": 0.75, \"loss\": 0.12561433017253876, \"time-step\": 4241}, {\"accuracy\": 0.75, \"loss\": 0.12559591233730316, \"time-step\": 4242}, {\"accuracy\": 0.75, \"loss\": 0.12561431527137756, \"time-step\": 4243}, {\"accuracy\": 0.75, \"loss\": 0.12559592723846436, \"time-step\": 4244}, {\"accuracy\": 0.75, \"loss\": 0.12561427056789398, \"time-step\": 4245}, {\"accuracy\": 0.75, \"loss\": 0.1255958527326584, \"time-step\": 4246}, {\"accuracy\": 0.75, \"loss\": 0.1256142556667328, \"time-step\": 4247}, {\"accuracy\": 0.75, \"loss\": 0.12559576332569122, \"time-step\": 4248}, {\"accuracy\": 0.75, \"loss\": 0.12561427056789398, \"time-step\": 4249}, {\"accuracy\": 0.75, \"loss\": 0.12559573352336884, \"time-step\": 4250}, {\"accuracy\": 0.75, \"loss\": 0.12561412155628204, \"time-step\": 4251}, {\"accuracy\": 0.75, \"loss\": 0.12559568881988525, \"time-step\": 4252}, {\"accuracy\": 0.75, \"loss\": 0.12561409175395966, \"time-step\": 4253}, {\"accuracy\": 0.75, \"loss\": 0.12559562921524048, \"time-step\": 4254}, {\"accuracy\": 0.75, \"loss\": 0.12561404705047607, \"time-step\": 4255}, {\"accuracy\": 0.75, \"loss\": 0.12559565901756287, \"time-step\": 4256}, {\"accuracy\": 0.75, \"loss\": 0.12561406195163727, \"time-step\": 4257}, {\"accuracy\": 0.75, \"loss\": 0.12559561431407928, \"time-step\": 4258}, {\"accuracy\": 0.75, \"loss\": 0.1256139874458313, \"time-step\": 4259}, {\"accuracy\": 0.75, \"loss\": 0.1255955994129181, \"time-step\": 4260}, {\"accuracy\": 0.75, \"loss\": 0.12561392784118652, \"time-step\": 4261}, {\"accuracy\": 0.75, \"loss\": 0.12559545040130615, \"time-step\": 4262}, {\"accuracy\": 0.75, \"loss\": 0.1256139576435089, \"time-step\": 4263}, {\"accuracy\": 0.75, \"loss\": 0.12559545040130615, \"time-step\": 4264}, {\"accuracy\": 0.75, \"loss\": 0.12561389803886414, \"time-step\": 4265}, {\"accuracy\": 0.75, \"loss\": 0.12559539079666138, \"time-step\": 4266}, {\"accuracy\": 0.75, \"loss\": 0.12561388313770294, \"time-step\": 4267}, {\"accuracy\": 0.75, \"loss\": 0.12559537589550018, \"time-step\": 4268}, {\"accuracy\": 0.75, \"loss\": 0.12561380863189697, \"time-step\": 4269}, {\"accuracy\": 0.75, \"loss\": 0.12559527158737183, \"time-step\": 4270}, {\"accuracy\": 0.75, \"loss\": 0.12561385333538055, \"time-step\": 4271}, {\"accuracy\": 0.75, \"loss\": 0.12559521198272705, \"time-step\": 4272}, {\"accuracy\": 0.75, \"loss\": 0.12561377882957458, \"time-step\": 4273}, {\"accuracy\": 0.75, \"loss\": 0.12559521198272705, \"time-step\": 4274}, {\"accuracy\": 0.75, \"loss\": 0.12561367452144623, \"time-step\": 4275}, {\"accuracy\": 0.75, \"loss\": 0.12559515237808228, \"time-step\": 4276}, {\"accuracy\": 0.75, \"loss\": 0.12561367452144623, \"time-step\": 4277}, {\"accuracy\": 0.75, \"loss\": 0.1255950778722763, \"time-step\": 4278}, {\"accuracy\": 0.75, \"loss\": 0.12561360001564026, \"time-step\": 4279}, {\"accuracy\": 0.75, \"loss\": 0.1255950629711151, \"time-step\": 4280}, {\"accuracy\": 0.75, \"loss\": 0.12561358511447906, \"time-step\": 4281}, {\"accuracy\": 0.75, \"loss\": 0.12559500336647034, \"time-step\": 4282}, {\"accuracy\": 0.75, \"loss\": 0.1256135106086731, \"time-step\": 4283}, {\"accuracy\": 0.75, \"loss\": 0.12559500336647034, \"time-step\": 4284}, {\"accuracy\": 0.75, \"loss\": 0.1256134808063507, \"time-step\": 4285}, {\"accuracy\": 0.75, \"loss\": 0.12559492886066437, \"time-step\": 4286}, {\"accuracy\": 0.75, \"loss\": 0.12561345100402832, \"time-step\": 4287}, {\"accuracy\": 0.75, \"loss\": 0.1255948543548584, \"time-step\": 4288}, {\"accuracy\": 0.75, \"loss\": 0.12561334669589996, \"time-step\": 4289}, {\"accuracy\": 0.75, \"loss\": 0.1255948692560196, \"time-step\": 4290}, {\"accuracy\": 0.75, \"loss\": 0.12561336159706116, \"time-step\": 4291}, {\"accuracy\": 0.75, \"loss\": 0.1255948841571808, \"time-step\": 4292}, {\"accuracy\": 0.75, \"loss\": 0.12561337649822235, \"time-step\": 4293}, {\"accuracy\": 0.75, \"loss\": 0.12559476494789124, \"time-step\": 4294}, {\"accuracy\": 0.75, \"loss\": 0.12561330199241638, \"time-step\": 4295}, {\"accuracy\": 0.75, \"loss\": 0.12559470534324646, \"time-step\": 4296}, {\"accuracy\": 0.75, \"loss\": 0.1256132870912552, \"time-step\": 4297}, {\"accuracy\": 0.75, \"loss\": 0.12559466063976288, \"time-step\": 4298}, {\"accuracy\": 0.75, \"loss\": 0.12561319768428802, \"time-step\": 4299}, {\"accuracy\": 0.75, \"loss\": 0.1255946159362793, \"time-step\": 4300}, {\"accuracy\": 0.75, \"loss\": 0.12561321258544922, \"time-step\": 4301}, {\"accuracy\": 0.75, \"loss\": 0.1255946159362793, \"time-step\": 4302}, {\"accuracy\": 0.75, \"loss\": 0.12561312317848206, \"time-step\": 4303}, {\"accuracy\": 0.75, \"loss\": 0.12559446692466736, \"time-step\": 4304}, {\"accuracy\": 0.75, \"loss\": 0.12561315298080444, \"time-step\": 4305}, {\"accuracy\": 0.75, \"loss\": 0.12559449672698975, \"time-step\": 4306}, {\"accuracy\": 0.75, \"loss\": 0.12561309337615967, \"time-step\": 4307}, {\"accuracy\": 0.75, \"loss\": 0.12559446692466736, \"time-step\": 4308}, {\"accuracy\": 0.75, \"loss\": 0.1256129890680313, \"time-step\": 4309}, {\"accuracy\": 0.75, \"loss\": 0.12559440732002258, \"time-step\": 4310}, {\"accuracy\": 0.75, \"loss\": 0.1256130188703537, \"time-step\": 4311}, {\"accuracy\": 0.75, \"loss\": 0.12559430301189423, \"time-step\": 4312}, {\"accuracy\": 0.75, \"loss\": 0.12561297416687012, \"time-step\": 4313}, {\"accuracy\": 0.75, \"loss\": 0.12559431791305542, \"time-step\": 4314}, {\"accuracy\": 0.75, \"loss\": 0.12561286985874176, \"time-step\": 4315}, {\"accuracy\": 0.75, \"loss\": 0.12559425830841064, \"time-step\": 4316}, {\"accuracy\": 0.75, \"loss\": 0.12561291456222534, \"time-step\": 4317}, {\"accuracy\": 0.75, \"loss\": 0.12559419870376587, \"time-step\": 4318}, {\"accuracy\": 0.75, \"loss\": 0.12561282515525818, \"time-step\": 4319}, {\"accuracy\": 0.75, \"loss\": 0.1255941540002823, \"time-step\": 4320}, {\"accuracy\": 0.75, \"loss\": 0.1256127655506134, \"time-step\": 4321}, {\"accuracy\": 0.75, \"loss\": 0.12559407949447632, \"time-step\": 4322}, {\"accuracy\": 0.75, \"loss\": 0.12561273574829102, \"time-step\": 4323}, {\"accuracy\": 0.75, \"loss\": 0.12559403479099274, \"time-step\": 4324}, {\"accuracy\": 0.75, \"loss\": 0.12561272084712982, \"time-step\": 4325}, {\"accuracy\": 0.75, \"loss\": 0.12559400498867035, \"time-step\": 4326}, {\"accuracy\": 0.75, \"loss\": 0.12561266124248505, \"time-step\": 4327}, {\"accuracy\": 0.75, \"loss\": 0.12559397518634796, \"time-step\": 4328}, {\"accuracy\": 0.75, \"loss\": 0.12561258673667908, \"time-step\": 4329}, {\"accuracy\": 0.75, \"loss\": 0.1255938708782196, \"time-step\": 4330}, {\"accuracy\": 0.75, \"loss\": 0.1256125122308731, \"time-step\": 4331}, {\"accuracy\": 0.75, \"loss\": 0.1255938559770584, \"time-step\": 4332}, {\"accuracy\": 0.75, \"loss\": 0.1256125271320343, \"time-step\": 4333}, {\"accuracy\": 0.75, \"loss\": 0.12559378147125244, \"time-step\": 4334}, {\"accuracy\": 0.75, \"loss\": 0.12561243772506714, \"time-step\": 4335}, {\"accuracy\": 0.75, \"loss\": 0.12559375166893005, \"time-step\": 4336}, {\"accuracy\": 0.75, \"loss\": 0.12561246752738953, \"time-step\": 4337}, {\"accuracy\": 0.75, \"loss\": 0.12559367716312408, \"time-step\": 4338}, {\"accuracy\": 0.75, \"loss\": 0.12561240792274475, \"time-step\": 4339}, {\"accuracy\": 0.75, \"loss\": 0.12559369206428528, \"time-step\": 4340}, {\"accuracy\": 0.75, \"loss\": 0.12561237812042236, \"time-step\": 4341}, {\"accuracy\": 0.75, \"loss\": 0.12559360265731812, \"time-step\": 4342}, {\"accuracy\": 0.75, \"loss\": 0.12561234831809998, \"time-step\": 4343}, {\"accuracy\": 0.75, \"loss\": 0.12559355795383453, \"time-step\": 4344}, {\"accuracy\": 0.75, \"loss\": 0.12561213970184326, \"time-step\": 4345}, {\"accuracy\": 0.75, \"loss\": 0.12559354305267334, \"time-step\": 4346}, {\"accuracy\": 0.75, \"loss\": 0.12561219930648804, \"time-step\": 4347}, {\"accuracy\": 0.75, \"loss\": 0.12559351325035095, \"time-step\": 4348}, {\"accuracy\": 0.75, \"loss\": 0.12561216950416565, \"time-step\": 4349}, {\"accuracy\": 0.75, \"loss\": 0.1255934238433838, \"time-step\": 4350}, {\"accuracy\": 0.75, \"loss\": 0.12561210989952087, \"time-step\": 4351}, {\"accuracy\": 0.75, \"loss\": 0.125593364238739, \"time-step\": 4352}, {\"accuracy\": 0.75, \"loss\": 0.12561212480068207, \"time-step\": 4353}, {\"accuracy\": 0.75, \"loss\": 0.12559331953525543, \"time-step\": 4354}, {\"accuracy\": 0.75, \"loss\": 0.1256120502948761, \"time-step\": 4355}, {\"accuracy\": 0.75, \"loss\": 0.12559320032596588, \"time-step\": 4356}, {\"accuracy\": 0.75, \"loss\": 0.12561194598674774, \"time-step\": 4357}, {\"accuracy\": 0.75, \"loss\": 0.12559327483177185, \"time-step\": 4358}, {\"accuracy\": 0.75, \"loss\": 0.12561196088790894, \"time-step\": 4359}, {\"accuracy\": 0.75, \"loss\": 0.12559321522712708, \"time-step\": 4360}, {\"accuracy\": 0.75, \"loss\": 0.12561190128326416, \"time-step\": 4361}, {\"accuracy\": 0.75, \"loss\": 0.1255931556224823, \"time-step\": 4362}, {\"accuracy\": 0.75, \"loss\": 0.12561188638210297, \"time-step\": 4363}, {\"accuracy\": 0.75, \"loss\": 0.12559303641319275, \"time-step\": 4364}, {\"accuracy\": 0.75, \"loss\": 0.125611811876297, \"time-step\": 4365}, {\"accuracy\": 0.75, \"loss\": 0.12559302151203156, \"time-step\": 4366}, {\"accuracy\": 0.75, \"loss\": 0.12561175227165222, \"time-step\": 4367}, {\"accuracy\": 0.75, \"loss\": 0.1255929172039032, \"time-step\": 4368}, {\"accuracy\": 0.75, \"loss\": 0.12561170756816864, \"time-step\": 4369}, {\"accuracy\": 0.75, \"loss\": 0.125592902302742, \"time-step\": 4370}, {\"accuracy\": 0.75, \"loss\": 0.12561169266700745, \"time-step\": 4371}, {\"accuracy\": 0.75, \"loss\": 0.1255929172039032, \"time-step\": 4372}, {\"accuracy\": 0.75, \"loss\": 0.1256115585565567, \"time-step\": 4373}, {\"accuracy\": 0.75, \"loss\": 0.12559276819229126, \"time-step\": 4374}, {\"accuracy\": 0.75, \"loss\": 0.1256115734577179, \"time-step\": 4375}, {\"accuracy\": 0.75, \"loss\": 0.12559273838996887, \"time-step\": 4376}, {\"accuracy\": 0.75, \"loss\": 0.1256115585565567, \"time-step\": 4377}, {\"accuracy\": 0.75, \"loss\": 0.1255926787853241, \"time-step\": 4378}, {\"accuracy\": 0.75, \"loss\": 0.12561139464378357, \"time-step\": 4379}, {\"accuracy\": 0.75, \"loss\": 0.12559263408184052, \"time-step\": 4380}, {\"accuracy\": 0.75, \"loss\": 0.12561143934726715, \"time-step\": 4381}, {\"accuracy\": 0.75, \"loss\": 0.1255926638841629, \"time-step\": 4382}, {\"accuracy\": 0.75, \"loss\": 0.12561139464378357, \"time-step\": 4383}, {\"accuracy\": 0.75, \"loss\": 0.12559254467487335, \"time-step\": 4384}, {\"accuracy\": 0.75, \"loss\": 0.1256113499403, \"time-step\": 4385}, {\"accuracy\": 0.75, \"loss\": 0.12559248507022858, \"time-step\": 4386}, {\"accuracy\": 0.75, \"loss\": 0.1256112903356552, \"time-step\": 4387}, {\"accuracy\": 0.75, \"loss\": 0.12559249997138977, \"time-step\": 4388}, {\"accuracy\": 0.75, \"loss\": 0.12561127543449402, \"time-step\": 4389}, {\"accuracy\": 0.75, \"loss\": 0.1255923956632614, \"time-step\": 4390}, {\"accuracy\": 0.75, \"loss\": 0.12561124563217163, \"time-step\": 4391}, {\"accuracy\": 0.75, \"loss\": 0.12559227645397186, \"time-step\": 4392}, {\"accuracy\": 0.75, \"loss\": 0.12561115622520447, \"time-step\": 4393}, {\"accuracy\": 0.75, \"loss\": 0.12559232115745544, \"time-step\": 4394}, {\"accuracy\": 0.75, \"loss\": 0.1256110817193985, \"time-step\": 4395}, {\"accuracy\": 0.75, \"loss\": 0.12559223175048828, \"time-step\": 4396}, {\"accuracy\": 0.75, \"loss\": 0.12561103701591492, \"time-step\": 4397}, {\"accuracy\": 0.75, \"loss\": 0.1255921572446823, \"time-step\": 4398}, {\"accuracy\": 0.75, \"loss\": 0.12561093270778656, \"time-step\": 4399}, {\"accuracy\": 0.75, \"loss\": 0.12559206783771515, \"time-step\": 4400}, {\"accuracy\": 0.75, \"loss\": 0.12561097741127014, \"time-step\": 4401}, {\"accuracy\": 0.75, \"loss\": 0.12559205293655396, \"time-step\": 4402}, {\"accuracy\": 0.75, \"loss\": 0.12561087310314178, \"time-step\": 4403}, {\"accuracy\": 0.75, \"loss\": 0.12559205293655396, \"time-step\": 4404}, {\"accuracy\": 0.75, \"loss\": 0.12561088800430298, \"time-step\": 4405}, {\"accuracy\": 0.75, \"loss\": 0.12559197843074799, \"time-step\": 4406}, {\"accuracy\": 0.75, \"loss\": 0.12561076879501343, \"time-step\": 4407}, {\"accuracy\": 0.75, \"loss\": 0.12559200823307037, \"time-step\": 4408}, {\"accuracy\": 0.75, \"loss\": 0.1256108283996582, \"time-step\": 4409}, {\"accuracy\": 0.75, \"loss\": 0.12559187412261963, \"time-step\": 4410}, {\"accuracy\": 0.75, \"loss\": 0.12561073899269104, \"time-step\": 4411}, {\"accuracy\": 0.75, \"loss\": 0.12559184432029724, \"time-step\": 4412}, {\"accuracy\": 0.75, \"loss\": 0.12561072409152985, \"time-step\": 4413}, {\"accuracy\": 0.75, \"loss\": 0.12559174001216888, \"time-step\": 4414}, {\"accuracy\": 0.75, \"loss\": 0.1256106048822403, \"time-step\": 4415}, {\"accuracy\": 0.75, \"loss\": 0.12559175491333008, \"time-step\": 4416}, {\"accuracy\": 0.75, \"loss\": 0.12561054527759552, \"time-step\": 4417}, {\"accuracy\": 0.75, \"loss\": 0.12559162080287933, \"time-step\": 4418}, {\"accuracy\": 0.75, \"loss\": 0.12561053037643433, \"time-step\": 4419}, {\"accuracy\": 0.75, \"loss\": 0.12559154629707336, \"time-step\": 4420}, {\"accuracy\": 0.75, \"loss\": 0.12561039626598358, \"time-step\": 4421}, {\"accuracy\": 0.75, \"loss\": 0.1255914717912674, \"time-step\": 4422}, {\"accuracy\": 0.75, \"loss\": 0.1256103664636612, \"time-step\": 4423}, {\"accuracy\": 0.75, \"loss\": 0.12559156119823456, \"time-step\": 4424}, {\"accuracy\": 0.75, \"loss\": 0.12561039626598358, \"time-step\": 4425}, {\"accuracy\": 0.75, \"loss\": 0.12559141218662262, \"time-step\": 4426}, {\"accuracy\": 0.75, \"loss\": 0.1256103217601776, \"time-step\": 4427}, {\"accuracy\": 0.75, \"loss\": 0.12559130787849426, \"time-step\": 4428}, {\"accuracy\": 0.75, \"loss\": 0.12561026215553284, \"time-step\": 4429}, {\"accuracy\": 0.75, \"loss\": 0.12559136748313904, \"time-step\": 4430}, {\"accuracy\": 0.75, \"loss\": 0.12561023235321045, \"time-step\": 4431}, {\"accuracy\": 0.75, \"loss\": 0.1255912482738495, \"time-step\": 4432}, {\"accuracy\": 0.75, \"loss\": 0.1256100982427597, \"time-step\": 4433}, {\"accuracy\": 0.75, \"loss\": 0.12559109926223755, \"time-step\": 4434}, {\"accuracy\": 0.75, \"loss\": 0.12561005353927612, \"time-step\": 4435}, {\"accuracy\": 0.75, \"loss\": 0.12559111416339874, \"time-step\": 4436}, {\"accuracy\": 0.75, \"loss\": 0.12561005353927612, \"time-step\": 4437}, {\"accuracy\": 0.75, \"loss\": 0.12559105455875397, \"time-step\": 4438}, {\"accuracy\": 0.75, \"loss\": 0.12560997903347015, \"time-step\": 4439}, {\"accuracy\": 0.75, \"loss\": 0.1255909651517868, \"time-step\": 4440}, {\"accuracy\": 0.75, \"loss\": 0.12560991942882538, \"time-step\": 4441}, {\"accuracy\": 0.75, \"loss\": 0.1255909502506256, \"time-step\": 4442}, {\"accuracy\": 0.75, \"loss\": 0.12560990452766418, \"time-step\": 4443}, {\"accuracy\": 0.75, \"loss\": 0.12559093534946442, \"time-step\": 4444}, {\"accuracy\": 0.75, \"loss\": 0.12560981512069702, \"time-step\": 4445}, {\"accuracy\": 0.75, \"loss\": 0.12559092044830322, \"time-step\": 4446}, {\"accuracy\": 0.75, \"loss\": 0.1256098747253418, \"time-step\": 4447}, {\"accuracy\": 0.75, \"loss\": 0.12559078633785248, \"time-step\": 4448}, {\"accuracy\": 0.75, \"loss\": 0.12560978531837463, \"time-step\": 4449}, {\"accuracy\": 0.75, \"loss\": 0.12559077143669128, \"time-step\": 4450}, {\"accuracy\": 0.75, \"loss\": 0.1256096363067627, \"time-step\": 4451}, {\"accuracy\": 0.75, \"loss\": 0.1255907118320465, \"time-step\": 4452}, {\"accuracy\": 0.75, \"loss\": 0.1256096363067627, \"time-step\": 4453}, {\"accuracy\": 0.75, \"loss\": 0.12559056282043457, \"time-step\": 4454}, {\"accuracy\": 0.75, \"loss\": 0.1256096363067627, \"time-step\": 4455}, {\"accuracy\": 0.75, \"loss\": 0.1255904734134674, \"time-step\": 4456}, {\"accuracy\": 0.75, \"loss\": 0.12560953199863434, \"time-step\": 4457}, {\"accuracy\": 0.75, \"loss\": 0.12559053301811218, \"time-step\": 4458}, {\"accuracy\": 0.75, \"loss\": 0.12560944259166718, \"time-step\": 4459}, {\"accuracy\": 0.75, \"loss\": 0.12559041380882263, \"time-step\": 4460}, {\"accuracy\": 0.75, \"loss\": 0.1256094127893448, \"time-step\": 4461}, {\"accuracy\": 0.75, \"loss\": 0.12559038400650024, \"time-step\": 4462}, {\"accuracy\": 0.75, \"loss\": 0.1256093680858612, \"time-step\": 4463}, {\"accuracy\": 0.75, \"loss\": 0.12559029459953308, \"time-step\": 4464}, {\"accuracy\": 0.75, \"loss\": 0.12560929358005524, \"time-step\": 4465}, {\"accuracy\": 0.75, \"loss\": 0.12559029459953308, \"time-step\": 4466}, {\"accuracy\": 0.75, \"loss\": 0.12560923397541046, \"time-step\": 4467}, {\"accuracy\": 0.75, \"loss\": 0.12559013068675995, \"time-step\": 4468}, {\"accuracy\": 0.75, \"loss\": 0.12560924887657166, \"time-step\": 4469}, {\"accuracy\": 0.75, \"loss\": 0.12559008598327637, \"time-step\": 4470}, {\"accuracy\": 0.75, \"loss\": 0.1256091445684433, \"time-step\": 4471}, {\"accuracy\": 0.75, \"loss\": 0.12559005618095398, \"time-step\": 4472}, {\"accuracy\": 0.75, \"loss\": 0.12560904026031494, \"time-step\": 4473}, {\"accuracy\": 0.75, \"loss\": 0.1255900263786316, \"time-step\": 4474}, {\"accuracy\": 0.75, \"loss\": 0.12560901045799255, \"time-step\": 4475}, {\"accuracy\": 0.75, \"loss\": 0.12558996677398682, \"time-step\": 4476}, {\"accuracy\": 0.75, \"loss\": 0.12560895085334778, \"time-step\": 4477}, {\"accuracy\": 0.75, \"loss\": 0.12558987736701965, \"time-step\": 4478}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 4479}, {\"accuracy\": 0.75, \"loss\": 0.12558984756469727, \"time-step\": 4480}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 4481}, {\"accuracy\": 0.75, \"loss\": 0.12558968365192413, \"time-step\": 4482}, {\"accuracy\": 0.75, \"loss\": 0.12560886144638062, \"time-step\": 4483}, {\"accuracy\": 0.75, \"loss\": 0.12558971345424652, \"time-step\": 4484}, {\"accuracy\": 0.75, \"loss\": 0.12560878694057465, \"time-step\": 4485}, {\"accuracy\": 0.75, \"loss\": 0.12558963894844055, \"time-step\": 4486}, {\"accuracy\": 0.75, \"loss\": 0.1256086379289627, \"time-step\": 4487}, {\"accuracy\": 0.75, \"loss\": 0.12558965384960175, \"time-step\": 4488}, {\"accuracy\": 0.75, \"loss\": 0.1256086677312851, \"time-step\": 4489}, {\"accuracy\": 0.75, \"loss\": 0.1255895495414734, \"time-step\": 4490}, {\"accuracy\": 0.75, \"loss\": 0.12560857832431793, \"time-step\": 4491}, {\"accuracy\": 0.75, \"loss\": 0.12558946013450623, \"time-step\": 4492}, {\"accuracy\": 0.75, \"loss\": 0.12560850381851196, \"time-step\": 4493}, {\"accuracy\": 0.75, \"loss\": 0.1255895048379898, \"time-step\": 4494}, {\"accuracy\": 0.75, \"loss\": 0.125608429312706, \"time-step\": 4495}, {\"accuracy\": 0.75, \"loss\": 0.12558935582637787, \"time-step\": 4496}, {\"accuracy\": 0.75, \"loss\": 0.125608429312706, \"time-step\": 4497}, {\"accuracy\": 0.75, \"loss\": 0.12558935582637787, \"time-step\": 4498}, {\"accuracy\": 0.75, \"loss\": 0.1256084144115448, \"time-step\": 4499}, {\"accuracy\": 0.75, \"loss\": 0.1255892515182495, \"time-step\": 4500}, {\"accuracy\": 0.75, \"loss\": 0.12560832500457764, \"time-step\": 4501}, {\"accuracy\": 0.75, \"loss\": 0.12558922171592712, \"time-step\": 4502}, {\"accuracy\": 0.75, \"loss\": 0.12560831010341644, \"time-step\": 4503}, {\"accuracy\": 0.75, \"loss\": 0.12558910250663757, \"time-step\": 4504}, {\"accuracy\": 0.75, \"loss\": 0.12560820579528809, \"time-step\": 4505}, {\"accuracy\": 0.75, \"loss\": 0.12558898329734802, \"time-step\": 4506}, {\"accuracy\": 0.75, \"loss\": 0.12560813128948212, \"time-step\": 4507}, {\"accuracy\": 0.75, \"loss\": 0.1255890280008316, \"time-step\": 4508}, {\"accuracy\": 0.75, \"loss\": 0.12560808658599854, \"time-step\": 4509}, {\"accuracy\": 0.75, \"loss\": 0.12558890879154205, \"time-step\": 4510}, {\"accuracy\": 0.75, \"loss\": 0.12560805678367615, \"time-step\": 4511}, {\"accuracy\": 0.75, \"loss\": 0.1255890280008316, \"time-step\": 4512}, {\"accuracy\": 0.75, \"loss\": 0.12560798227787018, \"time-step\": 4513}, {\"accuracy\": 0.75, \"loss\": 0.1255888193845749, \"time-step\": 4514}, {\"accuracy\": 0.75, \"loss\": 0.12560787796974182, \"time-step\": 4515}, {\"accuracy\": 0.75, \"loss\": 0.1255888193845749, \"time-step\": 4516}, {\"accuracy\": 0.75, \"loss\": 0.1256079226732254, \"time-step\": 4517}, {\"accuracy\": 0.75, \"loss\": 0.12558867037296295, \"time-step\": 4518}, {\"accuracy\": 0.75, \"loss\": 0.12560784816741943, \"time-step\": 4519}, {\"accuracy\": 0.75, \"loss\": 0.12558870017528534, \"time-step\": 4520}, {\"accuracy\": 0.75, \"loss\": 0.12560778856277466, \"time-step\": 4521}, {\"accuracy\": 0.75, \"loss\": 0.1255885660648346, \"time-step\": 4522}, {\"accuracy\": 0.75, \"loss\": 0.1256076842546463, \"time-step\": 4523}, {\"accuracy\": 0.75, \"loss\": 0.12558847665786743, \"time-step\": 4524}, {\"accuracy\": 0.75, \"loss\": 0.12560757994651794, \"time-step\": 4525}, {\"accuracy\": 0.75, \"loss\": 0.12558835744857788, \"time-step\": 4526}, {\"accuracy\": 0.75, \"loss\": 0.12560753524303436, \"time-step\": 4527}, {\"accuracy\": 0.75, \"loss\": 0.12558838725090027, \"time-step\": 4528}, {\"accuracy\": 0.75, \"loss\": 0.12560749053955078, \"time-step\": 4529}, {\"accuracy\": 0.75, \"loss\": 0.1255883425474167, \"time-step\": 4530}, {\"accuracy\": 0.75, \"loss\": 0.12560737133026123, \"time-step\": 4531}, {\"accuracy\": 0.75, \"loss\": 0.1255883127450943, \"time-step\": 4532}, {\"accuracy\": 0.75, \"loss\": 0.12560737133026123, \"time-step\": 4533}, {\"accuracy\": 0.75, \"loss\": 0.12558825314044952, \"time-step\": 4534}, {\"accuracy\": 0.75, \"loss\": 0.12560735642910004, \"time-step\": 4535}, {\"accuracy\": 0.75, \"loss\": 0.12558811902999878, \"time-step\": 4536}, {\"accuracy\": 0.75, \"loss\": 0.1256071776151657, \"time-step\": 4537}, {\"accuracy\": 0.75, \"loss\": 0.12558816373348236, \"time-step\": 4538}, {\"accuracy\": 0.75, \"loss\": 0.12560725212097168, \"time-step\": 4539}, {\"accuracy\": 0.75, \"loss\": 0.12558798491954803, \"time-step\": 4540}, {\"accuracy\": 0.75, \"loss\": 0.12560716271400452, \"time-step\": 4541}, {\"accuracy\": 0.75, \"loss\": 0.12558788061141968, \"time-step\": 4542}, {\"accuracy\": 0.75, \"loss\": 0.12560704350471497, \"time-step\": 4543}, {\"accuracy\": 0.75, \"loss\": 0.12558798491954803, \"time-step\": 4544}, {\"accuracy\": 0.75, \"loss\": 0.12560707330703735, \"time-step\": 4545}, {\"accuracy\": 0.75, \"loss\": 0.12558779120445251, \"time-step\": 4546}, {\"accuracy\": 0.75, \"loss\": 0.12560689449310303, \"time-step\": 4547}, {\"accuracy\": 0.75, \"loss\": 0.12558779120445251, \"time-step\": 4548}, {\"accuracy\": 0.75, \"loss\": 0.12560690939426422, \"time-step\": 4549}, {\"accuracy\": 0.75, \"loss\": 0.12558767199516296, \"time-step\": 4550}, {\"accuracy\": 0.75, \"loss\": 0.12560683488845825, \"time-step\": 4551}, {\"accuracy\": 0.75, \"loss\": 0.12558767199516296, \"time-step\": 4552}, {\"accuracy\": 0.75, \"loss\": 0.12560681998729706, \"time-step\": 4553}, {\"accuracy\": 0.75, \"loss\": 0.1255875676870346, \"time-step\": 4554}, {\"accuracy\": 0.75, \"loss\": 0.1256067454814911, \"time-step\": 4555}, {\"accuracy\": 0.75, \"loss\": 0.12558746337890625, \"time-step\": 4556}, {\"accuracy\": 0.75, \"loss\": 0.12560662627220154, \"time-step\": 4557}, {\"accuracy\": 0.75, \"loss\": 0.12558746337890625, \"time-step\": 4558}, {\"accuracy\": 0.75, \"loss\": 0.12560655176639557, \"time-step\": 4559}, {\"accuracy\": 0.75, \"loss\": 0.12558747828006744, \"time-step\": 4560}, {\"accuracy\": 0.75, \"loss\": 0.12560652196407318, \"time-step\": 4561}, {\"accuracy\": 0.75, \"loss\": 0.12558729946613312, \"time-step\": 4562}, {\"accuracy\": 0.75, \"loss\": 0.1256064772605896, \"time-step\": 4563}, {\"accuracy\": 0.75, \"loss\": 0.12558723986148834, \"time-step\": 4564}, {\"accuracy\": 0.75, \"loss\": 0.125606507062912, \"time-step\": 4565}, {\"accuracy\": 0.75, \"loss\": 0.12558718025684357, \"time-step\": 4566}, {\"accuracy\": 0.75, \"loss\": 0.12560632824897766, \"time-step\": 4567}, {\"accuracy\": 0.75, \"loss\": 0.1255871206521988, \"time-step\": 4568}, {\"accuracy\": 0.75, \"loss\": 0.12560628354549408, \"time-step\": 4569}, {\"accuracy\": 0.75, \"loss\": 0.12558695673942566, \"time-step\": 4570}, {\"accuracy\": 0.75, \"loss\": 0.12560614943504333, \"time-step\": 4571}, {\"accuracy\": 0.75, \"loss\": 0.12558695673942566, \"time-step\": 4572}, {\"accuracy\": 0.75, \"loss\": 0.12560616433620453, \"time-step\": 4573}, {\"accuracy\": 0.75, \"loss\": 0.12558691203594208, \"time-step\": 4574}, {\"accuracy\": 0.75, \"loss\": 0.12560610473155975, \"time-step\": 4575}, {\"accuracy\": 0.75, \"loss\": 0.1255868673324585, \"time-step\": 4576}, {\"accuracy\": 0.75, \"loss\": 0.1256060153245926, \"time-step\": 4577}, {\"accuracy\": 0.75, \"loss\": 0.12558682262897491, \"time-step\": 4578}, {\"accuracy\": 0.75, \"loss\": 0.12560586631298065, \"time-step\": 4579}, {\"accuracy\": 0.75, \"loss\": 0.12558673322200775, \"time-step\": 4580}, {\"accuracy\": 0.75, \"loss\": 0.12560591101646423, \"time-step\": 4581}, {\"accuracy\": 0.75, \"loss\": 0.12558665871620178, \"time-step\": 4582}, {\"accuracy\": 0.75, \"loss\": 0.12560580670833588, \"time-step\": 4583}, {\"accuracy\": 0.75, \"loss\": 0.12558655440807343, \"time-step\": 4584}, {\"accuracy\": 0.75, \"loss\": 0.12560583651065826, \"time-step\": 4585}, {\"accuracy\": 0.75, \"loss\": 0.12558650970458984, \"time-step\": 4586}, {\"accuracy\": 0.75, \"loss\": 0.1256057322025299, \"time-step\": 4587}, {\"accuracy\": 0.75, \"loss\": 0.12558642029762268, \"time-step\": 4588}, {\"accuracy\": 0.75, \"loss\": 0.12560562789440155, \"time-step\": 4589}, {\"accuracy\": 0.75, \"loss\": 0.12558642029762268, \"time-step\": 4590}, {\"accuracy\": 0.75, \"loss\": 0.12560558319091797, \"time-step\": 4591}, {\"accuracy\": 0.75, \"loss\": 0.12558625638484955, \"time-step\": 4592}, {\"accuracy\": 0.75, \"loss\": 0.1256054937839508, \"time-step\": 4593}, {\"accuracy\": 0.75, \"loss\": 0.12558630108833313, \"time-step\": 4594}, {\"accuracy\": 0.75, \"loss\": 0.12560543417930603, \"time-step\": 4595}, {\"accuracy\": 0.75, \"loss\": 0.12558613717556, \"time-step\": 4596}, {\"accuracy\": 0.75, \"loss\": 0.12560535967350006, \"time-step\": 4597}, {\"accuracy\": 0.75, \"loss\": 0.12558613717556, \"time-step\": 4598}, {\"accuracy\": 0.75, \"loss\": 0.12560534477233887, \"time-step\": 4599}, {\"accuracy\": 0.75, \"loss\": 0.12558604776859283, \"time-step\": 4600}, {\"accuracy\": 0.75, \"loss\": 0.1256052553653717, \"time-step\": 4601}, {\"accuracy\": 0.75, \"loss\": 0.12558597326278687, \"time-step\": 4602}, {\"accuracy\": 0.75, \"loss\": 0.12560513615608215, \"time-step\": 4603}, {\"accuracy\": 0.75, \"loss\": 0.1255858987569809, \"time-step\": 4604}, {\"accuracy\": 0.75, \"loss\": 0.12560516595840454, \"time-step\": 4605}, {\"accuracy\": 0.75, \"loss\": 0.1255858689546585, \"time-step\": 4606}, {\"accuracy\": 0.75, \"loss\": 0.125605046749115, \"time-step\": 4607}, {\"accuracy\": 0.75, \"loss\": 0.12558580935001373, \"time-step\": 4608}, {\"accuracy\": 0.75, \"loss\": 0.1256050020456314, \"time-step\": 4609}, {\"accuracy\": 0.75, \"loss\": 0.12558579444885254, \"time-step\": 4610}, {\"accuracy\": 0.75, \"loss\": 0.12560492753982544, \"time-step\": 4611}, {\"accuracy\": 0.75, \"loss\": 0.1255856603384018, \"time-step\": 4612}, {\"accuracy\": 0.75, \"loss\": 0.12560491263866425, \"time-step\": 4613}, {\"accuracy\": 0.75, \"loss\": 0.12558554112911224, \"time-step\": 4614}, {\"accuracy\": 0.75, \"loss\": 0.1256047934293747, \"time-step\": 4615}, {\"accuracy\": 0.75, \"loss\": 0.12558549642562866, \"time-step\": 4616}, {\"accuracy\": 0.75, \"loss\": 0.1256047785282135, \"time-step\": 4617}, {\"accuracy\": 0.75, \"loss\": 0.12558549642562866, \"time-step\": 4618}, {\"accuracy\": 0.75, \"loss\": 0.12560468912124634, \"time-step\": 4619}, {\"accuracy\": 0.75, \"loss\": 0.12558534741401672, \"time-step\": 4620}, {\"accuracy\": 0.75, \"loss\": 0.12560467422008514, \"time-step\": 4621}, {\"accuracy\": 0.75, \"loss\": 0.12558525800704956, \"time-step\": 4622}, {\"accuracy\": 0.75, \"loss\": 0.12560464441776276, \"time-step\": 4623}, {\"accuracy\": 0.75, \"loss\": 0.1255851686000824, \"time-step\": 4624}, {\"accuracy\": 0.75, \"loss\": 0.12560445070266724, \"time-step\": 4625}, {\"accuracy\": 0.75, \"loss\": 0.1255851536989212, \"time-step\": 4626}, {\"accuracy\": 0.75, \"loss\": 0.12560436129570007, \"time-step\": 4627}, {\"accuracy\": 0.75, \"loss\": 0.12558504939079285, \"time-step\": 4628}, {\"accuracy\": 0.75, \"loss\": 0.1256043165922165, \"time-step\": 4629}, {\"accuracy\": 0.75, \"loss\": 0.12558498978614807, \"time-step\": 4630}, {\"accuracy\": 0.75, \"loss\": 0.1256042718887329, \"time-step\": 4631}, {\"accuracy\": 0.75, \"loss\": 0.12558506429195404, \"time-step\": 4632}, {\"accuracy\": 0.75, \"loss\": 0.1256042718887329, \"time-step\": 4633}, {\"accuracy\": 0.75, \"loss\": 0.12558487057685852, \"time-step\": 4634}, {\"accuracy\": 0.75, \"loss\": 0.12560421228408813, \"time-step\": 4635}, {\"accuracy\": 0.75, \"loss\": 0.12558484077453613, \"time-step\": 4636}, {\"accuracy\": 0.75, \"loss\": 0.12560410797595978, \"time-step\": 4637}, {\"accuracy\": 0.75, \"loss\": 0.12558479607105255, \"time-step\": 4638}, {\"accuracy\": 0.75, \"loss\": 0.1256040334701538, \"time-step\": 4639}, {\"accuracy\": 0.75, \"loss\": 0.1255847066640854, \"time-step\": 4640}, {\"accuracy\": 0.75, \"loss\": 0.12560401856899261, \"time-step\": 4641}, {\"accuracy\": 0.75, \"loss\": 0.12558460235595703, \"time-step\": 4642}, {\"accuracy\": 0.75, \"loss\": 0.12560386955738068, \"time-step\": 4643}, {\"accuracy\": 0.75, \"loss\": 0.12558448314666748, \"time-step\": 4644}, {\"accuracy\": 0.75, \"loss\": 0.1256038248538971, \"time-step\": 4645}, {\"accuracy\": 0.75, \"loss\": 0.1255844682455063, \"time-step\": 4646}, {\"accuracy\": 0.75, \"loss\": 0.1256037801504135, \"time-step\": 4647}, {\"accuracy\": 0.75, \"loss\": 0.12558439373970032, \"time-step\": 4648}, {\"accuracy\": 0.75, \"loss\": 0.12560373544692993, \"time-step\": 4649}, {\"accuracy\": 0.75, \"loss\": 0.12558428943157196, \"time-step\": 4650}, {\"accuracy\": 0.75, \"loss\": 0.12560363113880157, \"time-step\": 4651}, {\"accuracy\": 0.75, \"loss\": 0.12558425962924957, \"time-step\": 4652}, {\"accuracy\": 0.75, \"loss\": 0.1256036013364792, \"time-step\": 4653}, {\"accuracy\": 0.75, \"loss\": 0.125584214925766, \"time-step\": 4654}, {\"accuracy\": 0.75, \"loss\": 0.12560349702835083, \"time-step\": 4655}, {\"accuracy\": 0.75, \"loss\": 0.12558411061763763, \"time-step\": 4656}, {\"accuracy\": 0.75, \"loss\": 0.12560342252254486, \"time-step\": 4657}, {\"accuracy\": 0.75, \"loss\": 0.12558409571647644, \"time-step\": 4658}, {\"accuracy\": 0.75, \"loss\": 0.1256033182144165, \"time-step\": 4659}, {\"accuracy\": 0.75, \"loss\": 0.12558400630950928, \"time-step\": 4660}, {\"accuracy\": 0.75, \"loss\": 0.12560328841209412, \"time-step\": 4661}, {\"accuracy\": 0.75, \"loss\": 0.12558382749557495, \"time-step\": 4662}, {\"accuracy\": 0.75, \"loss\": 0.12560328841209412, \"time-step\": 4663}, {\"accuracy\": 0.75, \"loss\": 0.12558381259441376, \"time-step\": 4664}, {\"accuracy\": 0.75, \"loss\": 0.12560313940048218, \"time-step\": 4665}, {\"accuracy\": 0.75, \"loss\": 0.12558376789093018, \"time-step\": 4666}, {\"accuracy\": 0.75, \"loss\": 0.12560312449932098, \"time-step\": 4667}, {\"accuracy\": 0.75, \"loss\": 0.12558366358280182, \"time-step\": 4668}, {\"accuracy\": 0.75, \"loss\": 0.12560302019119263, \"time-step\": 4669}, {\"accuracy\": 0.75, \"loss\": 0.1255837082862854, \"time-step\": 4670}, {\"accuracy\": 0.75, \"loss\": 0.12560290098190308, \"time-step\": 4671}, {\"accuracy\": 0.75, \"loss\": 0.1255834698677063, \"time-step\": 4672}, {\"accuracy\": 0.75, \"loss\": 0.12560290098190308, \"time-step\": 4673}, {\"accuracy\": 0.75, \"loss\": 0.1255834400653839, \"time-step\": 4674}, {\"accuracy\": 0.75, \"loss\": 0.12560279667377472, \"time-step\": 4675}, {\"accuracy\": 0.75, \"loss\": 0.12558339536190033, \"time-step\": 4676}, {\"accuracy\": 0.75, \"loss\": 0.12560272216796875, \"time-step\": 4677}, {\"accuracy\": 0.75, \"loss\": 0.12558330595493317, \"time-step\": 4678}, {\"accuracy\": 0.75, \"loss\": 0.12560270726680756, \"time-step\": 4679}, {\"accuracy\": 0.75, \"loss\": 0.12558327615261078, \"time-step\": 4680}, {\"accuracy\": 0.75, \"loss\": 0.12560252845287323, \"time-step\": 4681}, {\"accuracy\": 0.75, \"loss\": 0.1255832016468048, \"time-step\": 4682}, {\"accuracy\": 0.75, \"loss\": 0.12560249865055084, \"time-step\": 4683}, {\"accuracy\": 0.75, \"loss\": 0.12558318674564362, \"time-step\": 4684}, {\"accuracy\": 0.75, \"loss\": 0.12560240924358368, \"time-step\": 4685}, {\"accuracy\": 0.75, \"loss\": 0.12558309733867645, \"time-step\": 4686}, {\"accuracy\": 0.75, \"loss\": 0.12560239434242249, \"time-step\": 4687}, {\"accuracy\": 0.75, \"loss\": 0.12558305263519287, \"time-step\": 4688}, {\"accuracy\": 0.75, \"loss\": 0.12560229003429413, \"time-step\": 4689}, {\"accuracy\": 0.75, \"loss\": 0.12558288872241974, \"time-step\": 4690}, {\"accuracy\": 0.75, \"loss\": 0.1256023645401001, \"time-step\": 4691}, {\"accuracy\": 0.75, \"loss\": 0.12558287382125854, \"time-step\": 4692}, {\"accuracy\": 0.75, \"loss\": 0.12560220062732697, \"time-step\": 4693}, {\"accuracy\": 0.75, \"loss\": 0.125582754611969, \"time-step\": 4694}, {\"accuracy\": 0.75, \"loss\": 0.12560215592384338, \"time-step\": 4695}, {\"accuracy\": 0.75, \"loss\": 0.12558266520500183, \"time-step\": 4696}, {\"accuracy\": 0.75, \"loss\": 0.1256020963191986, \"time-step\": 4697}, {\"accuracy\": 0.75, \"loss\": 0.12558256089687347, \"time-step\": 4698}, {\"accuracy\": 0.75, \"loss\": 0.12560193240642548, \"time-step\": 4699}, {\"accuracy\": 0.75, \"loss\": 0.12558260560035706, \"time-step\": 4700}, {\"accuracy\": 0.75, \"loss\": 0.1256019026041031, \"time-step\": 4701}, {\"accuracy\": 0.75, \"loss\": 0.1255824863910675, \"time-step\": 4702}, {\"accuracy\": 0.75, \"loss\": 0.12560179829597473, \"time-step\": 4703}, {\"accuracy\": 0.75, \"loss\": 0.12558238208293915, \"time-step\": 4704}, {\"accuracy\": 0.75, \"loss\": 0.12560181319713593, \"time-step\": 4705}, {\"accuracy\": 0.75, \"loss\": 0.12558230757713318, \"time-step\": 4706}, {\"accuracy\": 0.75, \"loss\": 0.125601664185524, \"time-step\": 4707}, {\"accuracy\": 0.75, \"loss\": 0.12558221817016602, \"time-step\": 4708}, {\"accuracy\": 0.75, \"loss\": 0.1256016194820404, \"time-step\": 4709}, {\"accuracy\": 0.75, \"loss\": 0.12558217346668243, \"time-step\": 4710}, {\"accuracy\": 0.75, \"loss\": 0.12560158967971802, \"time-step\": 4711}, {\"accuracy\": 0.75, \"loss\": 0.12558208405971527, \"time-step\": 4712}, {\"accuracy\": 0.75, \"loss\": 0.12560148537158966, \"time-step\": 4713}, {\"accuracy\": 0.75, \"loss\": 0.1255819946527481, \"time-step\": 4714}, {\"accuracy\": 0.75, \"loss\": 0.1256013810634613, \"time-step\": 4715}, {\"accuracy\": 0.75, \"loss\": 0.12558192014694214, \"time-step\": 4716}, {\"accuracy\": 0.75, \"loss\": 0.1256013810634613, \"time-step\": 4717}, {\"accuracy\": 0.75, \"loss\": 0.12558192014694214, \"time-step\": 4718}, {\"accuracy\": 0.75, \"loss\": 0.1256013661623001, \"time-step\": 4719}, {\"accuracy\": 0.75, \"loss\": 0.12558183073997498, \"time-step\": 4720}, {\"accuracy\": 0.75, \"loss\": 0.1256011724472046, \"time-step\": 4721}, {\"accuracy\": 0.75, \"loss\": 0.1255817711353302, \"time-step\": 4722}, {\"accuracy\": 0.75, \"loss\": 0.12560109794139862, \"time-step\": 4723}, {\"accuracy\": 0.75, \"loss\": 0.12558159232139587, \"time-step\": 4724}, {\"accuracy\": 0.75, \"loss\": 0.12560103833675385, \"time-step\": 4725}, {\"accuracy\": 0.75, \"loss\": 0.12558159232139587, \"time-step\": 4726}, {\"accuracy\": 0.75, \"loss\": 0.12560096383094788, \"time-step\": 4727}, {\"accuracy\": 0.75, \"loss\": 0.12558141350746155, \"time-step\": 4728}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 4729}, {\"accuracy\": 0.75, \"loss\": 0.12558136880397797, \"time-step\": 4730}, {\"accuracy\": 0.75, \"loss\": 0.1256009042263031, \"time-step\": 4731}, {\"accuracy\": 0.75, \"loss\": 0.12558136880397797, \"time-step\": 4732}, {\"accuracy\": 0.75, \"loss\": 0.1256006807088852, \"time-step\": 4733}, {\"accuracy\": 0.75, \"loss\": 0.12558118999004364, \"time-step\": 4734}, {\"accuracy\": 0.75, \"loss\": 0.12560062110424042, \"time-step\": 4735}, {\"accuracy\": 0.75, \"loss\": 0.12558118999004364, \"time-step\": 4736}, {\"accuracy\": 0.75, \"loss\": 0.12560059130191803, \"time-step\": 4737}, {\"accuracy\": 0.75, \"loss\": 0.12558117508888245, \"time-step\": 4738}, {\"accuracy\": 0.75, \"loss\": 0.12560048699378967, \"time-step\": 4739}, {\"accuracy\": 0.75, \"loss\": 0.1255810707807541, \"time-step\": 4740}, {\"accuracy\": 0.75, \"loss\": 0.12560048699378967, \"time-step\": 4741}, {\"accuracy\": 0.75, \"loss\": 0.1255810409784317, \"time-step\": 4742}, {\"accuracy\": 0.75, \"loss\": 0.1256004124879837, \"time-step\": 4743}, {\"accuracy\": 0.75, \"loss\": 0.12558089196681976, \"time-step\": 4744}, {\"accuracy\": 0.75, \"loss\": 0.12560027837753296, \"time-step\": 4745}, {\"accuracy\": 0.75, \"loss\": 0.1255808025598526, \"time-step\": 4746}, {\"accuracy\": 0.75, \"loss\": 0.12560024857521057, \"time-step\": 4747}, {\"accuracy\": 0.75, \"loss\": 0.1255807876586914, \"time-step\": 4748}, {\"accuracy\": 0.75, \"loss\": 0.1256001740694046, \"time-step\": 4749}, {\"accuracy\": 0.75, \"loss\": 0.12558068335056305, \"time-step\": 4750}, {\"accuracy\": 0.75, \"loss\": 0.12560009956359863, \"time-step\": 4751}, {\"accuracy\": 0.75, \"loss\": 0.12558065354824066, \"time-step\": 4752}, {\"accuracy\": 0.75, \"loss\": 0.12559999525547028, \"time-step\": 4753}, {\"accuracy\": 0.75, \"loss\": 0.12558048963546753, \"time-step\": 4754}, {\"accuracy\": 0.75, \"loss\": 0.1255999207496643, \"time-step\": 4755}, {\"accuracy\": 0.75, \"loss\": 0.12558047473430634, \"time-step\": 4756}, {\"accuracy\": 0.75, \"loss\": 0.1255999058485031, \"time-step\": 4757}, {\"accuracy\": 0.75, \"loss\": 0.12558043003082275, \"time-step\": 4758}, {\"accuracy\": 0.75, \"loss\": 0.12559978663921356, \"time-step\": 4759}, {\"accuracy\": 0.75, \"loss\": 0.12558035552501678, \"time-step\": 4760}, {\"accuracy\": 0.75, \"loss\": 0.12559974193572998, \"time-step\": 4761}, {\"accuracy\": 0.75, \"loss\": 0.12558025121688843, \"time-step\": 4762}, {\"accuracy\": 0.75, \"loss\": 0.1255996823310852, \"time-step\": 4763}, {\"accuracy\": 0.75, \"loss\": 0.12558016180992126, \"time-step\": 4764}, {\"accuracy\": 0.75, \"loss\": 0.12559956312179565, \"time-step\": 4765}, {\"accuracy\": 0.75, \"loss\": 0.12558017671108246, \"time-step\": 4766}, {\"accuracy\": 0.75, \"loss\": 0.12559954822063446, \"time-step\": 4767}, {\"accuracy\": 0.75, \"loss\": 0.12557999789714813, \"time-step\": 4768}, {\"accuracy\": 0.75, \"loss\": 0.12559935450553894, \"time-step\": 4769}, {\"accuracy\": 0.75, \"loss\": 0.1255798637866974, \"time-step\": 4770}, {\"accuracy\": 0.75, \"loss\": 0.12559938430786133, \"time-step\": 4771}, {\"accuracy\": 0.75, \"loss\": 0.12557989358901978, \"time-step\": 4772}, {\"accuracy\": 0.75, \"loss\": 0.1255992203950882, \"time-step\": 4773}, {\"accuracy\": 0.75, \"loss\": 0.12557977437973022, \"time-step\": 4774}, {\"accuracy\": 0.75, \"loss\": 0.125599205493927, \"time-step\": 4775}, {\"accuracy\": 0.75, \"loss\": 0.12557971477508545, \"time-step\": 4776}, {\"accuracy\": 0.75, \"loss\": 0.12559910118579865, \"time-step\": 4777}, {\"accuracy\": 0.75, \"loss\": 0.12557965517044067, \"time-step\": 4778}, {\"accuracy\": 0.75, \"loss\": 0.12559905648231506, \"time-step\": 4779}, {\"accuracy\": 0.75, \"loss\": 0.12557955086231232, \"time-step\": 4780}, {\"accuracy\": 0.75, \"loss\": 0.12559902667999268, \"time-step\": 4781}, {\"accuracy\": 0.75, \"loss\": 0.12557955086231232, \"time-step\": 4782}, {\"accuracy\": 0.75, \"loss\": 0.1255989372730255, \"time-step\": 4783}, {\"accuracy\": 0.75, \"loss\": 0.12557941675186157, \"time-step\": 4784}, {\"accuracy\": 0.75, \"loss\": 0.12559880316257477, \"time-step\": 4785}, {\"accuracy\": 0.75, \"loss\": 0.12557925283908844, \"time-step\": 4786}, {\"accuracy\": 0.75, \"loss\": 0.12559877336025238, \"time-step\": 4787}, {\"accuracy\": 0.75, \"loss\": 0.12557928264141083, \"time-step\": 4788}, {\"accuracy\": 0.75, \"loss\": 0.12559868395328522, \"time-step\": 4789}, {\"accuracy\": 0.75, \"loss\": 0.12557923793792725, \"time-step\": 4790}, {\"accuracy\": 0.75, \"loss\": 0.12559862434864044, \"time-step\": 4791}, {\"accuracy\": 0.75, \"loss\": 0.12557904422283173, \"time-step\": 4792}, {\"accuracy\": 0.75, \"loss\": 0.12559852004051208, \"time-step\": 4793}, {\"accuracy\": 0.75, \"loss\": 0.12557899951934814, \"time-step\": 4794}, {\"accuracy\": 0.75, \"loss\": 0.1255984902381897, \"time-step\": 4795}, {\"accuracy\": 0.75, \"loss\": 0.12557892501354218, \"time-step\": 4796}, {\"accuracy\": 0.75, \"loss\": 0.12559843063354492, \"time-step\": 4797}, {\"accuracy\": 0.75, \"loss\": 0.1255788505077362, \"time-step\": 4798}, {\"accuracy\": 0.75, \"loss\": 0.12559834122657776, \"time-step\": 4799}, {\"accuracy\": 0.75, \"loss\": 0.12557880580425262, \"time-step\": 4800}, {\"accuracy\": 0.75, \"loss\": 0.1255982667207718, \"time-step\": 4801}, {\"accuracy\": 0.75, \"loss\": 0.1255786418914795, \"time-step\": 4802}, {\"accuracy\": 0.75, \"loss\": 0.1255982369184494, \"time-step\": 4803}, {\"accuracy\": 0.75, \"loss\": 0.1255786269903183, \"time-step\": 4804}, {\"accuracy\": 0.75, \"loss\": 0.12559808790683746, \"time-step\": 4805}, {\"accuracy\": 0.75, \"loss\": 0.12557853758335114, \"time-step\": 4806}, {\"accuracy\": 0.75, \"loss\": 0.1255979686975479, \"time-step\": 4807}, {\"accuracy\": 0.75, \"loss\": 0.12557847797870636, \"time-step\": 4808}, {\"accuracy\": 0.75, \"loss\": 0.12559784948825836, \"time-step\": 4809}, {\"accuracy\": 0.75, \"loss\": 0.125578373670578, \"time-step\": 4810}, {\"accuracy\": 0.75, \"loss\": 0.12559781968593597, \"time-step\": 4811}, {\"accuracy\": 0.75, \"loss\": 0.12557829916477203, \"time-step\": 4812}, {\"accuracy\": 0.75, \"loss\": 0.12559765577316284, \"time-step\": 4813}, {\"accuracy\": 0.75, \"loss\": 0.1255781501531601, \"time-step\": 4814}, {\"accuracy\": 0.75, \"loss\": 0.1255977302789688, \"time-step\": 4815}, {\"accuracy\": 0.75, \"loss\": 0.12557807564735413, \"time-step\": 4816}, {\"accuracy\": 0.75, \"loss\": 0.12559764087200165, \"time-step\": 4817}, {\"accuracy\": 0.75, \"loss\": 0.12557798624038696, \"time-step\": 4818}, {\"accuracy\": 0.75, \"loss\": 0.12559747695922852, \"time-step\": 4819}, {\"accuracy\": 0.75, \"loss\": 0.12557794153690338, \"time-step\": 4820}, {\"accuracy\": 0.75, \"loss\": 0.12559741735458374, \"time-step\": 4821}, {\"accuracy\": 0.75, \"loss\": 0.12557783722877502, \"time-step\": 4822}, {\"accuracy\": 0.75, \"loss\": 0.125597283244133, \"time-step\": 4823}, {\"accuracy\": 0.75, \"loss\": 0.12557776272296906, \"time-step\": 4824}, {\"accuracy\": 0.75, \"loss\": 0.125597283244133, \"time-step\": 4825}, {\"accuracy\": 0.75, \"loss\": 0.12557780742645264, \"time-step\": 4826}, {\"accuracy\": 0.75, \"loss\": 0.12559722363948822, \"time-step\": 4827}, {\"accuracy\": 0.75, \"loss\": 0.1255776435136795, \"time-step\": 4828}, {\"accuracy\": 0.75, \"loss\": 0.12559714913368225, \"time-step\": 4829}, {\"accuracy\": 0.75, \"loss\": 0.12557750940322876, \"time-step\": 4830}, {\"accuracy\": 0.75, \"loss\": 0.1255970448255539, \"time-step\": 4831}, {\"accuracy\": 0.75, \"loss\": 0.12557747960090637, \"time-step\": 4832}, {\"accuracy\": 0.75, \"loss\": 0.12559688091278076, \"time-step\": 4833}, {\"accuracy\": 0.75, \"loss\": 0.12557736039161682, \"time-step\": 4834}, {\"accuracy\": 0.75, \"loss\": 0.12559685111045837, \"time-step\": 4835}, {\"accuracy\": 0.75, \"loss\": 0.12557724118232727, \"time-step\": 4836}, {\"accuracy\": 0.75, \"loss\": 0.1255967617034912, \"time-step\": 4837}, {\"accuracy\": 0.75, \"loss\": 0.1255771815776825, \"time-step\": 4838}, {\"accuracy\": 0.75, \"loss\": 0.12559668719768524, \"time-step\": 4839}, {\"accuracy\": 0.75, \"loss\": 0.12557724118232727, \"time-step\": 4840}, {\"accuracy\": 0.75, \"loss\": 0.12559664249420166, \"time-step\": 4841}, {\"accuracy\": 0.75, \"loss\": 0.1255771368741989, \"time-step\": 4842}, {\"accuracy\": 0.75, \"loss\": 0.12559658288955688, \"time-step\": 4843}, {\"accuracy\": 0.75, \"loss\": 0.12557706236839294, \"time-step\": 4844}, {\"accuracy\": 0.75, \"loss\": 0.1255965530872345, \"time-step\": 4845}, {\"accuracy\": 0.75, \"loss\": 0.12557688355445862, \"time-step\": 4846}, {\"accuracy\": 0.75, \"loss\": 0.12559644877910614, \"time-step\": 4847}, {\"accuracy\": 0.75, \"loss\": 0.12557683885097504, \"time-step\": 4848}, {\"accuracy\": 0.75, \"loss\": 0.1255962997674942, \"time-step\": 4849}, {\"accuracy\": 0.75, \"loss\": 0.12557671964168549, \"time-step\": 4850}, {\"accuracy\": 0.75, \"loss\": 0.125596284866333, \"time-step\": 4851}, {\"accuracy\": 0.75, \"loss\": 0.12557664513587952, \"time-step\": 4852}, {\"accuracy\": 0.75, \"loss\": 0.12559619545936584, \"time-step\": 4853}, {\"accuracy\": 0.75, \"loss\": 0.12557655572891235, \"time-step\": 4854}, {\"accuracy\": 0.75, \"loss\": 0.1255960762500763, \"time-step\": 4855}, {\"accuracy\": 0.75, \"loss\": 0.12557655572891235, \"time-step\": 4856}, {\"accuracy\": 0.75, \"loss\": 0.12559601664543152, \"time-step\": 4857}, {\"accuracy\": 0.75, \"loss\": 0.1255764216184616, \"time-step\": 4858}, {\"accuracy\": 0.75, \"loss\": 0.12559588253498077, \"time-step\": 4859}, {\"accuracy\": 0.75, \"loss\": 0.12557633221149445, \"time-step\": 4860}, {\"accuracy\": 0.75, \"loss\": 0.12559588253498077, \"time-step\": 4861}, {\"accuracy\": 0.75, \"loss\": 0.12557628750801086, \"time-step\": 4862}, {\"accuracy\": 0.75, \"loss\": 0.1255958378314972, \"time-step\": 4863}, {\"accuracy\": 0.75, \"loss\": 0.12557610869407654, \"time-step\": 4864}, {\"accuracy\": 0.75, \"loss\": 0.12559565901756287, \"time-step\": 4865}, {\"accuracy\": 0.75, \"loss\": 0.12557609379291534, \"time-step\": 4866}, {\"accuracy\": 0.75, \"loss\": 0.12559568881988525, \"time-step\": 4867}, {\"accuracy\": 0.75, \"loss\": 0.12557604908943176, \"time-step\": 4868}, {\"accuracy\": 0.75, \"loss\": 0.12559551000595093, \"time-step\": 4869}, {\"accuracy\": 0.75, \"loss\": 0.1255759745836258, \"time-step\": 4870}, {\"accuracy\": 0.75, \"loss\": 0.12559540569782257, \"time-step\": 4871}, {\"accuracy\": 0.75, \"loss\": 0.12557585537433624, \"time-step\": 4872}, {\"accuracy\": 0.75, \"loss\": 0.12559542059898376, \"time-step\": 4873}, {\"accuracy\": 0.75, \"loss\": 0.12557581067085266, \"time-step\": 4874}, {\"accuracy\": 0.75, \"loss\": 0.1255953162908554, \"time-step\": 4875}, {\"accuracy\": 0.75, \"loss\": 0.1255757063627243, \"time-step\": 4876}, {\"accuracy\": 0.75, \"loss\": 0.12559528648853302, \"time-step\": 4877}, {\"accuracy\": 0.75, \"loss\": 0.12557566165924072, \"time-step\": 4878}, {\"accuracy\": 0.75, \"loss\": 0.12559515237808228, \"time-step\": 4879}, {\"accuracy\": 0.75, \"loss\": 0.12557557225227356, \"time-step\": 4880}, {\"accuracy\": 0.75, \"loss\": 0.12559500336647034, \"time-step\": 4881}, {\"accuracy\": 0.75, \"loss\": 0.1255754828453064, \"time-step\": 4882}, {\"accuracy\": 0.75, \"loss\": 0.12559503316879272, \"time-step\": 4883}, {\"accuracy\": 0.75, \"loss\": 0.12557542324066162, \"time-step\": 4884}, {\"accuracy\": 0.75, \"loss\": 0.1255948394536972, \"time-step\": 4885}, {\"accuracy\": 0.75, \"loss\": 0.1255752444267273, \"time-step\": 4886}, {\"accuracy\": 0.75, \"loss\": 0.1255948394536972, \"time-step\": 4887}, {\"accuracy\": 0.75, \"loss\": 0.12557516992092133, \"time-step\": 4888}, {\"accuracy\": 0.75, \"loss\": 0.12559470534324646, \"time-step\": 4889}, {\"accuracy\": 0.75, \"loss\": 0.12557518482208252, \"time-step\": 4890}, {\"accuracy\": 0.75, \"loss\": 0.12559464573860168, \"time-step\": 4891}, {\"accuracy\": 0.75, \"loss\": 0.12557505071163177, \"time-step\": 4892}, {\"accuracy\": 0.75, \"loss\": 0.12559452652931213, \"time-step\": 4893}, {\"accuracy\": 0.75, \"loss\": 0.12557494640350342, \"time-step\": 4894}, {\"accuracy\": 0.75, \"loss\": 0.12559448182582855, \"time-step\": 4895}, {\"accuracy\": 0.75, \"loss\": 0.12557485699653625, \"time-step\": 4896}, {\"accuracy\": 0.75, \"loss\": 0.1255943328142166, \"time-step\": 4897}, {\"accuracy\": 0.75, \"loss\": 0.12557484209537506, \"time-step\": 4898}, {\"accuracy\": 0.75, \"loss\": 0.12559428811073303, \"time-step\": 4899}, {\"accuracy\": 0.75, \"loss\": 0.12557481229305267, \"time-step\": 4900}, {\"accuracy\": 0.75, \"loss\": 0.1255941540002823, \"time-step\": 4901}, {\"accuracy\": 0.75, \"loss\": 0.12557464838027954, \"time-step\": 4902}, {\"accuracy\": 0.75, \"loss\": 0.12559418380260468, \"time-step\": 4903}, {\"accuracy\": 0.75, \"loss\": 0.12557469308376312, \"time-step\": 4904}, {\"accuracy\": 0.75, \"loss\": 0.12559407949447632, \"time-step\": 4905}, {\"accuracy\": 0.75, \"loss\": 0.12557452917099, \"time-step\": 4906}, {\"accuracy\": 0.75, \"loss\": 0.12559400498867035, \"time-step\": 4907}, {\"accuracy\": 0.75, \"loss\": 0.12557440996170044, \"time-step\": 4908}, {\"accuracy\": 0.75, \"loss\": 0.1255938857793808, \"time-step\": 4909}, {\"accuracy\": 0.75, \"loss\": 0.12557438015937805, \"time-step\": 4910}, {\"accuracy\": 0.75, \"loss\": 0.1255938708782196, \"time-step\": 4911}, {\"accuracy\": 0.75, \"loss\": 0.12557430565357208, \"time-step\": 4912}, {\"accuracy\": 0.75, \"loss\": 0.12559372186660767, \"time-step\": 4913}, {\"accuracy\": 0.75, \"loss\": 0.12557415664196014, \"time-step\": 4914}, {\"accuracy\": 0.75, \"loss\": 0.12559369206428528, \"time-step\": 4915}, {\"accuracy\": 0.75, \"loss\": 0.12557409703731537, \"time-step\": 4916}, {\"accuracy\": 0.75, \"loss\": 0.1255936175584793, \"time-step\": 4917}, {\"accuracy\": 0.75, \"loss\": 0.12557393312454224, \"time-step\": 4918}, {\"accuracy\": 0.75, \"loss\": 0.12559351325035095, \"time-step\": 4919}, {\"accuracy\": 0.75, \"loss\": 0.12557393312454224, \"time-step\": 4920}, {\"accuracy\": 0.75, \"loss\": 0.12559346854686737, \"time-step\": 4921}, {\"accuracy\": 0.75, \"loss\": 0.1255737841129303, \"time-step\": 4922}, {\"accuracy\": 0.75, \"loss\": 0.125593364238739, \"time-step\": 4923}, {\"accuracy\": 0.75, \"loss\": 0.12557363510131836, \"time-step\": 4924}, {\"accuracy\": 0.75, \"loss\": 0.12559327483177185, \"time-step\": 4925}, {\"accuracy\": 0.75, \"loss\": 0.12557366490364075, \"time-step\": 4926}, {\"accuracy\": 0.75, \"loss\": 0.1255931556224823, \"time-step\": 4927}, {\"accuracy\": 0.75, \"loss\": 0.1255735605955124, \"time-step\": 4928}, {\"accuracy\": 0.75, \"loss\": 0.12559305131435394, \"time-step\": 4929}, {\"accuracy\": 0.75, \"loss\": 0.1255735158920288, \"time-step\": 4930}, {\"accuracy\": 0.75, \"loss\": 0.12559300661087036, \"time-step\": 4931}, {\"accuracy\": 0.75, \"loss\": 0.12557345628738403, \"time-step\": 4932}, {\"accuracy\": 0.75, \"loss\": 0.125592902302742, \"time-step\": 4933}, {\"accuracy\": 0.75, \"loss\": 0.12557336688041687, \"time-step\": 4934}, {\"accuracy\": 0.75, \"loss\": 0.12559279799461365, \"time-step\": 4935}, {\"accuracy\": 0.75, \"loss\": 0.1255732923746109, \"time-step\": 4936}, {\"accuracy\": 0.75, \"loss\": 0.12559279799461365, \"time-step\": 4937}, {\"accuracy\": 0.75, \"loss\": 0.12557318806648254, \"time-step\": 4938}, {\"accuracy\": 0.75, \"loss\": 0.12559263408184052, \"time-step\": 4939}, {\"accuracy\": 0.75, \"loss\": 0.1255730837583542, \"time-step\": 4940}, {\"accuracy\": 0.75, \"loss\": 0.12559258937835693, \"time-step\": 4941}, {\"accuracy\": 0.75, \"loss\": 0.12557309865951538, \"time-step\": 4942}, {\"accuracy\": 0.75, \"loss\": 0.12559248507022858, \"time-step\": 4943}, {\"accuracy\": 0.75, \"loss\": 0.12557287514209747, \"time-step\": 4944}, {\"accuracy\": 0.75, \"loss\": 0.12559251487255096, \"time-step\": 4945}, {\"accuracy\": 0.75, \"loss\": 0.12557284533977509, \"time-step\": 4946}, {\"accuracy\": 0.75, \"loss\": 0.12559236586093903, \"time-step\": 4947}, {\"accuracy\": 0.75, \"loss\": 0.1255728006362915, \"time-step\": 4948}, {\"accuracy\": 0.75, \"loss\": 0.1255922168493271, \"time-step\": 4949}, {\"accuracy\": 0.75, \"loss\": 0.12557260692119598, \"time-step\": 4950}, {\"accuracy\": 0.75, \"loss\": 0.1255921721458435, \"time-step\": 4951}, {\"accuracy\": 0.75, \"loss\": 0.12557260692119598, \"time-step\": 4952}, {\"accuracy\": 0.75, \"loss\": 0.12559206783771515, \"time-step\": 4953}, {\"accuracy\": 0.75, \"loss\": 0.12557242810726166, \"time-step\": 4954}, {\"accuracy\": 0.75, \"loss\": 0.12559209764003754, \"time-step\": 4955}, {\"accuracy\": 0.75, \"loss\": 0.12557238340377808, \"time-step\": 4956}, {\"accuracy\": 0.75, \"loss\": 0.12559188902378082, \"time-step\": 4957}, {\"accuracy\": 0.75, \"loss\": 0.1255723237991333, \"time-step\": 4958}, {\"accuracy\": 0.75, \"loss\": 0.12559181451797485, \"time-step\": 4959}, {\"accuracy\": 0.75, \"loss\": 0.12557223439216614, \"time-step\": 4960}, {\"accuracy\": 0.75, \"loss\": 0.12559179961681366, \"time-step\": 4961}, {\"accuracy\": 0.75, \"loss\": 0.1255720853805542, \"time-step\": 4962}, {\"accuracy\": 0.75, \"loss\": 0.1255916953086853, \"time-step\": 4963}, {\"accuracy\": 0.75, \"loss\": 0.12557214498519897, \"time-step\": 4964}, {\"accuracy\": 0.75, \"loss\": 0.12559162080287933, \"time-step\": 4965}, {\"accuracy\": 0.75, \"loss\": 0.12557193636894226, \"time-step\": 4966}, {\"accuracy\": 0.75, \"loss\": 0.1255914866924286, \"time-step\": 4967}, {\"accuracy\": 0.75, \"loss\": 0.12557187676429749, \"time-step\": 4968}, {\"accuracy\": 0.75, \"loss\": 0.12559136748313904, \"time-step\": 4969}, {\"accuracy\": 0.75, \"loss\": 0.12557172775268555, \"time-step\": 4970}, {\"accuracy\": 0.75, \"loss\": 0.12559127807617188, \"time-step\": 4971}, {\"accuracy\": 0.75, \"loss\": 0.12557172775268555, \"time-step\": 4972}, {\"accuracy\": 0.75, \"loss\": 0.1255911886692047, \"time-step\": 4973}, {\"accuracy\": 0.75, \"loss\": 0.12557163834571838, \"time-step\": 4974}, {\"accuracy\": 0.75, \"loss\": 0.12559108436107635, \"time-step\": 4975}, {\"accuracy\": 0.75, \"loss\": 0.12557154893875122, \"time-step\": 4976}, {\"accuracy\": 0.75, \"loss\": 0.12559103965759277, \"time-step\": 4977}, {\"accuracy\": 0.75, \"loss\": 0.12557150423526764, \"time-step\": 4978}, {\"accuracy\": 0.75, \"loss\": 0.12559100985527039, \"time-step\": 4979}, {\"accuracy\": 0.75, \"loss\": 0.1255713552236557, \"time-step\": 4980}, {\"accuracy\": 0.75, \"loss\": 0.12559090554714203, \"time-step\": 4981}, {\"accuracy\": 0.75, \"loss\": 0.12557125091552734, \"time-step\": 4982}, {\"accuracy\": 0.75, \"loss\": 0.12559084594249725, \"time-step\": 4983}, {\"accuracy\": 0.75, \"loss\": 0.12557122111320496, \"time-step\": 4984}, {\"accuracy\": 0.75, \"loss\": 0.1255907118320465, \"time-step\": 4985}, {\"accuracy\": 0.75, \"loss\": 0.12557104229927063, \"time-step\": 4986}, {\"accuracy\": 0.75, \"loss\": 0.12559066712856293, \"time-step\": 4987}, {\"accuracy\": 0.75, \"loss\": 0.12557101249694824, \"time-step\": 4988}, {\"accuracy\": 0.75, \"loss\": 0.12559059262275696, \"time-step\": 4989}, {\"accuracy\": 0.75, \"loss\": 0.12557090818881989, \"time-step\": 4990}, {\"accuracy\": 0.75, \"loss\": 0.1255904734134674, \"time-step\": 4991}, {\"accuracy\": 0.75, \"loss\": 0.12557080388069153, \"time-step\": 4992}, {\"accuracy\": 0.75, \"loss\": 0.12559033930301666, \"time-step\": 4993}, {\"accuracy\": 0.75, \"loss\": 0.12557077407836914, \"time-step\": 4994}, {\"accuracy\": 0.75, \"loss\": 0.12559033930301666, \"time-step\": 4995}, {\"accuracy\": 0.75, \"loss\": 0.125570610165596, \"time-step\": 4996}, {\"accuracy\": 0.75, \"loss\": 0.12559010088443756, \"time-step\": 4997}, {\"accuracy\": 0.75, \"loss\": 0.12557053565979004, \"time-step\": 4998}, {\"accuracy\": 0.75, \"loss\": 0.12559011578559875, \"time-step\": 4999}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['acc']\n",
    "\n",
    "df = pd.DataFrame({\"accuracy\":accuracy, \"loss\":loss, \"time-step\": np.arange(0, len(accuracy))})\n",
    "\n",
    "base = alt.Chart(df).mark_line(color=\"blue\").encode(x=\"time-step\", y=\"accuracy\")\n",
    "loss = alt.Chart(df).mark_line(color=\"red\").encode(x=\"time-step\", y=\"loss\")\n",
    "(base  + loss).properties(title='Chart 2').resolve_scale(y='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy goes to 100% in around 1,000 epochs (note that different runs may slightly change the results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a strict sense, LSTM is a type of layer instead of a type of network. What I've calling LSTM networks is basically any RNN composed of LSTM layers. Most RNNs you'll find in the wild (i.e., the internet) use either LSTMs or [Gated Recurrent Units (GRU)](https://en.wikipedia.org/wiki/Gated_recurrent_unit). We don't cover GRU here since they are very similar to LSTMs and this chapter is dense enough as it is. If you want to learn more about GRU see [Cho et al (2014)](https://arxiv.org/abs/1406.1078) and [Chapter 9.1 from Zhang (2020)](https://d2l.ai/chapter_recurrent-modern/gru.html).\n",
    "\n",
    "For this section, I'll base the code in the example provided by Chollet (2017) in chapter 6. As a side note, if you are interested in learning Keras in-depth, [Chollet's book](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438) is probably the best source since he is the creator of Keras library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preprocessing data from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are like me,  you like to check the [IMDB](https://www.imdb.com/) reviews before watching a movie. For this example, we will make use of the [IMDB dataset](https://www.imdb.com/interfaces/), and Lucky us, Keras comes pre-packaged with it. The IMDB dataset comprises 50,000 movie reviews, 50% positive and 50% negative. Keras give access to a numerically encoded version of the dataset where each word is mapped to sequences of integers. We can download the dataset by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for this section\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This time I also imported Tensorflow, and from there Keras layers and models. Keras happens to be integrated with Tensorflow, as a high-level interface, so nothing important changes when doing this. Yet, there are some implementation issues with the optimizer that require importing from Tensorflow to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 5000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `num_words=5000` restrict the dataset to the top 5,000 most frequent words. We do this to avoid highly infrequent words. Often, infrequent words are either typos or words for which we don't have enough statistical information to learn useful representations. \n",
    "\n",
    "Data is downloaded as a (25000,) tuples of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (25000,), train-labels shape: (25000,)\n",
      "test-data shape: (25000,), test-labels shape: (25000,) \n",
      "\n",
      "first 10 words of first sequence: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65],\n",
      "first sequence label: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {train_data.shape}, train-labels shape: {train_labels.shape}')\n",
    "print(f'test-data shape: {test_data.shape}, test-labels shape: {test_labels.shape} \\n')\n",
    "print(f'first 10 words of first sequence: {train_data[0][0:10]},\\nfirst sequence label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about the review contents, the code snippet below decodes the first review into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 100 characters of first sequence:\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the....\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "print(f'first 100 characters of first sequence:\\n{decoded_review[0:100]}....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to **\"pad\" each sequence with zeros** such that all sequences are of the same length. We do this because Keras layers expect same-length vectors as input sequences. Given that we are considering only the 5,000 more frequent words, we have max length of any sequence is 5,000. Hence, we have to pad every sequence to have length 5,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_data, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we go from a list of list (samples= 25000,), to a matrix of shape (samples=25000, maxleng=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (25000, 5000), train-labels shape: (25000, 5000) \n",
      "\n",
      "[  0   0   0 ...  19 178  32] [  0   0   0 ...  14   6 717]\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {X_train.shape}, train-labels shape: {X_test.shape} \\n')\n",
    "print(X_train[0],X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will take only the first 5,000 training and testing examples. We do this because training RNNs is computationally expensive, and we don't have access to enough hardware resources to train a large model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size=5000\n",
    "training_sentences = X_train[0:training_size]\n",
    "testing_sentences =  X_test[0:training_size]\n",
    "training_labels = train_labels[0:training_size]\n",
    "testing_labels = test_labels[0:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-data shape: (5000, 5000), train-labels shape: (5000,)\n",
      "test-data shape: (5000, 5000), test-labels shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'train-data shape: {training_sentences.shape}, train-labels shape: {training_labels.shape}')\n",
    "print(f'test-data shape: {testing_sentences.shape}, test-labels shape: {testing_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the percentage of positive reviews samples on training and testing as a sanity check. We want this to be close to 50% so the sample is balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of positive reviews in training: 0.5092\n",
      "percentage of positive reviews in testing: 0.4858\n"
     ]
    }
   ],
   "source": [
    "print(f'percentage of positive reviews in training: {training_labels.sum()/training_size}')\n",
    "print(f'percentage of positive reviews in testing: {testing_labels.sum()/training_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use word embeddings instead of one-hot encodings this time. Again, Keras provides convenience functions (or layer) to learn word embeddings along with RNNs training. An embedding in Keras is a layer that takes two inputs as a minimum: the **max length of a sequence** (i.e., the max number of tokens), and the **desired dimensionality of the embedding** (i.e., in how many vectors you want to represent the tokens). For instance, for an embedding with 5,000 tokens and 32 embedding vectors we just define `model.add(Embedding(5,000, 32))`. We will do this when defining the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM architecture in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining RNN with LSTM layers is remarkably simple with Keras (considering how complex LSTMs are as mathematical objects). I'll define a relatively \"shallow\" network with just 1 hidden LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network as a linear stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add embedding layer with:\n",
    "    # - Max number of tokens: 10,000\n",
    "    # - Number of embeddings vectors: 32 \n",
    "model.add(Embedding(maxlen, 32))\n",
    "\n",
    "# Add LSTM layer with 32 units (sequence length)\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add output layer with sigmoid activation unit\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Application: IMDB review prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train and test our RNN. I'll run just five epochs, again, because we don't have enough computational resources and for a demo is more than enough. If you run this, it may take around 5-15 minutes in a CPU.  For instance, my Intel i7-8550U took ~10 min to run five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(training_sentences, training_labels,\n",
    "                    epochs=5,\n",
    "                    batch_size=128, # update gradients every 128 sequences\n",
    "                    validation_split=0.2, # validation subsample\n",
    "                    verbose=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: a \"validation split\" is different from the testing set: It's a sub-sample from the training set. For instance, with a training sample of 5,000, the `validation_split = 0.2` will split the data in a 4,000 effective training set and a 1,000 validation set. The network is trained only in the training set, whereas the validation set is used as a real-time(ish) way to help with hyper-parameter tunning, by synchronously evaluating the network in such a sub-sample. To learn more about this see the [Wikipedia article on the topic](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training accuracy:0.8964999914169312\n",
      "final validation accuracy:0.843999981880188\n"
     ]
    }
   ],
   "source": [
    "print(f\"final training accuracy:{history.history['acc'][-1]}\")\n",
    "print(f\"final validation accuracy:{history.history['val_acc'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a **training accuracy of ~90%** and **validation accuracy of ~84%** (note that different runs may slightly change the results). The top-pane in **Chart 3** shows the training and validation curves for accuracy, whereas the bottom-pane shows the same for the loss. It is clear that the network overfitting the data by the 3rd epoch. This is expected as our architecture is shallow, the training set relatively small, and no regularization method was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7f2cf5625a9d47e48e02f5c0b09f3348\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-7f2cf5625a9d47e48e02f5c0b09f3348\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"vconcat\": [{\"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#0202d6\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"#7272a1\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"val_accuracy\"}}}]}, {\"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"#d60202\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"#cc6e6e\"}, \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"time-step\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"val_loss\"}}}]}], \"data\": {\"name\": \"data-bdebec54e5c79b4012d98fed6d8aeb60\"}, \"title\": \"Chart 3\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-bdebec54e5c79b4012d98fed6d8aeb60\": [{\"accuracy\": 0.5544999837875366, \"val_accuracy\": 0.7170000076293945, \"loss\": 0.6878184242248535, \"val_loss\": 0.6635834822654724, \"time-step\": 1}, {\"accuracy\": 0.7559999823570251, \"val_accuracy\": 0.8149999976158142, \"loss\": 0.6069478569030762, \"val_loss\": 0.5463335819244385, \"time-step\": 2}, {\"accuracy\": 0.828000009059906, \"val_accuracy\": 0.6370000243186951, \"loss\": 0.4711909568309784, \"val_loss\": 0.7196103053092957, \"time-step\": 3}, {\"accuracy\": 0.8672500252723694, \"val_accuracy\": 0.8289999961853027, \"loss\": 0.3661901972293854, \"val_loss\": 0.418640040397644, \"time-step\": 4}, {\"accuracy\": 0.8964999914169312, \"val_accuracy\": 0.843999981880188, \"loss\": 0.2921113691329956, \"val_loss\": 0.3783629412651062, \"time-step\": 5}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "accuracy = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "df = pd.DataFrame({\"accuracy\":accuracy,\n",
    "                   \"val_accuracy\": val_acc,\n",
    "                   \"loss\":loss,\n",
    "                   \"val_loss\": val_loss,\n",
    "                   \"time-step\": np.arange(1, len(accuracy)+1)})\n",
    "\n",
    "accu = alt.Chart(df).mark_line(color=\"#0202d6\").encode(x=\"time-step\", y=\"accuracy\")\n",
    "val_accu = alt.Chart(df).mark_line(color=\"#7272a1\").encode(x=\"time-step\", y=\"val_accuracy\")\n",
    "\n",
    "loss = alt.Chart(df).mark_line(color=\"#d60202\").encode(x=\"time-step\", y=\"loss\")\n",
    "val_loss = alt.Chart(df).mark_line(color=\"#cc6e6e\").encode(x=\"time-step\", y=\"val_loss\")\n",
    "\n",
    "((accu  + val_accu)&(loss + val_loss)).properties(title='Chart 3') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model obtains a **test set accuracy of ~84%** echoing the results from the validation set. All things considered, this is a very respectable result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss score: 0.3852371459245682\n",
      "Test accuracy score:0.8384000062942505\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testing_sentences, testing_labels, verbose=0)\n",
    "print(f'Test loss score: {score[0]}')\n",
    "print(f'Test accuracy score:{ score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs is hard and costly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in previous sections, there are three well-known issues that make training RNNs really hard: (1) vanishing gradients, (2) exploding gradients, (3) and its sequential nature, which make them computationally expensive as parallelization is difficult. I won't discuss again these issues. Many techniques have been developed to address all these issues, from architectures like LSTM,  GRU, and ResNets, to techniques like gradient clipping and regularization (Pascanu et al (2012); for an up to date (i.e., 2020) review of this issues see [Chapter 9 of Zhang et al book](https://d2l.ai/chapter_recurrent-modern/index.html).). \n",
    "\n",
    "The quest for solutions to RNNs deficiencies has prompt the development of new architectures like Encoder-Decoder networks with \"attention\" mechanisms (Bahdanau et al, 2014; Vaswani et al, 2017). This new type of architecture seems to be outperforming RNNs in tasks like machine translation and text generation, in addition to overcoming some RNN deficiencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do RNNs \"really\" understand...anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critics like Gary Marcus have pointed out the apparent inability of neural-networks based models to \"really\" understand their outputs (Marcus, 2018). This is prominent for RNNs since they have been used profusely used in the context of language generation and understanding. For instance, even state-of-the-art models like [OpenAI GPT-2](https://openai.com/blog/better-language-models/) sometimes produce incoherent sentences. Marcus gives the [following example](https://thegradient.pub/gpt2-and-the-nature-of-intelligence/):\n",
    "\n",
    "> (**Marcus**) Suppose for example that I ask the system what happens when I put two trophies a table and another: *I put two trophies on a table, and then add another, the total number is...* \n",
    "\n",
    "> (**GPT-2 answer**) *...is five trophies and I'm like, 'Well, I can live with that, right?*\n",
    "\n",
    "From Marcus' perspective, this lack of coherence is an exemplar of GPT-2 incapacity to understand language. \n",
    "\n",
    "Yet, I'll argue two things. First, this is an unfairly underspecified question: **What do we mean by understanding?** From a cognitive science perspective, this is a fundamental yet strikingly hard question to answer. If you ask five cognitive science what does it \"really\" mean to understand something you are likely to get five different answers. What do we need is a falsifiable way to decide when a system \"really\" understands language. Is lack of coherence enough? I produce incoherent phrases all the time, and I know lots of people that do the same. In any case, it is important to question whether human-level understanding of language (however you want to define it) is necessary to show that a computational model of any cognitive process is a good model or not. We have several great models of many natural phenomena, yet not a single one gets all the aspects of the phenomena perfectly. For instance, Marcus has said that the fact that GPT-2 sometimes produces incoherent sentences is somehow a proof that human \"thoughts\" (i.e., internal representations) can't possibly be represented as vectors (like neural nets do), which I believe is non-sequitur. \n",
    "\n",
    "Second, **Why should we expect that a network trained for a narrow task like language production should understand what language \"really\" is?** The exercise of comparing computational models of \"cognitive processes\" with \"full-blown\" human cognition, makes as much sense as comparing a model of bipedal locomotion with the entire motor control system of an animal. A model of bipedal locomotion is just that: **a model of a sub-system or sub-process within a larger system, not a reproduction of the entire system**. The fact that a model of bipedal locomotion does not capture well the mechanics of \"jumping\", does not undermine it's veracity or utility, in the same manner, that the inability of a model of language production to \"understand\" all aspects of language does not undermine its plausibility as a model of...languague production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks have been prolific models in cognitive science (Munakata et al, 1997; St. John, 1992; Plaut et al., 1996; Christiansen & Chater, 1999; Botvinick & Plaut, 2004; Muñoz-Organero et al., 2019), bringing together intuitions about how cognitive systems work in time-dependent domains, and how neural networks may accommodate such processes.\n",
    "\n",
    "Nevertheless, problems like vanishing gradients, exploding gradients, and computational inefficiency (i.e., lack of parallelization) have difficulted RNN use in many domains. Although new architectures (without recursive structures) have been developed to improve RNN results and overcome its limitations, they remain relevant from a cognitive science perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. ArXiv Preprint ArXiv:1409.0473.\n",
    "- Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.\n",
    "- Botvinick, M., & Plaut, D. C. (2004). Doing without schema hierarchies: A recurrent connectionist approach to normal and impaired routine sequential action. Psychological Review, 111(2), 395.\n",
    "- Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Current Opinion in Neurobiology, 46, 1–6. https://doi.org/10.1016/j.conb.2017.06.003\n",
    "- Chen, G. (2016). A gentle tutorial of recurrent neural network with error backpropagation. arXiv preprint arXiv:1610.02583.\n",
    "- Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1\n",
    "- François, C. (2017). 6. Deep Learning for text and sequences. Deep learning with Python. Manning.\n",
    "- Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.\n",
    "- Christiansen, M. H., & Chater, N. (1999). Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2), 157–205.\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). 10. Sequence Modeling: Recurrent and Recursive Nets. In Deep Learning. MIT Press. https://www.deeplearningbook.org/contents/mlp.html\n",
    "- Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Psychology Press.\n",
    "- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.\n",
    "- Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. *Proceedings of the National Academy of Sciences*, *79*(8), 2554–2558.\n",
    "- Güçlü, U., & van Gerven, M. A. (2017). Modeling the dynamics of human brain activity with recurrent neural networks. Frontiers in Computational Neuroscience, 11, 7.\n",
    "- Graves, A. (2012). Supervised sequence labelling. In Supervised sequence labelling with recurrent neural networks (pp. 5-13). Springer, Berlin, Heidelberg.\n",
    "- Jarne, C., & Laje, R. (2019). A detailed study of recurrent neural networks used to model tasks in the cerebral cortex. ArXiv Preprint ArXiv:1906.01094.\n",
    "- John, M. F. (1992). The story gestalt: A model of knowledge-intensive processes in text comprehension. Cognitive Science, 16(2), 271–306.\n",
    "- Jordan, M. I. (1986). Serial order: A parallel distributed processing approach, ICS Report 8604. *Institute for Cognitive Science, UCSD, La Jolla*.\n",
    "- Marcus, G. (2018). Deep learning: A critical appraisal. ArXiv Preprint ArXiv:1801.00631.\n",
    "- Munakata, Y., McClelland, J. L., Johnson, M. H., & Siegler, R. S. (1997). Rethinking infant knowledge: Toward an adaptive process account of successes and failures in object permanence tasks. Psychological Review, 104(4), 686.\n",
    "- Muñoz-Organero, M., Powell, L., Heller, B., Harpin, V., & Parker, J. (2019). Using Recurrent Neural Networks to Compare Movement Patterns in ADHD and Normally Developing Children Based on Acceleration Signals from the Wrist and Ankle. Sensors (Basel, Switzerland), 19(13). https://doi.org/10.3390/s19132935\n",
    "- K. J. Lang, A. H. Waibel, and G. E. Hinton. A Time-delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3(1):23-43, 1990\n",
    "- Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International Conference on Machine Learning, 1310–1318.\n",
    "- Philipp, G., Song, D., & Carbonell, J. G. (2017). The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions. ArXiv Preprint ArXiv:1712.05577.\n",
    "- Plaut, D. C., McClelland, J. L., Seidenberg, M. S., & Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103(1), 56.\n",
    "- Raj, B. (2020, Spring). Neural Networks: Hopfield Nets and Auto Associators [Lecture]. http://deeplearning.cs.cmu.edu/document/slides/lec17.hopfield.pdf\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \\Lukasz, & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 5998–6008.\n",
    "- Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). 8. Recurrent Neural Networks. In Dive into Deep Learning. https://d2l.ai/chapter_convolutional-neural-networks/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful online resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a list of my favorite online resources to learn more about Recurrent Neural Networks:\n",
    "\n",
    "- Stanford Lectures: Natural Language Processing with Deep Learning, Winter 2020. [Coruse webpage](http://web.stanford.edu/class/cs224n/index.html#schedule).\n",
    "- Bhiksha Raj's Deep Learning Lectures 13, 14, and 15 at CMU. [Course webpage](http://deeplearning.cs.cmu.edu/)\n",
    "- Geoffrey Hinton's Neural Network Lectures 7 and 8. [YouTube Videos](https://www.youtube.com/watch?v=2fRnHVVLf1Y&list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
